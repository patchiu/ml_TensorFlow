{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()\n",
    "        # 此處添加初始化程式碼（包含 call 方法中會用到的層），例如\n",
    "        # layer1 = tf.keras.layers.BuiltInLayer(...)\n",
    "        # layer2 = MyCustomLayer(...)\n",
    "\n",
    "    def call(self, input):\n",
    "        # 此處添加模型呼叫的程式碼（處理輸入並返回輸出），例如\n",
    "        # x = layer1(input)\n",
    "        # output = layer2(x)\n",
    "        return output\n",
    "\n",
    "    # 還可以添加自定義的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "     def add(self, x):\n",
    "         y = x+1\n",
    "         print(y)\n",
    "class B(A):\n",
    "    def add(self, x):\n",
    "        super().add(x)\n",
    "b = B()\n",
    "b.add(2)  # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.40784496],\n",
      "       [1.191065  ],\n",
      "       [1.9742855 ]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.78322077], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.zeros_initializer(),\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 以下程式碼結構與前一節類似\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(lr=0.01)\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)      # 呼叫模型 y_pred = model(X) 而不是顯式寫出 y_pred = a * X + b\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)    # 使用 model.variables 這一屬性直接獲得模型中的所有變數\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的圖片預設為uint8（0-255的數字）。以下程式碼將其正規化到0-1之間的浮點數，並在最後增加一維作為顏色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機取出batch_size個元素並返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten層將除第一維（batch_size）以外的維度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.402627\n",
      "batch 1: loss 2.376355\n",
      "batch 2: loss 2.239973\n",
      "batch 3: loss 2.168834\n",
      "batch 4: loss 2.037308\n",
      "batch 5: loss 1.987093\n",
      "batch 6: loss 1.977174\n",
      "batch 7: loss 1.900226\n",
      "batch 8: loss 1.809184\n",
      "batch 9: loss 1.937359\n",
      "batch 10: loss 1.800917\n",
      "batch 11: loss 1.648825\n",
      "batch 12: loss 1.529788\n",
      "batch 13: loss 1.624029\n",
      "batch 14: loss 1.635276\n",
      "batch 15: loss 1.350808\n",
      "batch 16: loss 1.574706\n",
      "batch 17: loss 1.301706\n",
      "batch 18: loss 1.341482\n",
      "batch 19: loss 1.277082\n",
      "batch 20: loss 1.414743\n",
      "batch 21: loss 1.206796\n",
      "batch 22: loss 1.226705\n",
      "batch 23: loss 1.254171\n",
      "batch 24: loss 1.220646\n",
      "batch 25: loss 1.146522\n",
      "batch 26: loss 1.004949\n",
      "batch 27: loss 1.026992\n",
      "batch 28: loss 1.027799\n",
      "batch 29: loss 1.015100\n",
      "batch 30: loss 1.072310\n",
      "batch 31: loss 0.955938\n",
      "batch 32: loss 0.873975\n",
      "batch 33: loss 0.872546\n",
      "batch 34: loss 0.923194\n",
      "batch 35: loss 0.848913\n",
      "batch 36: loss 0.818888\n",
      "batch 37: loss 0.835473\n",
      "batch 38: loss 0.845701\n",
      "batch 39: loss 0.644325\n",
      "batch 40: loss 0.603325\n",
      "batch 41: loss 0.809062\n",
      "batch 42: loss 0.808174\n",
      "batch 43: loss 0.644417\n",
      "batch 44: loss 0.829428\n",
      "batch 45: loss 0.683059\n",
      "batch 46: loss 0.697282\n",
      "batch 47: loss 0.749412\n",
      "batch 48: loss 0.722682\n",
      "batch 49: loss 0.770766\n",
      "batch 50: loss 0.659323\n",
      "batch 51: loss 0.813808\n",
      "batch 52: loss 0.601422\n",
      "batch 53: loss 0.647559\n",
      "batch 54: loss 0.600311\n",
      "batch 55: loss 0.602304\n",
      "batch 56: loss 0.620211\n",
      "batch 57: loss 0.534109\n",
      "batch 58: loss 0.461132\n",
      "batch 59: loss 0.637508\n",
      "batch 60: loss 0.580519\n",
      "batch 61: loss 0.688059\n",
      "batch 62: loss 0.721679\n",
      "batch 63: loss 0.600657\n",
      "batch 64: loss 0.570006\n",
      "batch 65: loss 0.534952\n",
      "batch 66: loss 0.440451\n",
      "batch 67: loss 0.664439\n",
      "batch 68: loss 0.365929\n",
      "batch 69: loss 0.440377\n",
      "batch 70: loss 0.757723\n",
      "batch 71: loss 0.492656\n",
      "batch 72: loss 0.634910\n",
      "batch 73: loss 0.388370\n",
      "batch 74: loss 0.273275\n",
      "batch 75: loss 0.588112\n",
      "batch 76: loss 0.466978\n",
      "batch 77: loss 0.602972\n",
      "batch 78: loss 0.336031\n",
      "batch 79: loss 0.432905\n",
      "batch 80: loss 0.378686\n",
      "batch 81: loss 0.337408\n",
      "batch 82: loss 0.465063\n",
      "batch 83: loss 0.432148\n",
      "batch 84: loss 0.426174\n",
      "batch 85: loss 0.463943\n",
      "batch 86: loss 0.618145\n",
      "batch 87: loss 0.669779\n",
      "batch 88: loss 0.362338\n",
      "batch 89: loss 0.793715\n",
      "batch 90: loss 0.513426\n",
      "batch 91: loss 0.259288\n",
      "batch 92: loss 0.314298\n",
      "batch 93: loss 0.326727\n",
      "batch 94: loss 0.528447\n",
      "batch 95: loss 0.507210\n",
      "batch 96: loss 0.580424\n",
      "batch 97: loss 0.393418\n",
      "batch 98: loss 0.679594\n",
      "batch 99: loss 0.512181\n",
      "batch 100: loss 0.354795\n",
      "batch 101: loss 0.525033\n",
      "batch 102: loss 0.604968\n",
      "batch 103: loss 0.375470\n",
      "batch 104: loss 0.493956\n",
      "batch 105: loss 0.422236\n",
      "batch 106: loss 0.462436\n",
      "batch 107: loss 0.560809\n",
      "batch 108: loss 0.627176\n",
      "batch 109: loss 0.585612\n",
      "batch 110: loss 0.505748\n",
      "batch 111: loss 0.477995\n",
      "batch 112: loss 0.483385\n",
      "batch 113: loss 0.344004\n",
      "batch 114: loss 0.491763\n",
      "batch 115: loss 0.419382\n",
      "batch 116: loss 0.577200\n",
      "batch 117: loss 0.477363\n",
      "batch 118: loss 0.368707\n",
      "batch 119: loss 0.348560\n",
      "batch 120: loss 0.416762\n",
      "batch 121: loss 0.460522\n",
      "batch 122: loss 0.570971\n",
      "batch 123: loss 0.454078\n",
      "batch 124: loss 0.329038\n",
      "batch 125: loss 0.334108\n",
      "batch 126: loss 0.394042\n",
      "batch 127: loss 0.252949\n",
      "batch 128: loss 0.332296\n",
      "batch 129: loss 0.319323\n",
      "batch 130: loss 0.338128\n",
      "batch 131: loss 0.417452\n",
      "batch 132: loss 0.408968\n",
      "batch 133: loss 0.333160\n",
      "batch 134: loss 0.436215\n",
      "batch 135: loss 0.325618\n",
      "batch 136: loss 0.368871\n",
      "batch 137: loss 0.370674\n",
      "batch 138: loss 0.379818\n",
      "batch 139: loss 0.361201\n",
      "batch 140: loss 0.437807\n",
      "batch 141: loss 0.441150\n",
      "batch 142: loss 0.923546\n",
      "batch 143: loss 0.525026\n",
      "batch 144: loss 0.365275\n",
      "batch 145: loss 0.482278\n",
      "batch 146: loss 0.283637\n",
      "batch 147: loss 0.557205\n",
      "batch 148: loss 0.417097\n",
      "batch 149: loss 0.455384\n",
      "batch 150: loss 0.304572\n",
      "batch 151: loss 0.326247\n",
      "batch 152: loss 0.278803\n",
      "batch 153: loss 0.567689\n",
      "batch 154: loss 0.375448\n",
      "batch 155: loss 0.154892\n",
      "batch 156: loss 0.320997\n",
      "batch 157: loss 0.468239\n",
      "batch 158: loss 0.421260\n",
      "batch 159: loss 0.571947\n",
      "batch 160: loss 0.405581\n",
      "batch 161: loss 0.279059\n",
      "batch 162: loss 0.361775\n",
      "batch 163: loss 0.264668\n",
      "batch 164: loss 0.479894\n",
      "batch 165: loss 0.456153\n",
      "batch 166: loss 0.348838\n",
      "batch 167: loss 0.215988\n",
      "batch 168: loss 0.187917\n",
      "batch 169: loss 0.368061\n",
      "batch 170: loss 0.391764\n",
      "batch 171: loss 0.414548\n",
      "batch 172: loss 0.357427\n",
      "batch 173: loss 0.481265\n",
      "batch 174: loss 0.369507\n",
      "batch 175: loss 0.238851\n",
      "batch 176: loss 0.559275\n",
      "batch 177: loss 0.458149\n",
      "batch 178: loss 0.524140\n",
      "batch 179: loss 0.297322\n",
      "batch 180: loss 0.414042\n",
      "batch 181: loss 0.659442\n",
      "batch 182: loss 0.318800\n",
      "batch 183: loss 0.325586\n",
      "batch 184: loss 0.371620\n",
      "batch 185: loss 0.427442\n",
      "batch 186: loss 0.273861\n",
      "batch 187: loss 0.360848\n",
      "batch 188: loss 0.260405\n",
      "batch 189: loss 0.504553\n",
      "batch 190: loss 0.347806\n",
      "batch 191: loss 0.413963\n",
      "batch 192: loss 0.359515\n",
      "batch 193: loss 0.377856\n",
      "batch 194: loss 0.295773\n",
      "batch 195: loss 0.211830\n",
      "batch 196: loss 0.318253\n",
      "batch 197: loss 0.366906\n",
      "batch 198: loss 0.470969\n",
      "batch 199: loss 0.320541\n",
      "batch 200: loss 0.375045\n",
      "batch 201: loss 0.373434\n",
      "batch 202: loss 0.303057\n",
      "batch 203: loss 0.345895\n",
      "batch 204: loss 0.442537\n",
      "batch 205: loss 0.329694\n",
      "batch 206: loss 0.283726\n",
      "batch 207: loss 0.166327\n",
      "batch 208: loss 0.272597\n",
      "batch 209: loss 0.306876\n",
      "batch 210: loss 0.193292\n",
      "batch 211: loss 0.387800\n",
      "batch 212: loss 0.474849\n",
      "batch 213: loss 0.516051\n",
      "batch 214: loss 0.193108\n",
      "batch 215: loss 0.361065\n",
      "batch 216: loss 0.290425\n",
      "batch 217: loss 0.331944\n",
      "batch 218: loss 0.213227\n",
      "batch 219: loss 0.403399\n",
      "batch 220: loss 0.238208\n",
      "batch 221: loss 0.371287\n",
      "batch 222: loss 0.255121\n",
      "batch 223: loss 0.241296\n",
      "batch 224: loss 0.313286\n",
      "batch 225: loss 0.329255\n",
      "batch 226: loss 0.155559\n",
      "batch 227: loss 0.391215\n",
      "batch 228: loss 0.223094\n",
      "batch 229: loss 0.336002\n",
      "batch 230: loss 0.344728\n",
      "batch 231: loss 0.263332\n",
      "batch 232: loss 0.438588\n",
      "batch 233: loss 0.443236\n",
      "batch 234: loss 0.244371\n",
      "batch 235: loss 0.257846\n",
      "batch 236: loss 0.226463\n",
      "batch 237: loss 0.247917\n",
      "batch 238: loss 0.477063\n",
      "batch 239: loss 0.342688\n",
      "batch 240: loss 0.352083\n",
      "batch 241: loss 0.296519\n",
      "batch 242: loss 0.426173\n",
      "batch 243: loss 0.564956\n",
      "batch 244: loss 0.427929\n",
      "batch 245: loss 0.317398\n",
      "batch 246: loss 0.434218\n",
      "batch 247: loss 0.422072\n",
      "batch 248: loss 0.402251\n",
      "batch 249: loss 0.244426\n",
      "batch 250: loss 0.545912\n",
      "batch 251: loss 0.276239\n",
      "batch 252: loss 0.257469\n",
      "batch 253: loss 0.289359\n",
      "batch 254: loss 0.390548\n",
      "batch 255: loss 0.347059\n",
      "batch 256: loss 0.317140\n",
      "batch 257: loss 0.328536\n",
      "batch 258: loss 0.294153\n",
      "batch 259: loss 0.492058\n",
      "batch 260: loss 0.365747\n",
      "batch 261: loss 0.215221\n",
      "batch 262: loss 0.334162\n",
      "batch 263: loss 0.513797\n",
      "batch 264: loss 0.167847\n",
      "batch 265: loss 0.365374\n",
      "batch 266: loss 0.436038\n",
      "batch 267: loss 0.187132\n",
      "batch 268: loss 0.320839\n",
      "batch 269: loss 0.324505\n",
      "batch 270: loss 0.411557\n",
      "batch 271: loss 0.253699\n",
      "batch 272: loss 0.236484\n",
      "batch 273: loss 0.377647\n",
      "batch 274: loss 0.329973\n",
      "batch 275: loss 0.223717\n",
      "batch 276: loss 0.311671\n",
      "batch 277: loss 0.148685\n",
      "batch 278: loss 0.275527\n",
      "batch 279: loss 0.311032\n",
      "batch 280: loss 0.307875\n",
      "batch 281: loss 0.274799\n",
      "batch 282: loss 0.281721\n",
      "batch 283: loss 0.382113\n",
      "batch 284: loss 0.304099\n",
      "batch 285: loss 0.311813\n",
      "batch 286: loss 0.364496\n",
      "batch 287: loss 0.231770\n",
      "batch 288: loss 0.422593\n",
      "batch 289: loss 0.428908\n",
      "batch 290: loss 0.255066\n",
      "batch 291: loss 0.235539\n",
      "batch 292: loss 0.446599\n",
      "batch 293: loss 0.206712\n",
      "batch 294: loss 0.469718\n",
      "batch 295: loss 0.313494\n",
      "batch 296: loss 0.355842\n",
      "batch 297: loss 0.317948\n",
      "batch 298: loss 0.252779\n",
      "batch 299: loss 0.304882\n",
      "batch 300: loss 0.219287\n",
      "batch 301: loss 0.406472\n",
      "batch 302: loss 0.283559\n",
      "batch 303: loss 0.184150\n",
      "batch 304: loss 0.212551\n",
      "batch 305: loss 0.239780\n",
      "batch 306: loss 0.121659\n",
      "batch 307: loss 0.190690\n",
      "batch 308: loss 0.422815\n",
      "batch 309: loss 0.281613\n",
      "batch 310: loss 0.323785\n",
      "batch 311: loss 0.247295\n",
      "batch 312: loss 0.343756\n",
      "batch 313: loss 0.210604\n",
      "batch 314: loss 0.259376\n",
      "batch 315: loss 0.420880\n",
      "batch 316: loss 0.287722\n",
      "batch 317: loss 0.228387\n",
      "batch 318: loss 0.177535\n",
      "batch 319: loss 0.240839\n",
      "batch 320: loss 0.398881\n",
      "batch 321: loss 0.393745\n",
      "batch 322: loss 0.566048\n",
      "batch 323: loss 0.304440\n",
      "batch 324: loss 0.261403\n",
      "batch 325: loss 0.263675\n",
      "batch 326: loss 0.227778\n",
      "batch 327: loss 0.138715\n",
      "batch 328: loss 0.423617\n",
      "batch 329: loss 0.274722\n",
      "batch 330: loss 0.245943\n",
      "batch 331: loss 0.296400\n",
      "batch 332: loss 0.352326\n",
      "batch 333: loss 0.136444\n",
      "batch 334: loss 0.349936\n",
      "batch 335: loss 0.227529\n",
      "batch 336: loss 0.266570\n",
      "batch 337: loss 0.170351\n",
      "batch 338: loss 0.298563\n",
      "batch 339: loss 0.117939\n",
      "batch 340: loss 0.221860\n",
      "batch 341: loss 0.426624\n",
      "batch 342: loss 0.159322\n",
      "batch 343: loss 0.214013\n",
      "batch 344: loss 0.520898\n",
      "batch 345: loss 0.254884\n",
      "batch 346: loss 0.085009\n",
      "batch 347: loss 0.413019\n",
      "batch 348: loss 0.224613\n",
      "batch 349: loss 0.406549\n",
      "batch 350: loss 0.288471\n",
      "batch 351: loss 0.243565\n",
      "batch 352: loss 0.324529\n",
      "batch 353: loss 0.626711\n",
      "batch 354: loss 0.124593\n",
      "batch 355: loss 0.303007\n",
      "batch 356: loss 0.312209\n",
      "batch 357: loss 0.269194\n",
      "batch 358: loss 0.152644\n",
      "batch 359: loss 0.195439\n",
      "batch 360: loss 0.189321\n",
      "batch 361: loss 0.502874\n",
      "batch 362: loss 0.376817\n",
      "batch 363: loss 0.239245\n",
      "batch 364: loss 0.410468\n",
      "batch 365: loss 0.286038\n",
      "batch 366: loss 0.357133\n",
      "batch 367: loss 0.194879\n",
      "batch 368: loss 0.172741\n",
      "batch 369: loss 0.249349\n",
      "batch 370: loss 0.378335\n",
      "batch 371: loss 0.231447\n",
      "batch 372: loss 0.152195\n",
      "batch 373: loss 0.353932\n",
      "batch 374: loss 0.157299\n",
      "batch 375: loss 0.437060\n",
      "batch 376: loss 0.144572\n",
      "batch 377: loss 0.245342\n",
      "batch 378: loss 0.375810\n",
      "batch 379: loss 0.202047\n",
      "batch 380: loss 0.216784\n",
      "batch 381: loss 0.157203\n",
      "batch 382: loss 0.213544\n",
      "batch 383: loss 0.155124\n",
      "batch 384: loss 0.164355\n",
      "batch 385: loss 0.312806\n",
      "batch 386: loss 0.298809\n",
      "batch 387: loss 0.354596\n",
      "batch 388: loss 0.302391\n",
      "batch 389: loss 0.291305\n",
      "batch 390: loss 0.284246\n",
      "batch 391: loss 0.287867\n",
      "batch 392: loss 0.114308\n",
      "batch 393: loss 0.179573\n",
      "batch 394: loss 0.354740\n",
      "batch 395: loss 0.102983\n",
      "batch 396: loss 0.151378\n",
      "batch 397: loss 0.203582\n",
      "batch 398: loss 0.125554\n",
      "batch 399: loss 0.364418\n",
      "batch 400: loss 0.186541\n",
      "batch 401: loss 0.202661\n",
      "batch 402: loss 0.181603\n",
      "batch 403: loss 0.214025\n",
      "batch 404: loss 0.643091\n",
      "batch 405: loss 0.431174\n",
      "batch 406: loss 0.311739\n",
      "batch 407: loss 0.324314\n",
      "batch 408: loss 0.298469\n",
      "batch 409: loss 0.250985\n",
      "batch 410: loss 0.262000\n",
      "batch 411: loss 0.173230\n",
      "batch 412: loss 0.354269\n",
      "batch 413: loss 0.242879\n",
      "batch 414: loss 0.240540\n",
      "batch 415: loss 0.133655\n",
      "batch 416: loss 0.436013\n",
      "batch 417: loss 0.273113\n",
      "batch 418: loss 0.297052\n",
      "batch 419: loss 0.177370\n",
      "batch 420: loss 0.211716\n",
      "batch 421: loss 0.256299\n",
      "batch 422: loss 0.170714\n",
      "batch 423: loss 0.438807\n",
      "batch 424: loss 0.337819\n",
      "batch 425: loss 0.530018\n",
      "batch 426: loss 0.353932\n",
      "batch 427: loss 0.207688\n",
      "batch 428: loss 0.155452\n",
      "batch 429: loss 0.129732\n",
      "batch 430: loss 0.211063\n",
      "batch 431: loss 0.224139\n",
      "batch 432: loss 0.162278\n",
      "batch 433: loss 0.382175\n",
      "batch 434: loss 0.309747\n",
      "batch 435: loss 0.207007\n",
      "batch 436: loss 0.282417\n",
      "batch 437: loss 0.140969\n",
      "batch 438: loss 0.201315\n",
      "batch 439: loss 0.158800\n",
      "batch 440: loss 0.399538\n",
      "batch 441: loss 0.233542\n",
      "batch 442: loss 0.309725\n",
      "batch 443: loss 0.152932\n",
      "batch 444: loss 0.244448\n",
      "batch 445: loss 0.365862\n",
      "batch 446: loss 0.195410\n",
      "batch 447: loss 0.211936\n",
      "batch 448: loss 0.447561\n",
      "batch 449: loss 0.367367\n",
      "batch 450: loss 0.234797\n",
      "batch 451: loss 0.375914\n",
      "batch 452: loss 0.206609\n",
      "batch 453: loss 0.442660\n",
      "batch 454: loss 0.391418\n",
      "batch 455: loss 0.272240\n",
      "batch 456: loss 0.241543\n",
      "batch 457: loss 0.157432\n",
      "batch 458: loss 0.208901\n",
      "batch 459: loss 0.284428\n",
      "batch 460: loss 0.284492\n",
      "batch 461: loss 0.297314\n",
      "batch 462: loss 0.225058\n",
      "batch 463: loss 0.696382\n",
      "batch 464: loss 0.312227\n",
      "batch 465: loss 0.207627\n",
      "batch 466: loss 0.231836\n",
      "batch 467: loss 0.220852\n",
      "batch 468: loss 0.373835\n",
      "batch 469: loss 0.445602\n",
      "batch 470: loss 0.405832\n",
      "batch 471: loss 0.184521\n",
      "batch 472: loss 0.279476\n",
      "batch 473: loss 0.429237\n",
      "batch 474: loss 0.215932\n",
      "batch 475: loss 0.192925\n",
      "batch 476: loss 0.239363\n",
      "batch 477: loss 0.252688\n",
      "batch 478: loss 0.357617\n",
      "batch 479: loss 0.213623\n",
      "batch 480: loss 0.088241\n",
      "batch 481: loss 0.266472\n",
      "batch 482: loss 0.198907\n",
      "batch 483: loss 0.321598\n",
      "batch 484: loss 0.173716\n",
      "batch 485: loss 0.187788\n",
      "batch 486: loss 0.302312\n",
      "batch 487: loss 0.379509\n",
      "batch 488: loss 0.138668\n",
      "batch 489: loss 0.280396\n",
      "batch 490: loss 0.229111\n",
      "batch 491: loss 0.307756\n",
      "batch 492: loss 0.155777\n",
      "batch 493: loss 0.340138\n",
      "batch 494: loss 0.336394\n",
      "batch 495: loss 0.272121\n",
      "batch 496: loss 0.197452\n",
      "batch 497: loss 0.218765\n",
      "batch 498: loss 0.270387\n",
      "batch 499: loss 0.250194\n",
      "batch 500: loss 0.389256\n",
      "batch 501: loss 0.221751\n",
      "batch 502: loss 0.111516\n",
      "batch 503: loss 0.220698\n",
      "batch 504: loss 0.252092\n",
      "batch 505: loss 0.252746\n",
      "batch 506: loss 0.104201\n",
      "batch 507: loss 0.248893\n",
      "batch 508: loss 0.196796\n",
      "batch 509: loss 0.241335\n",
      "batch 510: loss 0.466502\n",
      "batch 511: loss 0.130393\n",
      "batch 512: loss 0.304074\n",
      "batch 513: loss 0.299805\n",
      "batch 514: loss 0.174250\n",
      "batch 515: loss 0.200896\n",
      "batch 516: loss 0.209131\n",
      "batch 517: loss 0.204659\n",
      "batch 518: loss 0.125244\n",
      "batch 519: loss 0.323334\n",
      "batch 520: loss 0.281467\n",
      "batch 521: loss 0.393068\n",
      "batch 522: loss 0.161938\n",
      "batch 523: loss 0.187341\n",
      "batch 524: loss 0.289274\n",
      "batch 525: loss 0.319315\n",
      "batch 526: loss 0.310908\n",
      "batch 527: loss 0.229292\n",
      "batch 528: loss 0.320259\n",
      "batch 529: loss 0.264162\n",
      "batch 530: loss 0.276014\n",
      "batch 531: loss 0.273175\n",
      "batch 532: loss 0.105744\n",
      "batch 533: loss 0.529325\n",
      "batch 534: loss 0.192512\n",
      "batch 535: loss 0.245484\n",
      "batch 536: loss 0.237131\n",
      "batch 537: loss 0.289635\n",
      "batch 538: loss 0.147806\n",
      "batch 539: loss 0.172721\n",
      "batch 540: loss 0.191067\n",
      "batch 541: loss 0.254978\n",
      "batch 542: loss 0.385023\n",
      "batch 543: loss 0.176560\n",
      "batch 544: loss 0.294852\n",
      "batch 545: loss 0.240844\n",
      "batch 546: loss 0.421497\n",
      "batch 547: loss 0.422123\n",
      "batch 548: loss 0.133560\n",
      "batch 549: loss 0.262073\n",
      "batch 550: loss 0.170455\n",
      "batch 551: loss 0.153895\n",
      "batch 552: loss 0.301694\n",
      "batch 553: loss 0.392163\n",
      "batch 554: loss 0.222623\n",
      "batch 555: loss 0.392635\n",
      "batch 556: loss 0.245705\n",
      "batch 557: loss 0.328590\n",
      "batch 558: loss 0.266649\n",
      "batch 559: loss 0.135092\n",
      "batch 560: loss 0.260945\n",
      "batch 561: loss 0.135692\n",
      "batch 562: loss 0.400451\n",
      "batch 563: loss 0.167330\n",
      "batch 564: loss 0.265186\n",
      "batch 565: loss 0.181745\n",
      "batch 566: loss 0.159785\n",
      "batch 567: loss 0.226428\n",
      "batch 568: loss 0.253189\n",
      "batch 569: loss 0.177136\n",
      "batch 570: loss 0.292840\n",
      "batch 571: loss 0.192349\n",
      "batch 572: loss 0.185613\n",
      "batch 573: loss 0.110752\n",
      "batch 574: loss 0.159517\n",
      "batch 575: loss 0.331635\n",
      "batch 576: loss 0.339674\n",
      "batch 577: loss 0.536067\n",
      "batch 578: loss 0.510565\n",
      "batch 579: loss 0.176067\n",
      "batch 580: loss 0.199345\n",
      "batch 581: loss 0.159604\n",
      "batch 582: loss 0.280618\n",
      "batch 583: loss 0.161918\n",
      "batch 584: loss 0.270411\n",
      "batch 585: loss 0.243531\n",
      "batch 586: loss 0.175454\n",
      "batch 587: loss 0.163282\n",
      "batch 588: loss 0.374014\n",
      "batch 589: loss 0.282078\n",
      "batch 590: loss 0.257644\n",
      "batch 591: loss 0.204852\n",
      "batch 592: loss 0.259040\n",
      "batch 593: loss 0.239756\n",
      "batch 594: loss 0.259701\n",
      "batch 595: loss 0.211439\n",
      "batch 596: loss 0.087546\n",
      "batch 597: loss 0.120744\n",
      "batch 598: loss 0.209486\n",
      "batch 599: loss 0.218474\n",
      "batch 600: loss 0.238598\n",
      "batch 601: loss 0.296822\n",
      "batch 602: loss 0.289567\n",
      "batch 603: loss 0.278233\n",
      "batch 604: loss 0.362181\n",
      "batch 605: loss 0.212125\n",
      "batch 606: loss 0.352611\n",
      "batch 607: loss 0.291192\n",
      "batch 608: loss 0.194613\n",
      "batch 609: loss 0.346778\n",
      "batch 610: loss 0.152517\n",
      "batch 611: loss 0.286791\n",
      "batch 612: loss 0.307896\n",
      "batch 613: loss 0.081529\n",
      "batch 614: loss 0.323723\n",
      "batch 615: loss 0.141110\n",
      "batch 616: loss 0.165896\n",
      "batch 617: loss 0.142794\n",
      "batch 618: loss 0.218228\n",
      "batch 619: loss 0.181972\n",
      "batch 620: loss 0.134482\n",
      "batch 621: loss 0.148347\n",
      "batch 622: loss 0.075429\n",
      "batch 623: loss 0.252071\n",
      "batch 624: loss 0.231064\n",
      "batch 625: loss 0.120742\n",
      "batch 626: loss 0.192838\n",
      "batch 627: loss 0.248276\n",
      "batch 628: loss 0.178131\n",
      "batch 629: loss 0.163702\n",
      "batch 630: loss 0.235754\n",
      "batch 631: loss 0.237262\n",
      "batch 632: loss 0.211624\n",
      "batch 633: loss 0.355558\n",
      "batch 634: loss 0.084392\n",
      "batch 635: loss 0.364527\n",
      "batch 636: loss 0.163320\n",
      "batch 637: loss 0.078920\n",
      "batch 638: loss 0.386820\n",
      "batch 639: loss 0.225377\n",
      "batch 640: loss 0.162742\n",
      "batch 641: loss 0.161909\n",
      "batch 642: loss 0.143981\n",
      "batch 643: loss 0.096223\n",
      "batch 644: loss 0.142098\n",
      "batch 645: loss 0.152859\n",
      "batch 646: loss 0.178663\n",
      "batch 647: loss 0.304312\n",
      "batch 648: loss 0.273650\n",
      "batch 649: loss 0.486167\n",
      "batch 650: loss 0.240266\n",
      "batch 651: loss 0.299825\n",
      "batch 652: loss 0.240225\n",
      "batch 653: loss 0.180009\n",
      "batch 654: loss 0.123977\n",
      "batch 655: loss 0.327956\n",
      "batch 656: loss 0.175774\n",
      "batch 657: loss 0.170385\n",
      "batch 658: loss 0.095491\n",
      "batch 659: loss 0.323914\n",
      "batch 660: loss 0.154037\n",
      "batch 661: loss 0.312360\n",
      "batch 662: loss 0.270184\n",
      "batch 663: loss 0.403805\n",
      "batch 664: loss 0.216047\n",
      "batch 665: loss 0.144950\n",
      "batch 666: loss 0.178888\n",
      "batch 667: loss 0.132475\n",
      "batch 668: loss 0.224790\n",
      "batch 669: loss 0.195423\n",
      "batch 670: loss 0.117931\n",
      "batch 671: loss 0.213686\n",
      "batch 672: loss 0.239570\n",
      "batch 673: loss 0.324914\n",
      "batch 674: loss 0.277353\n",
      "batch 675: loss 0.179757\n",
      "batch 676: loss 0.226238\n",
      "batch 677: loss 0.189732\n",
      "batch 678: loss 0.331607\n",
      "batch 679: loss 0.177439\n",
      "batch 680: loss 0.319525\n",
      "batch 681: loss 0.540058\n",
      "batch 682: loss 0.121238\n",
      "batch 683: loss 0.145392\n",
      "batch 684: loss 0.314164\n",
      "batch 685: loss 0.167067\n",
      "batch 686: loss 0.115277\n",
      "batch 687: loss 0.371942\n",
      "batch 688: loss 0.219207\n",
      "batch 689: loss 0.300388\n",
      "batch 690: loss 0.147665\n",
      "batch 691: loss 0.155368\n",
      "batch 692: loss 0.456830\n",
      "batch 693: loss 0.336393\n",
      "batch 694: loss 0.163536\n",
      "batch 695: loss 0.202272\n",
      "batch 696: loss 0.175735\n",
      "batch 697: loss 0.205810\n",
      "batch 698: loss 0.201133\n",
      "batch 699: loss 0.192880\n",
      "batch 700: loss 0.227913\n",
      "batch 701: loss 0.267363\n",
      "batch 702: loss 0.168193\n",
      "batch 703: loss 0.083928\n",
      "batch 704: loss 0.159351\n",
      "batch 705: loss 0.321591\n",
      "batch 706: loss 0.417456\n",
      "batch 707: loss 0.438220\n",
      "batch 708: loss 0.078200\n",
      "batch 709: loss 0.137751\n",
      "batch 710: loss 0.152043\n",
      "batch 711: loss 0.152970\n",
      "batch 712: loss 0.261417\n",
      "batch 713: loss 0.109375\n",
      "batch 714: loss 0.158964\n",
      "batch 715: loss 0.130498\n",
      "batch 716: loss 0.157853\n",
      "batch 717: loss 0.254113\n",
      "batch 718: loss 0.166364\n",
      "batch 719: loss 0.100319\n",
      "batch 720: loss 0.108183\n",
      "batch 721: loss 0.181813\n",
      "batch 722: loss 0.140630\n",
      "batch 723: loss 0.164404\n",
      "batch 724: loss 0.235440\n",
      "batch 725: loss 0.268508\n",
      "batch 726: loss 0.264009\n",
      "batch 727: loss 0.297161\n",
      "batch 728: loss 0.080496\n",
      "batch 729: loss 0.312915\n",
      "batch 730: loss 0.214967\n",
      "batch 731: loss 0.293996\n",
      "batch 732: loss 0.440607\n",
      "batch 733: loss 0.288052\n",
      "batch 734: loss 0.229589\n",
      "batch 735: loss 0.297830\n",
      "batch 736: loss 0.106998\n",
      "batch 737: loss 0.237698\n",
      "batch 738: loss 0.148307\n",
      "batch 739: loss 0.250484\n",
      "batch 740: loss 0.194156\n",
      "batch 741: loss 0.104519\n",
      "batch 742: loss 0.215527\n",
      "batch 743: loss 0.074824\n",
      "batch 744: loss 0.200123\n",
      "batch 745: loss 0.105841\n",
      "batch 746: loss 0.184862\n",
      "batch 747: loss 0.163841\n",
      "batch 748: loss 0.123798\n",
      "batch 749: loss 0.310452\n",
      "batch 750: loss 0.371346\n",
      "batch 751: loss 0.144513\n",
      "batch 752: loss 0.157338\n",
      "batch 753: loss 0.132704\n",
      "batch 754: loss 0.173078\n",
      "batch 755: loss 0.158835\n",
      "batch 756: loss 0.071896\n",
      "batch 757: loss 0.211651\n",
      "batch 758: loss 0.338495\n",
      "batch 759: loss 0.227346\n",
      "batch 760: loss 0.218409\n",
      "batch 761: loss 0.171919\n",
      "batch 762: loss 0.384678\n",
      "batch 763: loss 0.177737\n",
      "batch 764: loss 0.182984\n",
      "batch 765: loss 0.212822\n",
      "batch 766: loss 0.124465\n",
      "batch 767: loss 0.080491\n",
      "batch 768: loss 0.100685\n",
      "batch 769: loss 0.076850\n",
      "batch 770: loss 0.134198\n",
      "batch 771: loss 0.156875\n",
      "batch 772: loss 0.133515\n",
      "batch 773: loss 0.293297\n",
      "batch 774: loss 0.260960\n",
      "batch 775: loss 0.038511\n",
      "batch 776: loss 0.211371\n",
      "batch 777: loss 0.165257\n",
      "batch 778: loss 0.240721\n",
      "batch 779: loss 0.044642\n",
      "batch 780: loss 0.178892\n",
      "batch 781: loss 0.112523\n",
      "batch 782: loss 0.176943\n",
      "batch 783: loss 0.538677\n",
      "batch 784: loss 0.063053\n",
      "batch 785: loss 0.109134\n",
      "batch 786: loss 0.149438\n",
      "batch 787: loss 0.194402\n",
      "batch 788: loss 0.193097\n",
      "batch 789: loss 0.280141\n",
      "batch 790: loss 0.182824\n",
      "batch 791: loss 0.115073\n",
      "batch 792: loss 0.156906\n",
      "batch 793: loss 0.215592\n",
      "batch 794: loss 0.185763\n",
      "batch 795: loss 0.238896\n",
      "batch 796: loss 0.188019\n",
      "batch 797: loss 0.084063\n",
      "batch 798: loss 0.314294\n",
      "batch 799: loss 0.177776\n",
      "batch 800: loss 0.162917\n",
      "batch 801: loss 0.100647\n",
      "batch 802: loss 0.113916\n",
      "batch 803: loss 0.128082\n",
      "batch 804: loss 0.129829\n",
      "batch 805: loss 0.183813\n",
      "batch 806: loss 0.164291\n",
      "batch 807: loss 0.108616\n",
      "batch 808: loss 0.134681\n",
      "batch 809: loss 0.237201\n",
      "batch 810: loss 0.283845\n",
      "batch 811: loss 0.146295\n",
      "batch 812: loss 0.223140\n",
      "batch 813: loss 0.288673\n",
      "batch 814: loss 0.210261\n",
      "batch 815: loss 0.220938\n",
      "batch 816: loss 0.327711\n",
      "batch 817: loss 0.138013\n",
      "batch 818: loss 0.328435\n",
      "batch 819: loss 0.310538\n",
      "batch 820: loss 0.082886\n",
      "batch 821: loss 0.150008\n",
      "batch 822: loss 0.123550\n",
      "batch 823: loss 0.185391\n",
      "batch 824: loss 0.110699\n",
      "batch 825: loss 0.200672\n",
      "batch 826: loss 0.237443\n",
      "batch 827: loss 0.362472\n",
      "batch 828: loss 0.175541\n",
      "batch 829: loss 0.085807\n",
      "batch 830: loss 0.064288\n",
      "batch 831: loss 0.157509\n",
      "batch 832: loss 0.163385\n",
      "batch 833: loss 0.204599\n",
      "batch 834: loss 0.135695\n",
      "batch 835: loss 0.381620\n",
      "batch 836: loss 0.369887\n",
      "batch 837: loss 0.195260\n",
      "batch 838: loss 0.324370\n",
      "batch 839: loss 0.247276\n",
      "batch 840: loss 0.175467\n",
      "batch 841: loss 0.314412\n",
      "batch 842: loss 0.471359\n",
      "batch 843: loss 0.140010\n",
      "batch 844: loss 0.086730\n",
      "batch 845: loss 0.086319\n",
      "batch 846: loss 0.239414\n",
      "batch 847: loss 0.194368\n",
      "batch 848: loss 0.168209\n",
      "batch 849: loss 0.471059\n",
      "batch 850: loss 0.230824\n",
      "batch 851: loss 0.258346\n",
      "batch 852: loss 0.090041\n",
      "batch 853: loss 0.325965\n",
      "batch 854: loss 0.194710\n",
      "batch 855: loss 0.198619\n",
      "batch 856: loss 0.254190\n",
      "batch 857: loss 0.269667\n",
      "batch 858: loss 0.320308\n",
      "batch 859: loss 0.078012\n",
      "batch 860: loss 0.165669\n",
      "batch 861: loss 0.174323\n",
      "batch 862: loss 0.222294\n",
      "batch 863: loss 0.154555\n",
      "batch 864: loss 0.112172\n",
      "batch 865: loss 0.399970\n",
      "batch 866: loss 0.239458\n",
      "batch 867: loss 0.125312\n",
      "batch 868: loss 0.238277\n",
      "batch 869: loss 0.188606\n",
      "batch 870: loss 0.118498\n",
      "batch 871: loss 0.123575\n",
      "batch 872: loss 0.224498\n",
      "batch 873: loss 0.239498\n",
      "batch 874: loss 0.169682\n",
      "batch 875: loss 0.170748\n",
      "batch 876: loss 0.180431\n",
      "batch 877: loss 0.273081\n",
      "batch 878: loss 0.031358\n",
      "batch 879: loss 0.110250\n",
      "batch 880: loss 0.113411\n",
      "batch 881: loss 0.233124\n",
      "batch 882: loss 0.156672\n",
      "batch 883: loss 0.284127\n",
      "batch 884: loss 0.115829\n",
      "batch 885: loss 0.192347\n",
      "batch 886: loss 0.269877\n",
      "batch 887: loss 0.145102\n",
      "batch 888: loss 0.140089\n",
      "batch 889: loss 0.098911\n",
      "batch 890: loss 0.406605\n",
      "batch 891: loss 0.136949\n",
      "batch 892: loss 0.175568\n",
      "batch 893: loss 0.220403\n",
      "batch 894: loss 0.432560\n",
      "batch 895: loss 0.168777\n",
      "batch 896: loss 0.158229\n",
      "batch 897: loss 0.135564\n",
      "batch 898: loss 0.187292\n",
      "batch 899: loss 0.124155\n",
      "batch 900: loss 0.292301\n",
      "batch 901: loss 0.198061\n",
      "batch 902: loss 0.247908\n",
      "batch 903: loss 0.289510\n",
      "batch 904: loss 0.157585\n",
      "batch 905: loss 0.314723\n",
      "batch 906: loss 0.120144\n",
      "batch 907: loss 0.178633\n",
      "batch 908: loss 0.068287\n",
      "batch 909: loss 0.126974\n",
      "batch 910: loss 0.161855\n",
      "batch 911: loss 0.170881\n",
      "batch 912: loss 0.191260\n",
      "batch 913: loss 0.208061\n",
      "batch 914: loss 0.157195\n",
      "batch 915: loss 0.258264\n",
      "batch 916: loss 0.460594\n",
      "batch 917: loss 0.211428\n",
      "batch 918: loss 0.274929\n",
      "batch 919: loss 0.240704\n",
      "batch 920: loss 0.109329\n",
      "batch 921: loss 0.216306\n",
      "batch 922: loss 0.391669\n",
      "batch 923: loss 0.143868\n",
      "batch 924: loss 0.144268\n",
      "batch 925: loss 0.152236\n",
      "batch 926: loss 0.169767\n",
      "batch 927: loss 0.282100\n",
      "batch 928: loss 0.181998\n",
      "batch 929: loss 0.180046\n",
      "batch 930: loss 0.179609\n",
      "batch 931: loss 0.113545\n",
      "batch 932: loss 0.233955\n",
      "batch 933: loss 0.106506\n",
      "batch 934: loss 0.095033\n",
      "batch 935: loss 0.196667\n",
      "batch 936: loss 0.129078\n",
      "batch 937: loss 0.180661\n",
      "batch 938: loss 0.191564\n",
      "batch 939: loss 0.169161\n",
      "batch 940: loss 0.107748\n",
      "batch 941: loss 0.163616\n",
      "batch 942: loss 0.192058\n",
      "batch 943: loss 0.099303\n",
      "batch 944: loss 0.084356\n",
      "batch 945: loss 0.339372\n",
      "batch 946: loss 0.175955\n",
      "batch 947: loss 0.137282\n",
      "batch 948: loss 0.090407\n",
      "batch 949: loss 0.104230\n",
      "batch 950: loss 0.208837\n",
      "batch 951: loss 0.121308\n",
      "batch 952: loss 0.343184\n",
      "batch 953: loss 0.280734\n",
      "batch 954: loss 0.080850\n",
      "batch 955: loss 0.121517\n",
      "batch 956: loss 0.169375\n",
      "batch 957: loss 0.209105\n",
      "batch 958: loss 0.112612\n",
      "batch 959: loss 0.272085\n",
      "batch 960: loss 0.356261\n",
      "batch 961: loss 0.102695\n",
      "batch 962: loss 0.240325\n",
      "batch 963: loss 0.204703\n",
      "batch 964: loss 0.202937\n",
      "batch 965: loss 0.141103\n",
      "batch 966: loss 0.178552\n",
      "batch 967: loss 0.204979\n",
      "batch 968: loss 0.257853\n",
      "batch 969: loss 0.117666\n",
      "batch 970: loss 0.021699\n",
      "batch 971: loss 0.082136\n",
      "batch 972: loss 0.189667\n",
      "batch 973: loss 0.207676\n",
      "batch 974: loss 0.136064\n",
      "batch 975: loss 0.102544\n",
      "batch 976: loss 0.146631\n",
      "batch 977: loss 0.221121\n",
      "batch 978: loss 0.079233\n",
      "batch 979: loss 0.138260\n",
      "batch 980: loss 0.134813\n",
      "batch 981: loss 0.290689\n",
      "batch 982: loss 0.128905\n",
      "batch 983: loss 0.213249\n",
      "batch 984: loss 0.222920\n",
      "batch 985: loss 0.188146\n",
      "batch 986: loss 0.258527\n",
      "batch 987: loss 0.096181\n",
      "batch 988: loss 0.201863\n",
      "batch 989: loss 0.166676\n",
      "batch 990: loss 0.162051\n",
      "batch 991: loss 0.279927\n",
      "batch 992: loss 0.209215\n",
      "batch 993: loss 0.254597\n",
      "batch 994: loss 0.296894\n",
      "batch 995: loss 0.104790\n",
      "batch 996: loss 0.104011\n",
      "batch 997: loss 0.206362\n",
      "batch 998: loss 0.176163\n",
      "batch 999: loss 0.220460\n",
      "batch 1000: loss 0.138666\n",
      "batch 1001: loss 0.232462\n",
      "batch 1002: loss 0.215436\n",
      "batch 1003: loss 0.149038\n",
      "batch 1004: loss 0.148493\n",
      "batch 1005: loss 0.119620\n",
      "batch 1006: loss 0.181372\n",
      "batch 1007: loss 0.276202\n",
      "batch 1008: loss 0.071502\n",
      "batch 1009: loss 0.174446\n",
      "batch 1010: loss 0.251502\n",
      "batch 1011: loss 0.070996\n",
      "batch 1012: loss 0.093021\n",
      "batch 1013: loss 0.170296\n",
      "batch 1014: loss 0.175844\n",
      "batch 1015: loss 0.147595\n",
      "batch 1016: loss 0.135973\n",
      "batch 1017: loss 0.147784\n",
      "batch 1018: loss 0.058547\n",
      "batch 1019: loss 0.324749\n",
      "batch 1020: loss 0.339866\n",
      "batch 1021: loss 0.186319\n",
      "batch 1022: loss 0.203350\n",
      "batch 1023: loss 0.098364\n",
      "batch 1024: loss 0.098233\n",
      "batch 1025: loss 0.131216\n",
      "batch 1026: loss 0.185351\n",
      "batch 1027: loss 0.125748\n",
      "batch 1028: loss 0.427847\n",
      "batch 1029: loss 0.368526\n",
      "batch 1030: loss 0.035844\n",
      "batch 1031: loss 0.195922\n",
      "batch 1032: loss 0.182912\n",
      "batch 1033: loss 0.052108\n",
      "batch 1034: loss 0.082385\n",
      "batch 1035: loss 0.135445\n",
      "batch 1036: loss 0.176769\n",
      "batch 1037: loss 0.186349\n",
      "batch 1038: loss 0.068370\n",
      "batch 1039: loss 0.133789\n",
      "batch 1040: loss 0.073065\n",
      "batch 1041: loss 0.087851\n",
      "batch 1042: loss 0.128930\n",
      "batch 1043: loss 0.126191\n",
      "batch 1044: loss 0.172846\n",
      "batch 1045: loss 0.227220\n",
      "batch 1046: loss 0.299912\n",
      "batch 1047: loss 0.107792\n",
      "batch 1048: loss 0.093745\n",
      "batch 1049: loss 0.117907\n",
      "batch 1050: loss 0.129746\n",
      "batch 1051: loss 0.132958\n",
      "batch 1052: loss 0.247934\n",
      "batch 1053: loss 0.264671\n",
      "batch 1054: loss 0.158087\n",
      "batch 1055: loss 0.180548\n",
      "batch 1056: loss 0.149717\n",
      "batch 1057: loss 0.054585\n",
      "batch 1058: loss 0.168766\n",
      "batch 1059: loss 0.351527\n",
      "batch 1060: loss 0.130560\n",
      "batch 1061: loss 0.241136\n",
      "batch 1062: loss 0.255000\n",
      "batch 1063: loss 0.239051\n",
      "batch 1064: loss 0.102525\n",
      "batch 1065: loss 0.281316\n",
      "batch 1066: loss 0.176385\n",
      "batch 1067: loss 0.207944\n",
      "batch 1068: loss 0.198375\n",
      "batch 1069: loss 0.318428\n",
      "batch 1070: loss 0.203082\n",
      "batch 1071: loss 0.106199\n",
      "batch 1072: loss 0.157041\n",
      "batch 1073: loss 0.170261\n",
      "batch 1074: loss 0.129346\n",
      "batch 1075: loss 0.143666\n",
      "batch 1076: loss 0.205214\n",
      "batch 1077: loss 0.146067\n",
      "batch 1078: loss 0.254989\n",
      "batch 1079: loss 0.535089\n",
      "batch 1080: loss 0.217354\n",
      "batch 1081: loss 0.181673\n",
      "batch 1082: loss 0.232590\n",
      "batch 1083: loss 0.360583\n",
      "batch 1084: loss 0.160273\n",
      "batch 1085: loss 0.161109\n",
      "batch 1086: loss 0.134746\n",
      "batch 1087: loss 0.046144\n",
      "batch 1088: loss 0.169560\n",
      "batch 1089: loss 0.198816\n",
      "batch 1090: loss 0.317399\n",
      "batch 1091: loss 0.339795\n",
      "batch 1092: loss 0.097456\n",
      "batch 1093: loss 0.139695\n",
      "batch 1094: loss 0.087433\n",
      "batch 1095: loss 0.161306\n",
      "batch 1096: loss 0.100357\n",
      "batch 1097: loss 0.236427\n",
      "batch 1098: loss 0.067601\n",
      "batch 1099: loss 0.084234\n",
      "batch 1100: loss 0.091597\n",
      "batch 1101: loss 0.093790\n",
      "batch 1102: loss 0.104082\n",
      "batch 1103: loss 0.159543\n",
      "batch 1104: loss 0.201215\n",
      "batch 1105: loss 0.237402\n",
      "batch 1106: loss 0.050283\n",
      "batch 1107: loss 0.129134\n",
      "batch 1108: loss 0.275541\n",
      "batch 1109: loss 0.325103\n",
      "batch 1110: loss 0.253848\n",
      "batch 1111: loss 0.127430\n",
      "batch 1112: loss 0.110347\n",
      "batch 1113: loss 0.170558\n",
      "batch 1114: loss 0.145356\n",
      "batch 1115: loss 0.218419\n",
      "batch 1116: loss 0.091767\n",
      "batch 1117: loss 0.244820\n",
      "batch 1118: loss 0.100179\n",
      "batch 1119: loss 0.122718\n",
      "batch 1120: loss 0.197390\n",
      "batch 1121: loss 0.073869\n",
      "batch 1122: loss 0.135315\n",
      "batch 1123: loss 0.210934\n",
      "batch 1124: loss 0.075435\n",
      "batch 1125: loss 0.116353\n",
      "batch 1126: loss 0.068331\n",
      "batch 1127: loss 0.143164\n",
      "batch 1128: loss 0.462219\n",
      "batch 1129: loss 0.248403\n",
      "batch 1130: loss 0.194935\n",
      "batch 1131: loss 0.218186\n",
      "batch 1132: loss 0.188459\n",
      "batch 1133: loss 0.099061\n",
      "batch 1134: loss 0.130838\n",
      "batch 1135: loss 0.210519\n",
      "batch 1136: loss 0.187200\n",
      "batch 1137: loss 0.092914\n",
      "batch 1138: loss 0.388476\n",
      "batch 1139: loss 0.162145\n",
      "batch 1140: loss 0.142495\n",
      "batch 1141: loss 0.097155\n",
      "batch 1142: loss 0.065978\n",
      "batch 1143: loss 0.120245\n",
      "batch 1144: loss 0.088153\n",
      "batch 1145: loss 0.158444\n",
      "batch 1146: loss 0.056458\n",
      "batch 1147: loss 0.223466\n",
      "batch 1148: loss 0.092606\n",
      "batch 1149: loss 0.033477\n",
      "batch 1150: loss 0.219478\n",
      "batch 1151: loss 0.248453\n",
      "batch 1152: loss 0.264602\n",
      "batch 1153: loss 0.178139\n",
      "batch 1154: loss 0.076046\n",
      "batch 1155: loss 0.081555\n",
      "batch 1156: loss 0.120108\n",
      "batch 1157: loss 0.110922\n",
      "batch 1158: loss 0.115482\n",
      "batch 1159: loss 0.090505\n",
      "batch 1160: loss 0.114980\n",
      "batch 1161: loss 0.133381\n",
      "batch 1162: loss 0.246948\n",
      "batch 1163: loss 0.062741\n",
      "batch 1164: loss 0.087058\n",
      "batch 1165: loss 0.120659\n",
      "batch 1166: loss 0.179565\n",
      "batch 1167: loss 0.184343\n",
      "batch 1168: loss 0.153437\n",
      "batch 1169: loss 0.104794\n",
      "batch 1170: loss 0.159870\n",
      "batch 1171: loss 0.161550\n",
      "batch 1172: loss 0.107480\n",
      "batch 1173: loss 0.113142\n",
      "batch 1174: loss 0.212221\n",
      "batch 1175: loss 0.214981\n",
      "batch 1176: loss 0.231232\n",
      "batch 1177: loss 0.076217\n",
      "batch 1178: loss 0.153996\n",
      "batch 1179: loss 0.119698\n",
      "batch 1180: loss 0.197067\n",
      "batch 1181: loss 0.122925\n",
      "batch 1182: loss 0.191349\n",
      "batch 1183: loss 0.063158\n",
      "batch 1184: loss 0.098691\n",
      "batch 1185: loss 0.232888\n",
      "batch 1186: loss 0.186843\n",
      "batch 1187: loss 0.091508\n",
      "batch 1188: loss 0.136443\n",
      "batch 1189: loss 0.157741\n",
      "batch 1190: loss 0.191978\n",
      "batch 1191: loss 0.186181\n",
      "batch 1192: loss 0.187822\n",
      "batch 1193: loss 0.188005\n",
      "batch 1194: loss 0.142032\n",
      "batch 1195: loss 0.154770\n",
      "batch 1196: loss 0.145284\n",
      "batch 1197: loss 0.073870\n",
      "batch 1198: loss 0.234134\n",
      "batch 1199: loss 0.122167\n",
      "batch 1200: loss 0.107228\n",
      "batch 1201: loss 0.063324\n",
      "batch 1202: loss 0.034798\n",
      "batch 1203: loss 0.109866\n",
      "batch 1204: loss 0.161934\n",
      "batch 1205: loss 0.110674\n",
      "batch 1206: loss 0.362006\n",
      "batch 1207: loss 0.121388\n",
      "batch 1208: loss 0.472212\n",
      "batch 1209: loss 0.048529\n",
      "batch 1210: loss 0.221373\n",
      "batch 1211: loss 0.061213\n",
      "batch 1212: loss 0.101107\n",
      "batch 1213: loss 0.117609\n",
      "batch 1214: loss 0.087756\n",
      "batch 1215: loss 0.080343\n",
      "batch 1216: loss 0.093955\n",
      "batch 1217: loss 0.114010\n",
      "batch 1218: loss 0.111440\n",
      "batch 1219: loss 0.033033\n",
      "batch 1220: loss 0.203536\n",
      "batch 1221: loss 0.135806\n",
      "batch 1222: loss 0.192037\n",
      "batch 1223: loss 0.262982\n",
      "batch 1224: loss 0.183653\n",
      "batch 1225: loss 0.166333\n",
      "batch 1226: loss 0.134710\n",
      "batch 1227: loss 0.175311\n",
      "batch 1228: loss 0.070379\n",
      "batch 1229: loss 0.224072\n",
      "batch 1230: loss 0.240153\n",
      "batch 1231: loss 0.167542\n",
      "batch 1232: loss 0.114350\n",
      "batch 1233: loss 0.364274\n",
      "batch 1234: loss 0.312259\n",
      "batch 1235: loss 0.142336\n",
      "batch 1236: loss 0.089089\n",
      "batch 1237: loss 0.055795\n",
      "batch 1238: loss 0.081965\n",
      "batch 1239: loss 0.237436\n",
      "batch 1240: loss 0.068729\n",
      "batch 1241: loss 0.095559\n",
      "batch 1242: loss 0.141758\n",
      "batch 1243: loss 0.080952\n",
      "batch 1244: loss 0.119405\n",
      "batch 1245: loss 0.180284\n",
      "batch 1246: loss 0.297969\n",
      "batch 1247: loss 0.186465\n",
      "batch 1248: loss 0.103358\n",
      "batch 1249: loss 0.105691\n",
      "batch 1250: loss 0.148330\n",
      "batch 1251: loss 0.159595\n",
      "batch 1252: loss 0.214631\n",
      "batch 1253: loss 0.172202\n",
      "batch 1254: loss 0.116933\n",
      "batch 1255: loss 0.212314\n",
      "batch 1256: loss 0.068192\n",
      "batch 1257: loss 0.140084\n",
      "batch 1258: loss 0.123594\n",
      "batch 1259: loss 0.127125\n",
      "batch 1260: loss 0.179689\n",
      "batch 1261: loss 0.110380\n",
      "batch 1262: loss 0.322736\n",
      "batch 1263: loss 0.056582\n",
      "batch 1264: loss 0.202322\n",
      "batch 1265: loss 0.191334\n",
      "batch 1266: loss 0.253311\n",
      "batch 1267: loss 0.089541\n",
      "batch 1268: loss 0.128391\n",
      "batch 1269: loss 0.069287\n",
      "batch 1270: loss 0.155406\n",
      "batch 1271: loss 0.202409\n",
      "batch 1272: loss 0.030685\n",
      "batch 1273: loss 0.224756\n",
      "batch 1274: loss 0.291991\n",
      "batch 1275: loss 0.341819\n",
      "batch 1276: loss 0.029457\n",
      "batch 1277: loss 0.071279\n",
      "batch 1278: loss 0.171484\n",
      "batch 1279: loss 0.168437\n",
      "batch 1280: loss 0.084292\n",
      "batch 1281: loss 0.210379\n",
      "batch 1282: loss 0.032977\n",
      "batch 1283: loss 0.150816\n",
      "batch 1284: loss 0.118699\n",
      "batch 1285: loss 0.144250\n",
      "batch 1286: loss 0.098299\n",
      "batch 1287: loss 0.063314\n",
      "batch 1288: loss 0.171007\n",
      "batch 1289: loss 0.103882\n",
      "batch 1290: loss 0.125681\n",
      "batch 1291: loss 0.330341\n",
      "batch 1292: loss 0.241462\n",
      "batch 1293: loss 0.142534\n",
      "batch 1294: loss 0.178084\n",
      "batch 1295: loss 0.261490\n",
      "batch 1296: loss 0.062339\n",
      "batch 1297: loss 0.307380\n",
      "batch 1298: loss 0.141012\n",
      "batch 1299: loss 0.051933\n",
      "batch 1300: loss 0.361791\n",
      "batch 1301: loss 0.048003\n",
      "batch 1302: loss 0.144576\n",
      "batch 1303: loss 0.068129\n",
      "batch 1304: loss 0.258347\n",
      "batch 1305: loss 0.384914\n",
      "batch 1306: loss 0.055938\n",
      "batch 1307: loss 0.265467\n",
      "batch 1308: loss 0.158251\n",
      "batch 1309: loss 0.055234\n",
      "batch 1310: loss 0.259495\n",
      "batch 1311: loss 0.141787\n",
      "batch 1312: loss 0.302230\n",
      "batch 1313: loss 0.150121\n",
      "batch 1314: loss 0.185753\n",
      "batch 1315: loss 0.085220\n",
      "batch 1316: loss 0.055591\n",
      "batch 1317: loss 0.053960\n",
      "batch 1318: loss 0.082630\n",
      "batch 1319: loss 0.224468\n",
      "batch 1320: loss 0.249781\n",
      "batch 1321: loss 0.107411\n",
      "batch 1322: loss 0.144587\n",
      "batch 1323: loss 0.107430\n",
      "batch 1324: loss 0.076201\n",
      "batch 1325: loss 0.133592\n",
      "batch 1326: loss 0.091265\n",
      "batch 1327: loss 0.390584\n",
      "batch 1328: loss 0.163084\n",
      "batch 1329: loss 0.139063\n",
      "batch 1330: loss 0.169937\n",
      "batch 1331: loss 0.031732\n",
      "batch 1332: loss 0.156046\n",
      "batch 1333: loss 0.043295\n",
      "batch 1334: loss 0.050193\n",
      "batch 1335: loss 0.290911\n",
      "batch 1336: loss 0.190885\n",
      "batch 1337: loss 0.085558\n",
      "batch 1338: loss 0.207170\n",
      "batch 1339: loss 0.245735\n",
      "batch 1340: loss 0.088277\n",
      "batch 1341: loss 0.088142\n",
      "batch 1342: loss 0.133239\n",
      "batch 1343: loss 0.154694\n",
      "batch 1344: loss 0.080001\n",
      "batch 1345: loss 0.129889\n",
      "batch 1346: loss 0.273935\n",
      "batch 1347: loss 0.277739\n",
      "batch 1348: loss 0.123469\n",
      "batch 1349: loss 0.157994\n",
      "batch 1350: loss 0.139696\n",
      "batch 1351: loss 0.283813\n",
      "batch 1352: loss 0.058199\n",
      "batch 1353: loss 0.160052\n",
      "batch 1354: loss 0.086011\n",
      "batch 1355: loss 0.210488\n",
      "batch 1356: loss 0.047228\n",
      "batch 1357: loss 0.072610\n",
      "batch 1358: loss 0.222081\n",
      "batch 1359: loss 0.169043\n",
      "batch 1360: loss 0.087316\n",
      "batch 1361: loss 0.154445\n",
      "batch 1362: loss 0.092046\n",
      "batch 1363: loss 0.112031\n",
      "batch 1364: loss 0.151306\n",
      "batch 1365: loss 0.085126\n",
      "batch 1366: loss 0.080225\n",
      "batch 1367: loss 0.151962\n",
      "batch 1368: loss 0.135536\n",
      "batch 1369: loss 0.137246\n",
      "batch 1370: loss 0.171885\n",
      "batch 1371: loss 0.141152\n",
      "batch 1372: loss 0.282557\n",
      "batch 1373: loss 0.037246\n",
      "batch 1374: loss 0.099543\n",
      "batch 1375: loss 0.151789\n",
      "batch 1376: loss 0.081307\n",
      "batch 1377: loss 0.188652\n",
      "batch 1378: loss 0.036266\n",
      "batch 1379: loss 0.119196\n",
      "batch 1380: loss 0.044048\n",
      "batch 1381: loss 0.135665\n",
      "batch 1382: loss 0.173165\n",
      "batch 1383: loss 0.050850\n",
      "batch 1384: loss 0.261994\n",
      "batch 1385: loss 0.185594\n",
      "batch 1386: loss 0.173237\n",
      "batch 1387: loss 0.279118\n",
      "batch 1388: loss 0.058705\n",
      "batch 1389: loss 0.068016\n",
      "batch 1390: loss 0.032275\n",
      "batch 1391: loss 0.252817\n",
      "batch 1392: loss 0.114830\n",
      "batch 1393: loss 0.029507\n",
      "batch 1394: loss 0.079663\n",
      "batch 1395: loss 0.064228\n",
      "batch 1396: loss 0.160122\n",
      "batch 1397: loss 0.059847\n",
      "batch 1398: loss 0.053812\n",
      "batch 1399: loss 0.170517\n",
      "batch 1400: loss 0.259576\n",
      "batch 1401: loss 0.193053\n",
      "batch 1402: loss 0.137140\n",
      "batch 1403: loss 0.344881\n",
      "batch 1404: loss 0.360161\n",
      "batch 1405: loss 0.127663\n",
      "batch 1406: loss 0.109174\n",
      "batch 1407: loss 0.094370\n",
      "batch 1408: loss 0.072728\n",
      "batch 1409: loss 0.097802\n",
      "batch 1410: loss 0.163746\n",
      "batch 1411: loss 0.257156\n",
      "batch 1412: loss 0.122540\n",
      "batch 1413: loss 0.079050\n",
      "batch 1414: loss 0.350976\n",
      "batch 1415: loss 0.139829\n",
      "batch 1416: loss 0.028829\n",
      "batch 1417: loss 0.117016\n",
      "batch 1418: loss 0.063808\n",
      "batch 1419: loss 0.182022\n",
      "batch 1420: loss 0.345445\n",
      "batch 1421: loss 0.142754\n",
      "batch 1422: loss 0.193888\n",
      "batch 1423: loss 0.162445\n",
      "batch 1424: loss 0.359837\n",
      "batch 1425: loss 0.057667\n",
      "batch 1426: loss 0.163886\n",
      "batch 1427: loss 0.337348\n",
      "batch 1428: loss 0.195105\n",
      "batch 1429: loss 0.083545\n",
      "batch 1430: loss 0.133760\n",
      "batch 1431: loss 0.098503\n",
      "batch 1432: loss 0.198420\n",
      "batch 1433: loss 0.082814\n",
      "batch 1434: loss 0.140167\n",
      "batch 1435: loss 0.053670\n",
      "batch 1436: loss 0.100415\n",
      "batch 1437: loss 0.257319\n",
      "batch 1438: loss 0.130716\n",
      "batch 1439: loss 0.025078\n",
      "batch 1440: loss 0.052474\n",
      "batch 1441: loss 0.185340\n",
      "batch 1442: loss 0.095592\n",
      "batch 1443: loss 0.069189\n",
      "batch 1444: loss 0.176110\n",
      "batch 1445: loss 0.178282\n",
      "batch 1446: loss 0.185776\n",
      "batch 1447: loss 0.115950\n",
      "batch 1448: loss 0.107156\n",
      "batch 1449: loss 0.184366\n",
      "batch 1450: loss 0.050241\n",
      "batch 1451: loss 0.183928\n",
      "batch 1452: loss 0.110411\n",
      "batch 1453: loss 0.230355\n",
      "batch 1454: loss 0.100080\n",
      "batch 1455: loss 0.212543\n",
      "batch 1456: loss 0.176623\n",
      "batch 1457: loss 0.142803\n",
      "batch 1458: loss 0.155558\n",
      "batch 1459: loss 0.303428\n",
      "batch 1460: loss 0.055818\n",
      "batch 1461: loss 0.226573\n",
      "batch 1462: loss 0.136329\n",
      "batch 1463: loss 0.149069\n",
      "batch 1464: loss 0.437548\n",
      "batch 1465: loss 0.040567\n",
      "batch 1466: loss 0.085210\n",
      "batch 1467: loss 0.060939\n",
      "batch 1468: loss 0.132270\n",
      "batch 1469: loss 0.077763\n",
      "batch 1470: loss 0.065014\n",
      "batch 1471: loss 0.064281\n",
      "batch 1472: loss 0.215483\n",
      "batch 1473: loss 0.163932\n",
      "batch 1474: loss 0.417772\n",
      "batch 1475: loss 0.047052\n",
      "batch 1476: loss 0.070857\n",
      "batch 1477: loss 0.124708\n",
      "batch 1478: loss 0.270550\n",
      "batch 1479: loss 0.032702\n",
      "batch 1480: loss 0.086232\n",
      "batch 1481: loss 0.138286\n",
      "batch 1482: loss 0.056372\n",
      "batch 1483: loss 0.131737\n",
      "batch 1484: loss 0.334436\n",
      "batch 1485: loss 0.101444\n",
      "batch 1486: loss 0.165827\n",
      "batch 1487: loss 0.071117\n",
      "batch 1488: loss 0.119090\n",
      "batch 1489: loss 0.296865\n",
      "batch 1490: loss 0.213656\n",
      "batch 1491: loss 0.085284\n",
      "batch 1492: loss 0.031889\n",
      "batch 1493: loss 0.127303\n",
      "batch 1494: loss 0.203906\n",
      "batch 1495: loss 0.151558\n",
      "batch 1496: loss 0.066750\n",
      "batch 1497: loss 0.248934\n",
      "batch 1498: loss 0.049874\n",
      "batch 1499: loss 0.045248\n",
      "batch 1500: loss 0.087417\n",
      "batch 1501: loss 0.128172\n",
      "batch 1502: loss 0.122195\n",
      "batch 1503: loss 0.113896\n",
      "batch 1504: loss 0.049194\n",
      "batch 1505: loss 0.113808\n",
      "batch 1506: loss 0.094939\n",
      "batch 1507: loss 0.154252\n",
      "batch 1508: loss 0.172425\n",
      "batch 1509: loss 0.054204\n",
      "batch 1510: loss 0.351977\n",
      "batch 1511: loss 0.214777\n",
      "batch 1512: loss 0.080277\n",
      "batch 1513: loss 0.131338\n",
      "batch 1514: loss 0.275682\n",
      "batch 1515: loss 0.022547\n",
      "batch 1516: loss 0.066967\n",
      "batch 1517: loss 0.091083\n",
      "batch 1518: loss 0.214411\n",
      "batch 1519: loss 0.161609\n",
      "batch 1520: loss 0.180450\n",
      "batch 1521: loss 0.097383\n",
      "batch 1522: loss 0.123114\n",
      "batch 1523: loss 0.037233\n",
      "batch 1524: loss 0.145895\n",
      "batch 1525: loss 0.191572\n",
      "batch 1526: loss 0.192598\n",
      "batch 1527: loss 0.189576\n",
      "batch 1528: loss 0.118798\n",
      "batch 1529: loss 0.144134\n",
      "batch 1530: loss 0.064534\n",
      "batch 1531: loss 0.124606\n",
      "batch 1532: loss 0.088964\n",
      "batch 1533: loss 0.103759\n",
      "batch 1534: loss 0.305314\n",
      "batch 1535: loss 0.224670\n",
      "batch 1536: loss 0.100723\n",
      "batch 1537: loss 0.173076\n",
      "batch 1538: loss 0.092386\n",
      "batch 1539: loss 0.022653\n",
      "batch 1540: loss 0.133154\n",
      "batch 1541: loss 0.093873\n",
      "batch 1542: loss 0.210076\n",
      "batch 1543: loss 0.295916\n",
      "batch 1544: loss 0.134357\n",
      "batch 1545: loss 0.218940\n",
      "batch 1546: loss 0.095173\n",
      "batch 1547: loss 0.170611\n",
      "batch 1548: loss 0.133850\n",
      "batch 1549: loss 0.070058\n",
      "batch 1550: loss 0.158770\n",
      "batch 1551: loss 0.095805\n",
      "batch 1552: loss 0.133884\n",
      "batch 1553: loss 0.029451\n",
      "batch 1554: loss 0.250166\n",
      "batch 1555: loss 0.044110\n",
      "batch 1556: loss 0.240655\n",
      "batch 1557: loss 0.085254\n",
      "batch 1558: loss 0.089822\n",
      "batch 1559: loss 0.116192\n",
      "batch 1560: loss 0.190133\n",
      "batch 1561: loss 0.162132\n",
      "batch 1562: loss 0.092267\n",
      "batch 1563: loss 0.103907\n",
      "batch 1564: loss 0.163749\n",
      "batch 1565: loss 0.098795\n",
      "batch 1566: loss 0.198708\n",
      "batch 1567: loss 0.170750\n",
      "batch 1568: loss 0.112201\n",
      "batch 1569: loss 0.082461\n",
      "batch 1570: loss 0.047142\n",
      "batch 1571: loss 0.123775\n",
      "batch 1572: loss 0.176067\n",
      "batch 1573: loss 0.237947\n",
      "batch 1574: loss 0.102917\n",
      "batch 1575: loss 0.136671\n",
      "batch 1576: loss 0.094706\n",
      "batch 1577: loss 0.204711\n",
      "batch 1578: loss 0.127698\n",
      "batch 1579: loss 0.065299\n",
      "batch 1580: loss 0.108304\n",
      "batch 1581: loss 0.076852\n",
      "batch 1582: loss 0.118572\n",
      "batch 1583: loss 0.139755\n",
      "batch 1584: loss 0.194683\n",
      "batch 1585: loss 0.207048\n",
      "batch 1586: loss 0.139888\n",
      "batch 1587: loss 0.044766\n",
      "batch 1588: loss 0.125022\n",
      "batch 1589: loss 0.110794\n",
      "batch 1590: loss 0.077620\n",
      "batch 1591: loss 0.090080\n",
      "batch 1592: loss 0.182449\n",
      "batch 1593: loss 0.090057\n",
      "batch 1594: loss 0.276520\n",
      "batch 1595: loss 0.091081\n",
      "batch 1596: loss 0.142375\n",
      "batch 1597: loss 0.120421\n",
      "batch 1598: loss 0.155060\n",
      "batch 1599: loss 0.100933\n",
      "batch 1600: loss 0.145618\n",
      "batch 1601: loss 0.065655\n",
      "batch 1602: loss 0.107865\n",
      "batch 1603: loss 0.345248\n",
      "batch 1604: loss 0.158954\n",
      "batch 1605: loss 0.260484\n",
      "batch 1606: loss 0.253728\n",
      "batch 1607: loss 0.138079\n",
      "batch 1608: loss 0.155000\n",
      "batch 1609: loss 0.048230\n",
      "batch 1610: loss 0.069275\n",
      "batch 1611: loss 0.090671\n",
      "batch 1612: loss 0.075188\n",
      "batch 1613: loss 0.063027\n",
      "batch 1614: loss 0.220012\n",
      "batch 1615: loss 0.084064\n",
      "batch 1616: loss 0.048357\n",
      "batch 1617: loss 0.052207\n",
      "batch 1618: loss 0.159261\n",
      "batch 1619: loss 0.061710\n",
      "batch 1620: loss 0.066839\n",
      "batch 1621: loss 0.136518\n",
      "batch 1622: loss 0.043827\n",
      "batch 1623: loss 0.053598\n",
      "batch 1624: loss 0.126999\n",
      "batch 1625: loss 0.108682\n",
      "batch 1626: loss 0.020135\n",
      "batch 1627: loss 0.155156\n",
      "batch 1628: loss 0.075319\n",
      "batch 1629: loss 0.118795\n",
      "batch 1630: loss 0.071856\n",
      "batch 1631: loss 0.083435\n",
      "batch 1632: loss 0.195742\n",
      "batch 1633: loss 0.168559\n",
      "batch 1634: loss 0.025918\n",
      "batch 1635: loss 0.153529\n",
      "batch 1636: loss 0.026054\n",
      "batch 1637: loss 0.163318\n",
      "batch 1638: loss 0.048593\n",
      "batch 1639: loss 0.153967\n",
      "batch 1640: loss 0.186356\n",
      "batch 1641: loss 0.093063\n",
      "batch 1642: loss 0.070501\n",
      "batch 1643: loss 0.053782\n",
      "batch 1644: loss 0.212328\n",
      "batch 1645: loss 0.283333\n",
      "batch 1646: loss 0.100260\n",
      "batch 1647: loss 0.138442\n",
      "batch 1648: loss 0.089640\n",
      "batch 1649: loss 0.353990\n",
      "batch 1650: loss 0.129012\n",
      "batch 1651: loss 0.054276\n",
      "batch 1652: loss 0.160330\n",
      "batch 1653: loss 0.215829\n",
      "batch 1654: loss 0.344909\n",
      "batch 1655: loss 0.071624\n",
      "batch 1656: loss 0.081826\n",
      "batch 1657: loss 0.061679\n",
      "batch 1658: loss 0.136728\n",
      "batch 1659: loss 0.073661\n",
      "batch 1660: loss 0.094860\n",
      "batch 1661: loss 0.094612\n",
      "batch 1662: loss 0.264143\n",
      "batch 1663: loss 0.153867\n",
      "batch 1664: loss 0.249913\n",
      "batch 1665: loss 0.040612\n",
      "batch 1666: loss 0.105362\n",
      "batch 1667: loss 0.035029\n",
      "batch 1668: loss 0.103014\n",
      "batch 1669: loss 0.149074\n",
      "batch 1670: loss 0.076044\n",
      "batch 1671: loss 0.085195\n",
      "batch 1672: loss 0.030560\n",
      "batch 1673: loss 0.156740\n",
      "batch 1674: loss 0.263667\n",
      "batch 1675: loss 0.151360\n",
      "batch 1676: loss 0.112327\n",
      "batch 1677: loss 0.084917\n",
      "batch 1678: loss 0.098414\n",
      "batch 1679: loss 0.159448\n",
      "batch 1680: loss 0.169606\n",
      "batch 1681: loss 0.033829\n",
      "batch 1682: loss 0.113942\n",
      "batch 1683: loss 0.154228\n",
      "batch 1684: loss 0.237595\n",
      "batch 1685: loss 0.080872\n",
      "batch 1686: loss 0.136428\n",
      "batch 1687: loss 0.084237\n",
      "batch 1688: loss 0.126107\n",
      "batch 1689: loss 0.128376\n",
      "batch 1690: loss 0.119532\n",
      "batch 1691: loss 0.105168\n",
      "batch 1692: loss 0.067768\n",
      "batch 1693: loss 0.116426\n",
      "batch 1694: loss 0.051870\n",
      "batch 1695: loss 0.149993\n",
      "batch 1696: loss 0.050453\n",
      "batch 1697: loss 0.116236\n",
      "batch 1698: loss 0.130311\n",
      "batch 1699: loss 0.152071\n",
      "batch 1700: loss 0.155537\n",
      "batch 1701: loss 0.260491\n",
      "batch 1702: loss 0.099119\n",
      "batch 1703: loss 0.097993\n",
      "batch 1704: loss 0.296663\n",
      "batch 1705: loss 0.061570\n",
      "batch 1706: loss 0.314597\n",
      "batch 1707: loss 0.176666\n",
      "batch 1708: loss 0.202027\n",
      "batch 1709: loss 0.075726\n",
      "batch 1710: loss 0.263309\n",
      "batch 1711: loss 0.068326\n",
      "batch 1712: loss 0.152447\n",
      "batch 1713: loss 0.095617\n",
      "batch 1714: loss 0.072333\n",
      "batch 1715: loss 0.064246\n",
      "batch 1716: loss 0.090335\n",
      "batch 1717: loss 0.066148\n",
      "batch 1718: loss 0.136547\n",
      "batch 1719: loss 0.039888\n",
      "batch 1720: loss 0.161083\n",
      "batch 1721: loss 0.073075\n",
      "batch 1722: loss 0.152013\n",
      "batch 1723: loss 0.243027\n",
      "batch 1724: loss 0.108066\n",
      "batch 1725: loss 0.019598\n",
      "batch 1726: loss 0.066695\n",
      "batch 1727: loss 0.052932\n",
      "batch 1728: loss 0.080787\n",
      "batch 1729: loss 0.091426\n",
      "batch 1730: loss 0.042778\n",
      "batch 1731: loss 0.251320\n",
      "batch 1732: loss 0.158310\n",
      "batch 1733: loss 0.056637\n",
      "batch 1734: loss 0.026253\n",
      "batch 1735: loss 0.082928\n",
      "batch 1736: loss 0.193389\n",
      "batch 1737: loss 0.185048\n",
      "batch 1738: loss 0.237635\n",
      "batch 1739: loss 0.136939\n",
      "batch 1740: loss 0.123352\n",
      "batch 1741: loss 0.116830\n",
      "batch 1742: loss 0.117753\n",
      "batch 1743: loss 0.113488\n",
      "batch 1744: loss 0.161601\n",
      "batch 1745: loss 0.079855\n",
      "batch 1746: loss 0.026894\n",
      "batch 1747: loss 0.291807\n",
      "batch 1748: loss 0.193879\n",
      "batch 1749: loss 0.357435\n",
      "batch 1750: loss 0.052915\n",
      "batch 1751: loss 0.127847\n",
      "batch 1752: loss 0.048110\n",
      "batch 1753: loss 0.126969\n",
      "batch 1754: loss 0.157988\n",
      "batch 1755: loss 0.295735\n",
      "batch 1756: loss 0.044600\n",
      "batch 1757: loss 0.057058\n",
      "batch 1758: loss 0.138553\n",
      "batch 1759: loss 0.068278\n",
      "batch 1760: loss 0.062719\n",
      "batch 1761: loss 0.146884\n",
      "batch 1762: loss 0.218480\n",
      "batch 1763: loss 0.071585\n",
      "batch 1764: loss 0.190008\n",
      "batch 1765: loss 0.058771\n",
      "batch 1766: loss 0.039875\n",
      "batch 1767: loss 0.077444\n",
      "batch 1768: loss 0.144071\n",
      "batch 1769: loss 0.285055\n",
      "batch 1770: loss 0.081988\n",
      "batch 1771: loss 0.327643\n",
      "batch 1772: loss 0.034080\n",
      "batch 1773: loss 0.042153\n",
      "batch 1774: loss 0.108321\n",
      "batch 1775: loss 0.344779\n",
      "batch 1776: loss 0.073221\n",
      "batch 1777: loss 0.179727\n",
      "batch 1778: loss 0.124536\n",
      "batch 1779: loss 0.141557\n",
      "batch 1780: loss 0.062052\n",
      "batch 1781: loss 0.149501\n",
      "batch 1782: loss 0.104767\n",
      "batch 1783: loss 0.194383\n",
      "batch 1784: loss 0.074660\n",
      "batch 1785: loss 0.076874\n",
      "batch 1786: loss 0.091036\n",
      "batch 1787: loss 0.207352\n",
      "batch 1788: loss 0.131307\n",
      "batch 1789: loss 0.257487\n",
      "batch 1790: loss 0.044280\n",
      "batch 1791: loss 0.154716\n",
      "batch 1792: loss 0.085490\n",
      "batch 1793: loss 0.151139\n",
      "batch 1794: loss 0.222168\n",
      "batch 1795: loss 0.295668\n",
      "batch 1796: loss 0.084220\n",
      "batch 1797: loss 0.250134\n",
      "batch 1798: loss 0.084639\n",
      "batch 1799: loss 0.071580\n",
      "batch 1800: loss 0.063587\n",
      "batch 1801: loss 0.035001\n",
      "batch 1802: loss 0.123856\n",
      "batch 1803: loss 0.187109\n",
      "batch 1804: loss 0.175095\n",
      "batch 1805: loss 0.127653\n",
      "batch 1806: loss 0.091009\n",
      "batch 1807: loss 0.050496\n",
      "batch 1808: loss 0.158725\n",
      "batch 1809: loss 0.136309\n",
      "batch 1810: loss 0.185729\n",
      "batch 1811: loss 0.143974\n",
      "batch 1812: loss 0.073348\n",
      "batch 1813: loss 0.276485\n",
      "batch 1814: loss 0.116657\n",
      "batch 1815: loss 0.138522\n",
      "batch 1816: loss 0.194756\n",
      "batch 1817: loss 0.056664\n",
      "batch 1818: loss 0.146441\n",
      "batch 1819: loss 0.157091\n",
      "batch 1820: loss 0.049726\n",
      "batch 1821: loss 0.087637\n",
      "batch 1822: loss 0.109017\n",
      "batch 1823: loss 0.233512\n",
      "batch 1824: loss 0.288859\n",
      "batch 1825: loss 0.103627\n",
      "batch 1826: loss 0.018722\n",
      "batch 1827: loss 0.103490\n",
      "batch 1828: loss 0.094035\n",
      "batch 1829: loss 0.063138\n",
      "batch 1830: loss 0.035217\n",
      "batch 1831: loss 0.131419\n",
      "batch 1832: loss 0.049817\n",
      "batch 1833: loss 0.254504\n",
      "batch 1834: loss 0.148893\n",
      "batch 1835: loss 0.035164\n",
      "batch 1836: loss 0.117907\n",
      "batch 1837: loss 0.138918\n",
      "batch 1838: loss 0.046567\n",
      "batch 1839: loss 0.218685\n",
      "batch 1840: loss 0.082832\n",
      "batch 1841: loss 0.248358\n",
      "batch 1842: loss 0.396650\n",
      "batch 1843: loss 0.197033\n",
      "batch 1844: loss 0.181792\n",
      "batch 1845: loss 0.119779\n",
      "batch 1846: loss 0.206360\n",
      "batch 1847: loss 0.096903\n",
      "batch 1848: loss 0.135983\n",
      "batch 1849: loss 0.060570\n",
      "batch 1850: loss 0.067384\n",
      "batch 1851: loss 0.059824\n",
      "batch 1852: loss 0.106277\n",
      "batch 1853: loss 0.058888\n",
      "batch 1854: loss 0.115296\n",
      "batch 1855: loss 0.102218\n",
      "batch 1856: loss 0.145454\n",
      "batch 1857: loss 0.150964\n",
      "batch 1858: loss 0.140109\n",
      "batch 1859: loss 0.056014\n",
      "batch 1860: loss 0.141876\n",
      "batch 1861: loss 0.064372\n",
      "batch 1862: loss 0.109703\n",
      "batch 1863: loss 0.057051\n",
      "batch 1864: loss 0.162542\n",
      "batch 1865: loss 0.069749\n",
      "batch 1866: loss 0.141268\n",
      "batch 1867: loss 0.093320\n",
      "batch 1868: loss 0.040492\n",
      "batch 1869: loss 0.256455\n",
      "batch 1870: loss 0.272641\n",
      "batch 1871: loss 0.108428\n",
      "batch 1872: loss 0.302787\n",
      "batch 1873: loss 0.205076\n",
      "batch 1874: loss 0.160809\n",
      "batch 1875: loss 0.095822\n",
      "batch 1876: loss 0.248492\n",
      "batch 1877: loss 0.049956\n",
      "batch 1878: loss 0.069750\n",
      "batch 1879: loss 0.035705\n",
      "batch 1880: loss 0.036495\n",
      "batch 1881: loss 0.170644\n",
      "batch 1882: loss 0.213999\n",
      "batch 1883: loss 0.081336\n",
      "batch 1884: loss 0.052045\n",
      "batch 1885: loss 0.186734\n",
      "batch 1886: loss 0.151635\n",
      "batch 1887: loss 0.065466\n",
      "batch 1888: loss 0.172299\n",
      "batch 1889: loss 0.036551\n",
      "batch 1890: loss 0.170887\n",
      "batch 1891: loss 0.162537\n",
      "batch 1892: loss 0.126875\n",
      "batch 1893: loss 0.133518\n",
      "batch 1894: loss 0.012199\n",
      "batch 1895: loss 0.109173\n",
      "batch 1896: loss 0.087472\n",
      "batch 1897: loss 0.162162\n",
      "batch 1898: loss 0.090003\n",
      "batch 1899: loss 0.230656\n",
      "batch 1900: loss 0.084372\n",
      "batch 1901: loss 0.157134\n",
      "batch 1902: loss 0.096170\n",
      "batch 1903: loss 0.267288\n",
      "batch 1904: loss 0.030653\n",
      "batch 1905: loss 0.085924\n",
      "batch 1906: loss 0.095635\n",
      "batch 1907: loss 0.181391\n",
      "batch 1908: loss 0.098073\n",
      "batch 1909: loss 0.110507\n",
      "batch 1910: loss 0.082893\n",
      "batch 1911: loss 0.504703\n",
      "batch 1912: loss 0.119631\n",
      "batch 1913: loss 0.156694\n",
      "batch 1914: loss 0.036103\n",
      "batch 1915: loss 0.101557\n",
      "batch 1916: loss 0.155406\n",
      "batch 1917: loss 0.096916\n",
      "batch 1918: loss 0.154296\n",
      "batch 1919: loss 0.027190\n",
      "batch 1920: loss 0.383202\n",
      "batch 1921: loss 0.233215\n",
      "batch 1922: loss 0.221918\n",
      "batch 1923: loss 0.068726\n",
      "batch 1924: loss 0.092253\n",
      "batch 1925: loss 0.061932\n",
      "batch 1926: loss 0.052628\n",
      "batch 1927: loss 0.042009\n",
      "batch 1928: loss 0.321886\n",
      "batch 1929: loss 0.072931\n",
      "batch 1930: loss 0.217844\n",
      "batch 1931: loss 0.051459\n",
      "batch 1932: loss 0.072613\n",
      "batch 1933: loss 0.049510\n",
      "batch 1934: loss 0.128330\n",
      "batch 1935: loss 0.116495\n",
      "batch 1936: loss 0.065188\n",
      "batch 1937: loss 0.102072\n",
      "batch 1938: loss 0.114966\n",
      "batch 1939: loss 0.046122\n",
      "batch 1940: loss 0.190649\n",
      "batch 1941: loss 0.089358\n",
      "batch 1942: loss 0.140472\n",
      "batch 1943: loss 0.128251\n",
      "batch 1944: loss 0.194083\n",
      "batch 1945: loss 0.056916\n",
      "batch 1946: loss 0.108878\n",
      "batch 1947: loss 0.144911\n",
      "batch 1948: loss 0.221391\n",
      "batch 1949: loss 0.125708\n",
      "batch 1950: loss 0.053735\n",
      "batch 1951: loss 0.041509\n",
      "batch 1952: loss 0.103280\n",
      "batch 1953: loss 0.018580\n",
      "batch 1954: loss 0.088185\n",
      "batch 1955: loss 0.101633\n",
      "batch 1956: loss 0.229173\n",
      "batch 1957: loss 0.127172\n",
      "batch 1958: loss 0.076380\n",
      "batch 1959: loss 0.211021\n",
      "batch 1960: loss 0.090386\n",
      "batch 1961: loss 0.072137\n",
      "batch 1962: loss 0.075812\n",
      "batch 1963: loss 0.090164\n",
      "batch 1964: loss 0.135022\n",
      "batch 1965: loss 0.056512\n",
      "batch 1966: loss 0.153152\n",
      "batch 1967: loss 0.077474\n",
      "batch 1968: loss 0.090371\n",
      "batch 1969: loss 0.113070\n",
      "batch 1970: loss 0.047983\n",
      "batch 1971: loss 0.147955\n",
      "batch 1972: loss 0.110602\n",
      "batch 1973: loss 0.174314\n",
      "batch 1974: loss 0.132114\n",
      "batch 1975: loss 0.046144\n",
      "batch 1976: loss 0.161067\n",
      "batch 1977: loss 0.274200\n",
      "batch 1978: loss 0.112440\n",
      "batch 1979: loss 0.167121\n",
      "batch 1980: loss 0.039420\n",
      "batch 1981: loss 0.138775\n",
      "batch 1982: loss 0.052598\n",
      "batch 1983: loss 0.040112\n",
      "batch 1984: loss 0.029898\n",
      "batch 1985: loss 0.049798\n",
      "batch 1986: loss 0.075562\n",
      "batch 1987: loss 0.277382\n",
      "batch 1988: loss 0.091078\n",
      "batch 1989: loss 0.112372\n",
      "batch 1990: loss 0.158417\n",
      "batch 1991: loss 0.148456\n",
      "batch 1992: loss 0.057305\n",
      "batch 1993: loss 0.118181\n",
      "batch 1994: loss 0.069387\n",
      "batch 1995: loss 0.321410\n",
      "batch 1996: loss 0.146295\n",
      "batch 1997: loss 0.073827\n",
      "batch 1998: loss 0.105080\n",
      "batch 1999: loss 0.183450\n",
      "batch 2000: loss 0.084448\n",
      "batch 2001: loss 0.187661\n",
      "batch 2002: loss 0.044109\n",
      "batch 2003: loss 0.071758\n",
      "batch 2004: loss 0.031101\n",
      "batch 2005: loss 0.043086\n",
      "batch 2006: loss 0.063895\n",
      "batch 2007: loss 0.067502\n",
      "batch 2008: loss 0.192468\n",
      "batch 2009: loss 0.196409\n",
      "batch 2010: loss 0.116479\n",
      "batch 2011: loss 0.066152\n",
      "batch 2012: loss 0.279858\n",
      "batch 2013: loss 0.038660\n",
      "batch 2014: loss 0.019792\n",
      "batch 2015: loss 0.123567\n",
      "batch 2016: loss 0.055630\n",
      "batch 2017: loss 0.064373\n",
      "batch 2018: loss 0.088236\n",
      "batch 2019: loss 0.046987\n",
      "batch 2020: loss 0.097061\n",
      "batch 2021: loss 0.088311\n",
      "batch 2022: loss 0.080027\n",
      "batch 2023: loss 0.119071\n",
      "batch 2024: loss 0.048246\n",
      "batch 2025: loss 0.037294\n",
      "batch 2026: loss 0.309863\n",
      "batch 2027: loss 0.107394\n",
      "batch 2028: loss 0.051501\n",
      "batch 2029: loss 0.093781\n",
      "batch 2030: loss 0.160477\n",
      "batch 2031: loss 0.050389\n",
      "batch 2032: loss 0.148796\n",
      "batch 2033: loss 0.059796\n",
      "batch 2034: loss 0.068537\n",
      "batch 2035: loss 0.179422\n",
      "batch 2036: loss 0.261045\n",
      "batch 2037: loss 0.159821\n",
      "batch 2038: loss 0.038714\n",
      "batch 2039: loss 0.140276\n",
      "batch 2040: loss 0.187216\n",
      "batch 2041: loss 0.028770\n",
      "batch 2042: loss 0.135930\n",
      "batch 2043: loss 0.143837\n",
      "batch 2044: loss 0.137605\n",
      "batch 2045: loss 0.163365\n",
      "batch 2046: loss 0.306362\n",
      "batch 2047: loss 0.091064\n",
      "batch 2048: loss 0.149866\n",
      "batch 2049: loss 0.114824\n",
      "batch 2050: loss 0.041295\n",
      "batch 2051: loss 0.038510\n",
      "batch 2052: loss 0.216418\n",
      "batch 2053: loss 0.130260\n",
      "batch 2054: loss 0.112322\n",
      "batch 2055: loss 0.121710\n",
      "batch 2056: loss 0.166842\n",
      "batch 2057: loss 0.048293\n",
      "batch 2058: loss 0.067737\n",
      "batch 2059: loss 0.079414\n",
      "batch 2060: loss 0.137634\n",
      "batch 2061: loss 0.069544\n",
      "batch 2062: loss 0.129975\n",
      "batch 2063: loss 0.163641\n",
      "batch 2064: loss 0.083793\n",
      "batch 2065: loss 0.120951\n",
      "batch 2066: loss 0.160924\n",
      "batch 2067: loss 0.174488\n",
      "batch 2068: loss 0.144865\n",
      "batch 2069: loss 0.166085\n",
      "batch 2070: loss 0.109578\n",
      "batch 2071: loss 0.169532\n",
      "batch 2072: loss 0.117237\n",
      "batch 2073: loss 0.068873\n",
      "batch 2074: loss 0.066250\n",
      "batch 2075: loss 0.059504\n",
      "batch 2076: loss 0.267793\n",
      "batch 2077: loss 0.117876\n",
      "batch 2078: loss 0.095878\n",
      "batch 2079: loss 0.034112\n",
      "batch 2080: loss 0.064494\n",
      "batch 2081: loss 0.190576\n",
      "batch 2082: loss 0.105677\n",
      "batch 2083: loss 0.074160\n",
      "batch 2084: loss 0.210414\n",
      "batch 2085: loss 0.171248\n",
      "batch 2086: loss 0.061458\n",
      "batch 2087: loss 0.044634\n",
      "batch 2088: loss 0.060147\n",
      "batch 2089: loss 0.138233\n",
      "batch 2090: loss 0.132573\n",
      "batch 2091: loss 0.156781\n",
      "batch 2092: loss 0.053934\n",
      "batch 2093: loss 0.052802\n",
      "batch 2094: loss 0.123229\n",
      "batch 2095: loss 0.081069\n",
      "batch 2096: loss 0.051510\n",
      "batch 2097: loss 0.183984\n",
      "batch 2098: loss 0.078585\n",
      "batch 2099: loss 0.075504\n",
      "batch 2100: loss 0.180779\n",
      "batch 2101: loss 0.014581\n",
      "batch 2102: loss 0.103549\n",
      "batch 2103: loss 0.197146\n",
      "batch 2104: loss 0.028653\n",
      "batch 2105: loss 0.059198\n",
      "batch 2106: loss 0.056108\n",
      "batch 2107: loss 0.142514\n",
      "batch 2108: loss 0.105419\n",
      "batch 2109: loss 0.106605\n",
      "batch 2110: loss 0.138995\n",
      "batch 2111: loss 0.187381\n",
      "batch 2112: loss 0.121180\n",
      "batch 2113: loss 0.067105\n",
      "batch 2114: loss 0.082234\n",
      "batch 2115: loss 0.120963\n",
      "batch 2116: loss 0.053504\n",
      "batch 2117: loss 0.185310\n",
      "batch 2118: loss 0.043198\n",
      "batch 2119: loss 0.145928\n",
      "batch 2120: loss 0.100505\n",
      "batch 2121: loss 0.182146\n",
      "batch 2122: loss 0.207353\n",
      "batch 2123: loss 0.072881\n",
      "batch 2124: loss 0.087364\n",
      "batch 2125: loss 0.049734\n",
      "batch 2126: loss 0.193604\n",
      "batch 2127: loss 0.100806\n",
      "batch 2128: loss 0.107703\n",
      "batch 2129: loss 0.243081\n",
      "batch 2130: loss 0.087723\n",
      "batch 2131: loss 0.140725\n",
      "batch 2132: loss 0.249964\n",
      "batch 2133: loss 0.502762\n",
      "batch 2134: loss 0.079126\n",
      "batch 2135: loss 0.225784\n",
      "batch 2136: loss 0.056174\n",
      "batch 2137: loss 0.052094\n",
      "batch 2138: loss 0.037545\n",
      "batch 2139: loss 0.091490\n",
      "batch 2140: loss 0.076083\n",
      "batch 2141: loss 0.047005\n",
      "batch 2142: loss 0.045917\n",
      "batch 2143: loss 0.085046\n",
      "batch 2144: loss 0.211539\n",
      "batch 2145: loss 0.078462\n",
      "batch 2146: loss 0.128088\n",
      "batch 2147: loss 0.045389\n",
      "batch 2148: loss 0.050813\n",
      "batch 2149: loss 0.230571\n",
      "batch 2150: loss 0.083924\n",
      "batch 2151: loss 0.036806\n",
      "batch 2152: loss 0.183466\n",
      "batch 2153: loss 0.148669\n",
      "batch 2154: loss 0.049110\n",
      "batch 2155: loss 0.054142\n",
      "batch 2156: loss 0.047411\n",
      "batch 2157: loss 0.075399\n",
      "batch 2158: loss 0.070888\n",
      "batch 2159: loss 0.123746\n",
      "batch 2160: loss 0.225915\n",
      "batch 2161: loss 0.238031\n",
      "batch 2162: loss 0.120282\n",
      "batch 2163: loss 0.125770\n",
      "batch 2164: loss 0.091668\n",
      "batch 2165: loss 0.194858\n",
      "batch 2166: loss 0.155826\n",
      "batch 2167: loss 0.108127\n",
      "batch 2168: loss 0.137000\n",
      "batch 2169: loss 0.106590\n",
      "batch 2170: loss 0.161376\n",
      "batch 2171: loss 0.159756\n",
      "batch 2172: loss 0.126356\n",
      "batch 2173: loss 0.072062\n",
      "batch 2174: loss 0.109167\n",
      "batch 2175: loss 0.112292\n",
      "batch 2176: loss 0.070185\n",
      "batch 2177: loss 0.147176\n",
      "batch 2178: loss 0.120682\n",
      "batch 2179: loss 0.106998\n",
      "batch 2180: loss 0.039334\n",
      "batch 2181: loss 0.083739\n",
      "batch 2182: loss 0.043670\n",
      "batch 2183: loss 0.032332\n",
      "batch 2184: loss 0.286945\n",
      "batch 2185: loss 0.108858\n",
      "batch 2186: loss 0.152135\n",
      "batch 2187: loss 0.105872\n",
      "batch 2188: loss 0.033943\n",
      "batch 2189: loss 0.038944\n",
      "batch 2190: loss 0.115151\n",
      "batch 2191: loss 0.128341\n",
      "batch 2192: loss 0.090533\n",
      "batch 2193: loss 0.061568\n",
      "batch 2194: loss 0.194968\n",
      "batch 2195: loss 0.099809\n",
      "batch 2196: loss 0.208312\n",
      "batch 2197: loss 0.132890\n",
      "batch 2198: loss 0.044044\n",
      "batch 2199: loss 0.099220\n",
      "batch 2200: loss 0.090282\n",
      "batch 2201: loss 0.187756\n",
      "batch 2202: loss 0.177529\n",
      "batch 2203: loss 0.031948\n",
      "batch 2204: loss 0.165604\n",
      "batch 2205: loss 0.140313\n",
      "batch 2206: loss 0.020702\n",
      "batch 2207: loss 0.237716\n",
      "batch 2208: loss 0.090999\n",
      "batch 2209: loss 0.064057\n",
      "batch 2210: loss 0.131453\n",
      "batch 2211: loss 0.017603\n",
      "batch 2212: loss 0.195473\n",
      "batch 2213: loss 0.106423\n",
      "batch 2214: loss 0.041173\n",
      "batch 2215: loss 0.016339\n",
      "batch 2216: loss 0.182952\n",
      "batch 2217: loss 0.058108\n",
      "batch 2218: loss 0.065594\n",
      "batch 2219: loss 0.049326\n",
      "batch 2220: loss 0.093309\n",
      "batch 2221: loss 0.043028\n",
      "batch 2222: loss 0.039540\n",
      "batch 2223: loss 0.152577\n",
      "batch 2224: loss 0.064024\n",
      "batch 2225: loss 0.029057\n",
      "batch 2226: loss 0.043546\n",
      "batch 2227: loss 0.171609\n",
      "batch 2228: loss 0.091870\n",
      "batch 2229: loss 0.121194\n",
      "batch 2230: loss 0.065762\n",
      "batch 2231: loss 0.145432\n",
      "batch 2232: loss 0.135710\n",
      "batch 2233: loss 0.036392\n",
      "batch 2234: loss 0.143558\n",
      "batch 2235: loss 0.075169\n",
      "batch 2236: loss 0.124367\n",
      "batch 2237: loss 0.101294\n",
      "batch 2238: loss 0.151560\n",
      "batch 2239: loss 0.178655\n",
      "batch 2240: loss 0.132661\n",
      "batch 2241: loss 0.201097\n",
      "batch 2242: loss 0.189714\n",
      "batch 2243: loss 0.047908\n",
      "batch 2244: loss 0.114926\n",
      "batch 2245: loss 0.053211\n",
      "batch 2246: loss 0.054701\n",
      "batch 2247: loss 0.064175\n",
      "batch 2248: loss 0.026358\n",
      "batch 2249: loss 0.116491\n",
      "batch 2250: loss 0.093040\n",
      "batch 2251: loss 0.142082\n",
      "batch 2252: loss 0.095839\n",
      "batch 2253: loss 0.241084\n",
      "batch 2254: loss 0.098717\n",
      "batch 2255: loss 0.156292\n",
      "batch 2256: loss 0.117781\n",
      "batch 2257: loss 0.076814\n",
      "batch 2258: loss 0.104559\n",
      "batch 2259: loss 0.110684\n",
      "batch 2260: loss 0.132141\n",
      "batch 2261: loss 0.060732\n",
      "batch 2262: loss 0.051663\n",
      "batch 2263: loss 0.188370\n",
      "batch 2264: loss 0.092396\n",
      "batch 2265: loss 0.016183\n",
      "batch 2266: loss 0.072851\n",
      "batch 2267: loss 0.062853\n",
      "batch 2268: loss 0.088434\n",
      "batch 2269: loss 0.047004\n",
      "batch 2270: loss 0.084907\n",
      "batch 2271: loss 0.086753\n",
      "batch 2272: loss 0.086272\n",
      "batch 2273: loss 0.057879\n",
      "batch 2274: loss 0.152464\n",
      "batch 2275: loss 0.051053\n",
      "batch 2276: loss 0.114562\n",
      "batch 2277: loss 0.046779\n",
      "batch 2278: loss 0.144382\n",
      "batch 2279: loss 0.101874\n",
      "batch 2280: loss 0.055628\n",
      "batch 2281: loss 0.149012\n",
      "batch 2282: loss 0.176838\n",
      "batch 2283: loss 0.073029\n",
      "batch 2284: loss 0.059130\n",
      "batch 2285: loss 0.047990\n",
      "batch 2286: loss 0.109616\n",
      "batch 2287: loss 0.163150\n",
      "batch 2288: loss 0.203788\n",
      "batch 2289: loss 0.093127\n",
      "batch 2290: loss 0.108293\n",
      "batch 2291: loss 0.068323\n",
      "batch 2292: loss 0.074023\n",
      "batch 2293: loss 0.067515\n",
      "batch 2294: loss 0.134023\n",
      "batch 2295: loss 0.013371\n",
      "batch 2296: loss 0.144045\n",
      "batch 2297: loss 0.148389\n",
      "batch 2298: loss 0.093847\n",
      "batch 2299: loss 0.034257\n",
      "batch 2300: loss 0.090712\n",
      "batch 2301: loss 0.145032\n",
      "batch 2302: loss 0.055321\n",
      "batch 2303: loss 0.250822\n",
      "batch 2304: loss 0.046116\n",
      "batch 2305: loss 0.215194\n",
      "batch 2306: loss 0.045359\n",
      "batch 2307: loss 0.034419\n",
      "batch 2308: loss 0.041037\n",
      "batch 2309: loss 0.020523\n",
      "batch 2310: loss 0.065322\n",
      "batch 2311: loss 0.028505\n",
      "batch 2312: loss 0.127850\n",
      "batch 2313: loss 0.052560\n",
      "batch 2314: loss 0.031137\n",
      "batch 2315: loss 0.101832\n",
      "batch 2316: loss 0.034482\n",
      "batch 2317: loss 0.104998\n",
      "batch 2318: loss 0.259837\n",
      "batch 2319: loss 0.080096\n",
      "batch 2320: loss 0.054031\n",
      "batch 2321: loss 0.270233\n",
      "batch 2322: loss 0.094906\n",
      "batch 2323: loss 0.089848\n",
      "batch 2324: loss 0.052577\n",
      "batch 2325: loss 0.109853\n",
      "batch 2326: loss 0.129036\n",
      "batch 2327: loss 0.113697\n",
      "batch 2328: loss 0.172440\n",
      "batch 2329: loss 0.141577\n",
      "batch 2330: loss 0.058762\n",
      "batch 2331: loss 0.311949\n",
      "batch 2332: loss 0.095641\n",
      "batch 2333: loss 0.154839\n",
      "batch 2334: loss 0.060031\n",
      "batch 2335: loss 0.174378\n",
      "batch 2336: loss 0.306696\n",
      "batch 2337: loss 0.089952\n",
      "batch 2338: loss 0.233131\n",
      "batch 2339: loss 0.033997\n",
      "batch 2340: loss 0.064024\n",
      "batch 2341: loss 0.161764\n",
      "batch 2342: loss 0.120064\n",
      "batch 2343: loss 0.061999\n",
      "batch 2344: loss 0.062097\n",
      "batch 2345: loss 0.164360\n",
      "batch 2346: loss 0.056778\n",
      "batch 2347: loss 0.058100\n",
      "batch 2348: loss 0.164882\n",
      "batch 2349: loss 0.038312\n",
      "batch 2350: loss 0.145091\n",
      "batch 2351: loss 0.095432\n",
      "batch 2352: loss 0.227126\n",
      "batch 2353: loss 0.116776\n",
      "batch 2354: loss 0.193834\n",
      "batch 2355: loss 0.149483\n",
      "batch 2356: loss 0.043563\n",
      "batch 2357: loss 0.330166\n",
      "batch 2358: loss 0.072557\n",
      "batch 2359: loss 0.051741\n",
      "batch 2360: loss 0.046947\n",
      "batch 2361: loss 0.172432\n",
      "batch 2362: loss 0.123878\n",
      "batch 2363: loss 0.112325\n",
      "batch 2364: loss 0.097941\n",
      "batch 2365: loss 0.053290\n",
      "batch 2366: loss 0.023411\n",
      "batch 2367: loss 0.046497\n",
      "batch 2368: loss 0.023382\n",
      "batch 2369: loss 0.049715\n",
      "batch 2370: loss 0.030875\n",
      "batch 2371: loss 0.257026\n",
      "batch 2372: loss 0.035361\n",
      "batch 2373: loss 0.041384\n",
      "batch 2374: loss 0.233449\n",
      "batch 2375: loss 0.183913\n",
      "batch 2376: loss 0.157594\n",
      "batch 2377: loss 0.039691\n",
      "batch 2378: loss 0.060114\n",
      "batch 2379: loss 0.023550\n",
      "batch 2380: loss 0.195527\n",
      "batch 2381: loss 0.082767\n",
      "batch 2382: loss 0.030028\n",
      "batch 2383: loss 0.215441\n",
      "batch 2384: loss 0.076394\n",
      "batch 2385: loss 0.071700\n",
      "batch 2386: loss 0.069195\n",
      "batch 2387: loss 0.077759\n",
      "batch 2388: loss 0.160149\n",
      "batch 2389: loss 0.042927\n",
      "batch 2390: loss 0.189197\n",
      "batch 2391: loss 0.097054\n",
      "batch 2392: loss 0.097890\n",
      "batch 2393: loss 0.278890\n",
      "batch 2394: loss 0.041594\n",
      "batch 2395: loss 0.095285\n",
      "batch 2396: loss 0.167057\n",
      "batch 2397: loss 0.038381\n",
      "batch 2398: loss 0.050343\n",
      "batch 2399: loss 0.037812\n",
      "batch 2400: loss 0.032183\n",
      "batch 2401: loss 0.088743\n",
      "batch 2402: loss 0.089311\n",
      "batch 2403: loss 0.104032\n",
      "batch 2404: loss 0.100149\n",
      "batch 2405: loss 0.064912\n",
      "batch 2406: loss 0.055185\n",
      "batch 2407: loss 0.057180\n",
      "batch 2408: loss 0.029483\n",
      "batch 2409: loss 0.169210\n",
      "batch 2410: loss 0.123475\n",
      "batch 2411: loss 0.085580\n",
      "batch 2412: loss 0.168946\n",
      "batch 2413: loss 0.310606\n",
      "batch 2414: loss 0.058772\n",
      "batch 2415: loss 0.103110\n",
      "batch 2416: loss 0.261858\n",
      "batch 2417: loss 0.039051\n",
      "batch 2418: loss 0.039292\n",
      "batch 2419: loss 0.069595\n",
      "batch 2420: loss 0.033159\n",
      "batch 2421: loss 0.051885\n",
      "batch 2422: loss 0.090526\n",
      "batch 2423: loss 0.058342\n",
      "batch 2424: loss 0.204268\n",
      "batch 2425: loss 0.077726\n",
      "batch 2426: loss 0.162208\n",
      "batch 2427: loss 0.131330\n",
      "batch 2428: loss 0.242975\n",
      "batch 2429: loss 0.120798\n",
      "batch 2430: loss 0.039322\n",
      "batch 2431: loss 0.089617\n",
      "batch 2432: loss 0.129948\n",
      "batch 2433: loss 0.089268\n",
      "batch 2434: loss 0.057164\n",
      "batch 2435: loss 0.290065\n",
      "batch 2436: loss 0.200540\n",
      "batch 2437: loss 0.037700\n",
      "batch 2438: loss 0.101912\n",
      "batch 2439: loss 0.059110\n",
      "batch 2440: loss 0.153835\n",
      "batch 2441: loss 0.034372\n",
      "batch 2442: loss 0.042962\n",
      "batch 2443: loss 0.066856\n",
      "batch 2444: loss 0.197831\n",
      "batch 2445: loss 0.211305\n",
      "batch 2446: loss 0.075160\n",
      "batch 2447: loss 0.220937\n",
      "batch 2448: loss 0.078847\n",
      "batch 2449: loss 0.037726\n",
      "batch 2450: loss 0.114422\n",
      "batch 2451: loss 0.172453\n",
      "batch 2452: loss 0.193155\n",
      "batch 2453: loss 0.111633\n",
      "batch 2454: loss 0.087272\n",
      "batch 2455: loss 0.051902\n",
      "batch 2456: loss 0.135095\n",
      "batch 2457: loss 0.065619\n",
      "batch 2458: loss 0.089138\n",
      "batch 2459: loss 0.100558\n",
      "batch 2460: loss 0.191667\n",
      "batch 2461: loss 0.126759\n",
      "batch 2462: loss 0.137806\n",
      "batch 2463: loss 0.150331\n",
      "batch 2464: loss 0.095070\n",
      "batch 2465: loss 0.120893\n",
      "batch 2466: loss 0.092920\n",
      "batch 2467: loss 0.124978\n",
      "batch 2468: loss 0.055530\n",
      "batch 2469: loss 0.057169\n",
      "batch 2470: loss 0.078930\n",
      "batch 2471: loss 0.223408\n",
      "batch 2472: loss 0.089542\n",
      "batch 2473: loss 0.138376\n",
      "batch 2474: loss 0.091067\n",
      "batch 2475: loss 0.062400\n",
      "batch 2476: loss 0.215306\n",
      "batch 2477: loss 0.168198\n",
      "batch 2478: loss 0.069593\n",
      "batch 2479: loss 0.105004\n",
      "batch 2480: loss 0.057803\n",
      "batch 2481: loss 0.120710\n",
      "batch 2482: loss 0.050112\n",
      "batch 2483: loss 0.042271\n",
      "batch 2484: loss 0.040612\n",
      "batch 2485: loss 0.250808\n",
      "batch 2486: loss 0.112322\n",
      "batch 2487: loss 0.092439\n",
      "batch 2488: loss 0.062004\n",
      "batch 2489: loss 0.084717\n",
      "batch 2490: loss 0.200430\n",
      "batch 2491: loss 0.123870\n",
      "batch 2492: loss 0.042252\n",
      "batch 2493: loss 0.040250\n",
      "batch 2494: loss 0.032311\n",
      "batch 2495: loss 0.133685\n",
      "batch 2496: loss 0.174398\n",
      "batch 2497: loss 0.136861\n",
      "batch 2498: loss 0.097541\n",
      "batch 2499: loss 0.306169\n",
      "batch 2500: loss 0.209708\n",
      "batch 2501: loss 0.123184\n",
      "batch 2502: loss 0.095357\n",
      "batch 2503: loss 0.165296\n",
      "batch 2504: loss 0.091648\n",
      "batch 2505: loss 0.026113\n",
      "batch 2506: loss 0.149348\n",
      "batch 2507: loss 0.042169\n",
      "batch 2508: loss 0.179734\n",
      "batch 2509: loss 0.143352\n",
      "batch 2510: loss 0.033848\n",
      "batch 2511: loss 0.060431\n",
      "batch 2512: loss 0.048420\n",
      "batch 2513: loss 0.322574\n",
      "batch 2514: loss 0.176890\n",
      "batch 2515: loss 0.056244\n",
      "batch 2516: loss 0.283449\n",
      "batch 2517: loss 0.162130\n",
      "batch 2518: loss 0.122624\n",
      "batch 2519: loss 0.033504\n",
      "batch 2520: loss 0.034810\n",
      "batch 2521: loss 0.085999\n",
      "batch 2522: loss 0.050554\n",
      "batch 2523: loss 0.071044\n",
      "batch 2524: loss 0.345223\n",
      "batch 2525: loss 0.077620\n",
      "batch 2526: loss 0.109419\n",
      "batch 2527: loss 0.097128\n",
      "batch 2528: loss 0.082782\n",
      "batch 2529: loss 0.080158\n",
      "batch 2530: loss 0.097321\n",
      "batch 2531: loss 0.115136\n",
      "batch 2532: loss 0.112485\n",
      "batch 2533: loss 0.044459\n",
      "batch 2534: loss 0.083735\n",
      "batch 2535: loss 0.074760\n",
      "batch 2536: loss 0.064023\n",
      "batch 2537: loss 0.046920\n",
      "batch 2538: loss 0.162175\n",
      "batch 2539: loss 0.116522\n",
      "batch 2540: loss 0.091597\n",
      "batch 2541: loss 0.127702\n",
      "batch 2542: loss 0.135991\n",
      "batch 2543: loss 0.117880\n",
      "batch 2544: loss 0.199056\n",
      "batch 2545: loss 0.190925\n",
      "batch 2546: loss 0.097880\n",
      "batch 2547: loss 0.032486\n",
      "batch 2548: loss 0.063174\n",
      "batch 2549: loss 0.182415\n",
      "batch 2550: loss 0.145732\n",
      "batch 2551: loss 0.053496\n",
      "batch 2552: loss 0.068087\n",
      "batch 2553: loss 0.083219\n",
      "batch 2554: loss 0.112540\n",
      "batch 2555: loss 0.128685\n",
      "batch 2556: loss 0.052327\n",
      "batch 2557: loss 0.054371\n",
      "batch 2558: loss 0.068511\n",
      "batch 2559: loss 0.058335\n",
      "batch 2560: loss 0.033993\n",
      "batch 2561: loss 0.012265\n",
      "batch 2562: loss 0.147535\n",
      "batch 2563: loss 0.138966\n",
      "batch 2564: loss 0.050578\n",
      "batch 2565: loss 0.193447\n",
      "batch 2566: loss 0.066460\n",
      "batch 2567: loss 0.046093\n",
      "batch 2568: loss 0.056087\n",
      "batch 2569: loss 0.071991\n",
      "batch 2570: loss 0.140050\n",
      "batch 2571: loss 0.120965\n",
      "batch 2572: loss 0.134968\n",
      "batch 2573: loss 0.017120\n",
      "batch 2574: loss 0.056916\n",
      "batch 2575: loss 0.201647\n",
      "batch 2576: loss 0.030844\n",
      "batch 2577: loss 0.034414\n",
      "batch 2578: loss 0.132442\n",
      "batch 2579: loss 0.077721\n",
      "batch 2580: loss 0.074421\n",
      "batch 2581: loss 0.153316\n",
      "batch 2582: loss 0.014641\n",
      "batch 2583: loss 0.073105\n",
      "batch 2584: loss 0.045722\n",
      "batch 2585: loss 0.092100\n",
      "batch 2586: loss 0.192046\n",
      "batch 2587: loss 0.238926\n",
      "batch 2588: loss 0.162390\n",
      "batch 2589: loss 0.024099\n",
      "batch 2590: loss 0.115190\n",
      "batch 2591: loss 0.053509\n",
      "batch 2592: loss 0.030324\n",
      "batch 2593: loss 0.051157\n",
      "batch 2594: loss 0.128249\n",
      "batch 2595: loss 0.041794\n",
      "batch 2596: loss 0.056384\n",
      "batch 2597: loss 0.132611\n",
      "batch 2598: loss 0.192066\n",
      "batch 2599: loss 0.145774\n",
      "batch 2600: loss 0.089206\n",
      "batch 2601: loss 0.020916\n",
      "batch 2602: loss 0.135844\n",
      "batch 2603: loss 0.172211\n",
      "batch 2604: loss 0.078794\n",
      "batch 2605: loss 0.056304\n",
      "batch 2606: loss 0.095515\n",
      "batch 2607: loss 0.094510\n",
      "batch 2608: loss 0.151631\n",
      "batch 2609: loss 0.062515\n",
      "batch 2610: loss 0.071489\n",
      "batch 2611: loss 0.068362\n",
      "batch 2612: loss 0.067098\n",
      "batch 2613: loss 0.091833\n",
      "batch 2614: loss 0.151351\n",
      "batch 2615: loss 0.056127\n",
      "batch 2616: loss 0.114694\n",
      "batch 2617: loss 0.034646\n",
      "batch 2618: loss 0.046959\n",
      "batch 2619: loss 0.218466\n",
      "batch 2620: loss 0.055676\n",
      "batch 2621: loss 0.044281\n",
      "batch 2622: loss 0.060975\n",
      "batch 2623: loss 0.035623\n",
      "batch 2624: loss 0.056172\n",
      "batch 2625: loss 0.039702\n",
      "batch 2626: loss 0.099728\n",
      "batch 2627: loss 0.078327\n",
      "batch 2628: loss 0.093375\n",
      "batch 2629: loss 0.062263\n",
      "batch 2630: loss 0.142862\n",
      "batch 2631: loss 0.029091\n",
      "batch 2632: loss 0.063961\n",
      "batch 2633: loss 0.420012\n",
      "batch 2634: loss 0.072119\n",
      "batch 2635: loss 0.069601\n",
      "batch 2636: loss 0.057621\n",
      "batch 2637: loss 0.064755\n",
      "batch 2638: loss 0.025889\n",
      "batch 2639: loss 0.035352\n",
      "batch 2640: loss 0.092161\n",
      "batch 2641: loss 0.057656\n",
      "batch 2642: loss 0.142767\n",
      "batch 2643: loss 0.127515\n",
      "batch 2644: loss 0.043449\n",
      "batch 2645: loss 0.040146\n",
      "batch 2646: loss 0.047508\n",
      "batch 2647: loss 0.119510\n",
      "batch 2648: loss 0.131965\n",
      "batch 2649: loss 0.071600\n",
      "batch 2650: loss 0.073023\n",
      "batch 2651: loss 0.020914\n",
      "batch 2652: loss 0.048463\n",
      "batch 2653: loss 0.072324\n",
      "batch 2654: loss 0.094395\n",
      "batch 2655: loss 0.104839\n",
      "batch 2656: loss 0.147605\n",
      "batch 2657: loss 0.071380\n",
      "batch 2658: loss 0.390348\n",
      "batch 2659: loss 0.124536\n",
      "batch 2660: loss 0.019934\n",
      "batch 2661: loss 0.031809\n",
      "batch 2662: loss 0.185387\n",
      "batch 2663: loss 0.034449\n",
      "batch 2664: loss 0.051507\n",
      "batch 2665: loss 0.180392\n",
      "batch 2666: loss 0.131346\n",
      "batch 2667: loss 0.017109\n",
      "batch 2668: loss 0.061241\n",
      "batch 2669: loss 0.516384\n",
      "batch 2670: loss 0.134862\n",
      "batch 2671: loss 0.099754\n",
      "batch 2672: loss 0.091715\n",
      "batch 2673: loss 0.187806\n",
      "batch 2674: loss 0.153399\n",
      "batch 2675: loss 0.105602\n",
      "batch 2676: loss 0.047962\n",
      "batch 2677: loss 0.070463\n",
      "batch 2678: loss 0.094350\n",
      "batch 2679: loss 0.274603\n",
      "batch 2680: loss 0.163954\n",
      "batch 2681: loss 0.189031\n",
      "batch 2682: loss 0.024855\n",
      "batch 2683: loss 0.117741\n",
      "batch 2684: loss 0.085158\n",
      "batch 2685: loss 0.247267\n",
      "batch 2686: loss 0.157923\n",
      "batch 2687: loss 0.081279\n",
      "batch 2688: loss 0.199091\n",
      "batch 2689: loss 0.055116\n",
      "batch 2690: loss 0.195978\n",
      "batch 2691: loss 0.015422\n",
      "batch 2692: loss 0.071617\n",
      "batch 2693: loss 0.184922\n",
      "batch 2694: loss 0.062878\n",
      "batch 2695: loss 0.026937\n",
      "batch 2696: loss 0.029580\n",
      "batch 2697: loss 0.104921\n",
      "batch 2698: loss 0.118254\n",
      "batch 2699: loss 0.086815\n",
      "batch 2700: loss 0.039870\n",
      "batch 2701: loss 0.059133\n",
      "batch 2702: loss 0.101124\n",
      "batch 2703: loss 0.084398\n",
      "batch 2704: loss 0.034221\n",
      "batch 2705: loss 0.035964\n",
      "batch 2706: loss 0.056114\n",
      "batch 2707: loss 0.101103\n",
      "batch 2708: loss 0.088657\n",
      "batch 2709: loss 0.083976\n",
      "batch 2710: loss 0.038246\n",
      "batch 2711: loss 0.101190\n",
      "batch 2712: loss 0.032348\n",
      "batch 2713: loss 0.132195\n",
      "batch 2714: loss 0.170487\n",
      "batch 2715: loss 0.034634\n",
      "batch 2716: loss 0.149934\n",
      "batch 2717: loss 0.063824\n",
      "batch 2718: loss 0.154181\n",
      "batch 2719: loss 0.105478\n",
      "batch 2720: loss 0.054503\n",
      "batch 2721: loss 0.034787\n",
      "batch 2722: loss 0.132195\n",
      "batch 2723: loss 0.069818\n",
      "batch 2724: loss 0.140798\n",
      "batch 2725: loss 0.036667\n",
      "batch 2726: loss 0.152980\n",
      "batch 2727: loss 0.082763\n",
      "batch 2728: loss 0.101836\n",
      "batch 2729: loss 0.013931\n",
      "batch 2730: loss 0.096630\n",
      "batch 2731: loss 0.127267\n",
      "batch 2732: loss 0.063037\n",
      "batch 2733: loss 0.055515\n",
      "batch 2734: loss 0.109990\n",
      "batch 2735: loss 0.033025\n",
      "batch 2736: loss 0.172616\n",
      "batch 2737: loss 0.129828\n",
      "batch 2738: loss 0.153844\n",
      "batch 2739: loss 0.030874\n",
      "batch 2740: loss 0.090278\n",
      "batch 2741: loss 0.234141\n",
      "batch 2742: loss 0.065557\n",
      "batch 2743: loss 0.044906\n",
      "batch 2744: loss 0.119824\n",
      "batch 2745: loss 0.062743\n",
      "batch 2746: loss 0.145415\n",
      "batch 2747: loss 0.065707\n",
      "batch 2748: loss 0.049335\n",
      "batch 2749: loss 0.175976\n",
      "batch 2750: loss 0.182460\n",
      "batch 2751: loss 0.062844\n",
      "batch 2752: loss 0.106207\n",
      "batch 2753: loss 0.106848\n",
      "batch 2754: loss 0.065084\n",
      "batch 2755: loss 0.038627\n",
      "batch 2756: loss 0.021556\n",
      "batch 2757: loss 0.143810\n",
      "batch 2758: loss 0.317625\n",
      "batch 2759: loss 0.104785\n",
      "batch 2760: loss 0.073681\n",
      "batch 2761: loss 0.047572\n",
      "batch 2762: loss 0.045075\n",
      "batch 2763: loss 0.077599\n",
      "batch 2764: loss 0.225938\n",
      "batch 2765: loss 0.172550\n",
      "batch 2766: loss 0.064940\n",
      "batch 2767: loss 0.081043\n",
      "batch 2768: loss 0.129150\n",
      "batch 2769: loss 0.134042\n",
      "batch 2770: loss 0.178906\n",
      "batch 2771: loss 0.054288\n",
      "batch 2772: loss 0.123672\n",
      "batch 2773: loss 0.061777\n",
      "batch 2774: loss 0.057769\n",
      "batch 2775: loss 0.061879\n",
      "batch 2776: loss 0.268757\n",
      "batch 2777: loss 0.066613\n",
      "batch 2778: loss 0.099517\n",
      "batch 2779: loss 0.049674\n",
      "batch 2780: loss 0.056267\n",
      "batch 2781: loss 0.108598\n",
      "batch 2782: loss 0.121875\n",
      "batch 2783: loss 0.203767\n",
      "batch 2784: loss 0.035450\n",
      "batch 2785: loss 0.174026\n",
      "batch 2786: loss 0.086061\n",
      "batch 2787: loss 0.041011\n",
      "batch 2788: loss 0.143691\n",
      "batch 2789: loss 0.124106\n",
      "batch 2790: loss 0.032875\n",
      "batch 2791: loss 0.062581\n",
      "batch 2792: loss 0.098591\n",
      "batch 2793: loss 0.122465\n",
      "batch 2794: loss 0.111149\n",
      "batch 2795: loss 0.058134\n",
      "batch 2796: loss 0.030330\n",
      "batch 2797: loss 0.260125\n",
      "batch 2798: loss 0.034811\n",
      "batch 2799: loss 0.043578\n",
      "batch 2800: loss 0.038540\n",
      "batch 2801: loss 0.108335\n",
      "batch 2802: loss 0.074386\n",
      "batch 2803: loss 0.007817\n",
      "batch 2804: loss 0.039341\n",
      "batch 2805: loss 0.094948\n",
      "batch 2806: loss 0.180198\n",
      "batch 2807: loss 0.191559\n",
      "batch 2808: loss 0.045636\n",
      "batch 2809: loss 0.078117\n",
      "batch 2810: loss 0.034751\n",
      "batch 2811: loss 0.043752\n",
      "batch 2812: loss 0.044253\n",
      "batch 2813: loss 0.061807\n",
      "batch 2814: loss 0.124217\n",
      "batch 2815: loss 0.203961\n",
      "batch 2816: loss 0.142507\n",
      "batch 2817: loss 0.067106\n",
      "batch 2818: loss 0.164146\n",
      "batch 2819: loss 0.091145\n",
      "batch 2820: loss 0.075635\n",
      "batch 2821: loss 0.133208\n",
      "batch 2822: loss 0.162123\n",
      "batch 2823: loss 0.098142\n",
      "batch 2824: loss 0.044864\n",
      "batch 2825: loss 0.088271\n",
      "batch 2826: loss 0.093902\n",
      "batch 2827: loss 0.175459\n",
      "batch 2828: loss 0.029606\n",
      "batch 2829: loss 0.105521\n",
      "batch 2830: loss 0.022880\n",
      "batch 2831: loss 0.095913\n",
      "batch 2832: loss 0.083566\n",
      "batch 2833: loss 0.214969\n",
      "batch 2834: loss 0.211837\n",
      "batch 2835: loss 0.116710\n",
      "batch 2836: loss 0.017382\n",
      "batch 2837: loss 0.032050\n",
      "batch 2838: loss 0.093395\n",
      "batch 2839: loss 0.077490\n",
      "batch 2840: loss 0.112812\n",
      "batch 2841: loss 0.120685\n",
      "batch 2842: loss 0.070424\n",
      "batch 2843: loss 0.080555\n",
      "batch 2844: loss 0.098366\n",
      "batch 2845: loss 0.054269\n",
      "batch 2846: loss 0.060225\n",
      "batch 2847: loss 0.101962\n",
      "batch 2848: loss 0.172760\n",
      "batch 2849: loss 0.026678\n",
      "batch 2850: loss 0.039037\n",
      "batch 2851: loss 0.075702\n",
      "batch 2852: loss 0.202034\n",
      "batch 2853: loss 0.034119\n",
      "batch 2854: loss 0.035539\n",
      "batch 2855: loss 0.081521\n",
      "batch 2856: loss 0.031391\n",
      "batch 2857: loss 0.083333\n",
      "batch 2858: loss 0.280391\n",
      "batch 2859: loss 0.052605\n",
      "batch 2860: loss 0.213664\n",
      "batch 2861: loss 0.064669\n",
      "batch 2862: loss 0.030821\n",
      "batch 2863: loss 0.133425\n",
      "batch 2864: loss 0.061018\n",
      "batch 2865: loss 0.122865\n",
      "batch 2866: loss 0.061044\n",
      "batch 2867: loss 0.053481\n",
      "batch 2868: loss 0.022691\n",
      "batch 2869: loss 0.026377\n",
      "batch 2870: loss 0.010625\n",
      "batch 2871: loss 0.063061\n",
      "batch 2872: loss 0.058562\n",
      "batch 2873: loss 0.051089\n",
      "batch 2874: loss 0.125392\n",
      "batch 2875: loss 0.122266\n",
      "batch 2876: loss 0.209023\n",
      "batch 2877: loss 0.086741\n",
      "batch 2878: loss 0.232659\n",
      "batch 2879: loss 0.018689\n",
      "batch 2880: loss 0.164330\n",
      "batch 2881: loss 0.086608\n",
      "batch 2882: loss 0.036567\n",
      "batch 2883: loss 0.082006\n",
      "batch 2884: loss 0.063756\n",
      "batch 2885: loss 0.074249\n",
      "batch 2886: loss 0.151046\n",
      "batch 2887: loss 0.080911\n",
      "batch 2888: loss 0.090872\n",
      "batch 2889: loss 0.061565\n",
      "batch 2890: loss 0.014360\n",
      "batch 2891: loss 0.112349\n",
      "batch 2892: loss 0.153792\n",
      "batch 2893: loss 0.082102\n",
      "batch 2894: loss 0.063836\n",
      "batch 2895: loss 0.035329\n",
      "batch 2896: loss 0.051699\n",
      "batch 2897: loss 0.044770\n",
      "batch 2898: loss 0.046686\n",
      "batch 2899: loss 0.086640\n",
      "batch 2900: loss 0.059978\n",
      "batch 2901: loss 0.132889\n",
      "batch 2902: loss 0.051496\n",
      "batch 2903: loss 0.139930\n",
      "batch 2904: loss 0.047637\n",
      "batch 2905: loss 0.077219\n",
      "batch 2906: loss 0.096843\n",
      "batch 2907: loss 0.061564\n",
      "batch 2908: loss 0.026488\n",
      "batch 2909: loss 0.097251\n",
      "batch 2910: loss 0.028175\n",
      "batch 2911: loss 0.065568\n",
      "batch 2912: loss 0.077820\n",
      "batch 2913: loss 0.102782\n",
      "batch 2914: loss 0.166107\n",
      "batch 2915: loss 0.080242\n",
      "batch 2916: loss 0.056634\n",
      "batch 2917: loss 0.181793\n",
      "batch 2918: loss 0.013674\n",
      "batch 2919: loss 0.032105\n",
      "batch 2920: loss 0.071789\n",
      "batch 2921: loss 0.111072\n",
      "batch 2922: loss 0.045309\n",
      "batch 2923: loss 0.082218\n",
      "batch 2924: loss 0.037530\n",
      "batch 2925: loss 0.025952\n",
      "batch 2926: loss 0.236449\n",
      "batch 2927: loss 0.039723\n",
      "batch 2928: loss 0.152267\n",
      "batch 2929: loss 0.075120\n",
      "batch 2930: loss 0.042674\n",
      "batch 2931: loss 0.159768\n",
      "batch 2932: loss 0.082140\n",
      "batch 2933: loss 0.093539\n",
      "batch 2934: loss 0.015972\n",
      "batch 2935: loss 0.016666\n",
      "batch 2936: loss 0.108050\n",
      "batch 2937: loss 0.174613\n",
      "batch 2938: loss 0.192616\n",
      "batch 2939: loss 0.066361\n",
      "batch 2940: loss 0.020950\n",
      "batch 2941: loss 0.057701\n",
      "batch 2942: loss 0.239534\n",
      "batch 2943: loss 0.015140\n",
      "batch 2944: loss 0.052549\n",
      "batch 2945: loss 0.078461\n",
      "batch 2946: loss 0.079988\n",
      "batch 2947: loss 0.099107\n",
      "batch 2948: loss 0.066165\n",
      "batch 2949: loss 0.041137\n",
      "batch 2950: loss 0.050486\n",
      "batch 2951: loss 0.045511\n",
      "batch 2952: loss 0.057244\n",
      "batch 2953: loss 0.028747\n",
      "batch 2954: loss 0.047863\n",
      "batch 2955: loss 0.081816\n",
      "batch 2956: loss 0.067212\n",
      "batch 2957: loss 0.030161\n",
      "batch 2958: loss 0.086735\n",
      "batch 2959: loss 0.105803\n",
      "batch 2960: loss 0.015867\n",
      "batch 2961: loss 0.090783\n",
      "batch 2962: loss 0.086261\n",
      "batch 2963: loss 0.063507\n",
      "batch 2964: loss 0.025340\n",
      "batch 2965: loss 0.152795\n",
      "batch 2966: loss 0.024680\n",
      "batch 2967: loss 0.084002\n",
      "batch 2968: loss 0.101716\n",
      "batch 2969: loss 0.156776\n",
      "batch 2970: loss 0.135856\n",
      "batch 2971: loss 0.097335\n",
      "batch 2972: loss 0.037685\n",
      "batch 2973: loss 0.086479\n",
      "batch 2974: loss 0.035113\n",
      "batch 2975: loss 0.048947\n",
      "batch 2976: loss 0.051348\n",
      "batch 2977: loss 0.070793\n",
      "batch 2978: loss 0.045549\n",
      "batch 2979: loss 0.124136\n",
      "batch 2980: loss 0.034604\n",
      "batch 2981: loss 0.014353\n",
      "batch 2982: loss 0.087920\n",
      "batch 2983: loss 0.038672\n",
      "batch 2984: loss 0.022371\n",
      "batch 2985: loss 0.209737\n",
      "batch 2986: loss 0.023406\n",
      "batch 2987: loss 0.150989\n",
      "batch 2988: loss 0.038163\n",
      "batch 2989: loss 0.056904\n",
      "batch 2990: loss 0.072008\n",
      "batch 2991: loss 0.024318\n",
      "batch 2992: loss 0.164148\n",
      "batch 2993: loss 0.055432\n",
      "batch 2994: loss 0.025511\n",
      "batch 2995: loss 0.034754\n",
      "batch 2996: loss 0.114180\n",
      "batch 2997: loss 0.050988\n",
      "batch 2998: loss 0.098757\n",
      "batch 2999: loss 0.046012\n",
      "batch 3000: loss 0.136101\n",
      "batch 3001: loss 0.087469\n",
      "batch 3002: loss 0.074223\n",
      "batch 3003: loss 0.171785\n",
      "batch 3004: loss 0.063711\n",
      "batch 3005: loss 0.019981\n",
      "batch 3006: loss 0.038156\n",
      "batch 3007: loss 0.352455\n",
      "batch 3008: loss 0.127690\n",
      "batch 3009: loss 0.034797\n",
      "batch 3010: loss 0.129890\n",
      "batch 3011: loss 0.071099\n",
      "batch 3012: loss 0.173510\n",
      "batch 3013: loss 0.177022\n",
      "batch 3014: loss 0.060498\n",
      "batch 3015: loss 0.045001\n",
      "batch 3016: loss 0.125219\n",
      "batch 3017: loss 0.040988\n",
      "batch 3018: loss 0.123090\n",
      "batch 3019: loss 0.200015\n",
      "batch 3020: loss 0.165115\n",
      "batch 3021: loss 0.063602\n",
      "batch 3022: loss 0.044098\n",
      "batch 3023: loss 0.040901\n",
      "batch 3024: loss 0.136371\n",
      "batch 3025: loss 0.030425\n",
      "batch 3026: loss 0.068498\n",
      "batch 3027: loss 0.108601\n",
      "batch 3028: loss 0.084435\n",
      "batch 3029: loss 0.047772\n",
      "batch 3030: loss 0.161908\n",
      "batch 3031: loss 0.098485\n",
      "batch 3032: loss 0.040314\n",
      "batch 3033: loss 0.101686\n",
      "batch 3034: loss 0.071048\n",
      "batch 3035: loss 0.037893\n",
      "batch 3036: loss 0.077068\n",
      "batch 3037: loss 0.043062\n",
      "batch 3038: loss 0.062216\n",
      "batch 3039: loss 0.105695\n",
      "batch 3040: loss 0.031316\n",
      "batch 3041: loss 0.054883\n",
      "batch 3042: loss 0.139026\n",
      "batch 3043: loss 0.060661\n",
      "batch 3044: loss 0.077383\n",
      "batch 3045: loss 0.085488\n",
      "batch 3046: loss 0.123812\n",
      "batch 3047: loss 0.111245\n",
      "batch 3048: loss 0.097438\n",
      "batch 3049: loss 0.077460\n",
      "batch 3050: loss 0.159647\n",
      "batch 3051: loss 0.283501\n",
      "batch 3052: loss 0.125400\n",
      "batch 3053: loss 0.073604\n",
      "batch 3054: loss 0.202913\n",
      "batch 3055: loss 0.080929\n",
      "batch 3056: loss 0.035352\n",
      "batch 3057: loss 0.275389\n",
      "batch 3058: loss 0.053484\n",
      "batch 3059: loss 0.057937\n",
      "batch 3060: loss 0.103730\n",
      "batch 3061: loss 0.031705\n",
      "batch 3062: loss 0.016938\n",
      "batch 3063: loss 0.040420\n",
      "batch 3064: loss 0.070993\n",
      "batch 3065: loss 0.085120\n",
      "batch 3066: loss 0.100113\n",
      "batch 3067: loss 0.223492\n",
      "batch 3068: loss 0.020876\n",
      "batch 3069: loss 0.158439\n",
      "batch 3070: loss 0.047981\n",
      "batch 3071: loss 0.086904\n",
      "batch 3072: loss 0.293062\n",
      "batch 3073: loss 0.132911\n",
      "batch 3074: loss 0.065335\n",
      "batch 3075: loss 0.084514\n",
      "batch 3076: loss 0.063487\n",
      "batch 3077: loss 0.064101\n",
      "batch 3078: loss 0.041315\n",
      "batch 3079: loss 0.094183\n",
      "batch 3080: loss 0.065274\n",
      "batch 3081: loss 0.144498\n",
      "batch 3082: loss 0.094525\n",
      "batch 3083: loss 0.032886\n",
      "batch 3084: loss 0.100408\n",
      "batch 3085: loss 0.055725\n",
      "batch 3086: loss 0.197933\n",
      "batch 3087: loss 0.041755\n",
      "batch 3088: loss 0.023825\n",
      "batch 3089: loss 0.139986\n",
      "batch 3090: loss 0.085588\n",
      "batch 3091: loss 0.093414\n",
      "batch 3092: loss 0.122154\n",
      "batch 3093: loss 0.045076\n",
      "batch 3094: loss 0.021671\n",
      "batch 3095: loss 0.020839\n",
      "batch 3096: loss 0.031417\n",
      "batch 3097: loss 0.110042\n",
      "batch 3098: loss 0.052891\n",
      "batch 3099: loss 0.088663\n",
      "batch 3100: loss 0.018527\n",
      "batch 3101: loss 0.043907\n",
      "batch 3102: loss 0.093902\n",
      "batch 3103: loss 0.010289\n",
      "batch 3104: loss 0.096593\n",
      "batch 3105: loss 0.298089\n",
      "batch 3106: loss 0.029723\n",
      "batch 3107: loss 0.099238\n",
      "batch 3108: loss 0.119319\n",
      "batch 3109: loss 0.024223\n",
      "batch 3110: loss 0.132005\n",
      "batch 3111: loss 0.195847\n",
      "batch 3112: loss 0.050137\n",
      "batch 3113: loss 0.048032\n",
      "batch 3114: loss 0.246710\n",
      "batch 3115: loss 0.051960\n",
      "batch 3116: loss 0.031740\n",
      "batch 3117: loss 0.136430\n",
      "batch 3118: loss 0.037935\n",
      "batch 3119: loss 0.029665\n",
      "batch 3120: loss 0.065790\n",
      "batch 3121: loss 0.018448\n",
      "batch 3122: loss 0.122590\n",
      "batch 3123: loss 0.067615\n",
      "batch 3124: loss 0.039794\n",
      "batch 3125: loss 0.017691\n",
      "batch 3126: loss 0.061479\n",
      "batch 3127: loss 0.139951\n",
      "batch 3128: loss 0.079736\n",
      "batch 3129: loss 0.065262\n",
      "batch 3130: loss 0.126485\n",
      "batch 3131: loss 0.178666\n",
      "batch 3132: loss 0.041880\n",
      "batch 3133: loss 0.022803\n",
      "batch 3134: loss 0.029167\n",
      "batch 3135: loss 0.181652\n",
      "batch 3136: loss 0.033475\n",
      "batch 3137: loss 0.033763\n",
      "batch 3138: loss 0.038107\n",
      "batch 3139: loss 0.031615\n",
      "batch 3140: loss 0.045668\n",
      "batch 3141: loss 0.119375\n",
      "batch 3142: loss 0.108855\n",
      "batch 3143: loss 0.129235\n",
      "batch 3144: loss 0.207413\n",
      "batch 3145: loss 0.032251\n",
      "batch 3146: loss 0.044709\n",
      "batch 3147: loss 0.114637\n",
      "batch 3148: loss 0.040244\n",
      "batch 3149: loss 0.110726\n",
      "batch 3150: loss 0.117215\n",
      "batch 3151: loss 0.035837\n",
      "batch 3152: loss 0.022131\n",
      "batch 3153: loss 0.052206\n",
      "batch 3154: loss 0.123368\n",
      "batch 3155: loss 0.120981\n",
      "batch 3156: loss 0.137517\n",
      "batch 3157: loss 0.161590\n",
      "batch 3158: loss 0.162017\n",
      "batch 3159: loss 0.078763\n",
      "batch 3160: loss 0.154160\n",
      "batch 3161: loss 0.081025\n",
      "batch 3162: loss 0.091824\n",
      "batch 3163: loss 0.037059\n",
      "batch 3164: loss 0.067993\n",
      "batch 3165: loss 0.142727\n",
      "batch 3166: loss 0.142771\n",
      "batch 3167: loss 0.078008\n",
      "batch 3168: loss 0.129275\n",
      "batch 3169: loss 0.079582\n",
      "batch 3170: loss 0.095969\n",
      "batch 3171: loss 0.016338\n",
      "batch 3172: loss 0.029185\n",
      "batch 3173: loss 0.112725\n",
      "batch 3174: loss 0.135329\n",
      "batch 3175: loss 0.152128\n",
      "batch 3176: loss 0.031297\n",
      "batch 3177: loss 0.054279\n",
      "batch 3178: loss 0.198123\n",
      "batch 3179: loss 0.047834\n",
      "batch 3180: loss 0.167906\n",
      "batch 3181: loss 0.269651\n",
      "batch 3182: loss 0.110365\n",
      "batch 3183: loss 0.068666\n",
      "batch 3184: loss 0.021467\n",
      "batch 3185: loss 0.026791\n",
      "batch 3186: loss 0.112866\n",
      "batch 3187: loss 0.215135\n",
      "batch 3188: loss 0.173751\n",
      "batch 3189: loss 0.321237\n",
      "batch 3190: loss 0.149926\n",
      "batch 3191: loss 0.071609\n",
      "batch 3192: loss 0.028025\n",
      "batch 3193: loss 0.132302\n",
      "batch 3194: loss 0.021558\n",
      "batch 3195: loss 0.054573\n",
      "batch 3196: loss 0.127062\n",
      "batch 3197: loss 0.062078\n",
      "batch 3198: loss 0.097259\n",
      "batch 3199: loss 0.028015\n",
      "batch 3200: loss 0.049164\n",
      "batch 3201: loss 0.059705\n",
      "batch 3202: loss 0.027612\n",
      "batch 3203: loss 0.179275\n",
      "batch 3204: loss 0.078052\n",
      "batch 3205: loss 0.058627\n",
      "batch 3206: loss 0.079053\n",
      "batch 3207: loss 0.073919\n",
      "batch 3208: loss 0.094973\n",
      "batch 3209: loss 0.036922\n",
      "batch 3210: loss 0.106498\n",
      "batch 3211: loss 0.055314\n",
      "batch 3212: loss 0.045405\n",
      "batch 3213: loss 0.080165\n",
      "batch 3214: loss 0.028350\n",
      "batch 3215: loss 0.042332\n",
      "batch 3216: loss 0.057143\n",
      "batch 3217: loss 0.052994\n",
      "batch 3218: loss 0.130220\n",
      "batch 3219: loss 0.086281\n",
      "batch 3220: loss 0.178719\n",
      "batch 3221: loss 0.024247\n",
      "batch 3222: loss 0.052005\n",
      "batch 3223: loss 0.025966\n",
      "batch 3224: loss 0.033944\n",
      "batch 3225: loss 0.061330\n",
      "batch 3226: loss 0.069213\n",
      "batch 3227: loss 0.023421\n",
      "batch 3228: loss 0.033314\n",
      "batch 3229: loss 0.099958\n",
      "batch 3230: loss 0.111721\n",
      "batch 3231: loss 0.092206\n",
      "batch 3232: loss 0.148907\n",
      "batch 3233: loss 0.051095\n",
      "batch 3234: loss 0.137287\n",
      "batch 3235: loss 0.127147\n",
      "batch 3236: loss 0.156451\n",
      "batch 3237: loss 0.176697\n",
      "batch 3238: loss 0.029237\n",
      "batch 3239: loss 0.060401\n",
      "batch 3240: loss 0.107050\n",
      "batch 3241: loss 0.026067\n",
      "batch 3242: loss 0.066531\n",
      "batch 3243: loss 0.070271\n",
      "batch 3244: loss 0.126471\n",
      "batch 3245: loss 0.152409\n",
      "batch 3246: loss 0.062471\n",
      "batch 3247: loss 0.143470\n",
      "batch 3248: loss 0.059468\n",
      "batch 3249: loss 0.032682\n",
      "batch 3250: loss 0.131828\n",
      "batch 3251: loss 0.073538\n",
      "batch 3252: loss 0.156198\n",
      "batch 3253: loss 0.100860\n",
      "batch 3254: loss 0.127264\n",
      "batch 3255: loss 0.022726\n",
      "batch 3256: loss 0.141586\n",
      "batch 3257: loss 0.056158\n",
      "batch 3258: loss 0.069657\n",
      "batch 3259: loss 0.089438\n",
      "batch 3260: loss 0.064749\n",
      "batch 3261: loss 0.016908\n",
      "batch 3262: loss 0.172525\n",
      "batch 3263: loss 0.161568\n",
      "batch 3264: loss 0.027552\n",
      "batch 3265: loss 0.156336\n",
      "batch 3266: loss 0.177926\n",
      "batch 3267: loss 0.070341\n",
      "batch 3268: loss 0.085470\n",
      "batch 3269: loss 0.153733\n",
      "batch 3270: loss 0.078370\n",
      "batch 3271: loss 0.154904\n",
      "batch 3272: loss 0.093917\n",
      "batch 3273: loss 0.153393\n",
      "batch 3274: loss 0.092266\n",
      "batch 3275: loss 0.077068\n",
      "batch 3276: loss 0.104807\n",
      "batch 3277: loss 0.128971\n",
      "batch 3278: loss 0.035728\n",
      "batch 3279: loss 0.134881\n",
      "batch 3280: loss 0.026725\n",
      "batch 3281: loss 0.055691\n",
      "batch 3282: loss 0.032832\n",
      "batch 3283: loss 0.047793\n",
      "batch 3284: loss 0.074185\n",
      "batch 3285: loss 0.022248\n",
      "batch 3286: loss 0.064279\n",
      "batch 3287: loss 0.037468\n",
      "batch 3288: loss 0.134397\n",
      "batch 3289: loss 0.199603\n",
      "batch 3290: loss 0.046734\n",
      "batch 3291: loss 0.059935\n",
      "batch 3292: loss 0.268464\n",
      "batch 3293: loss 0.056718\n",
      "batch 3294: loss 0.054489\n",
      "batch 3295: loss 0.195660\n",
      "batch 3296: loss 0.082734\n",
      "batch 3297: loss 0.100468\n",
      "batch 3298: loss 0.069703\n",
      "batch 3299: loss 0.014989\n",
      "batch 3300: loss 0.093450\n",
      "batch 3301: loss 0.119102\n",
      "batch 3302: loss 0.075098\n",
      "batch 3303: loss 0.042396\n",
      "batch 3304: loss 0.120594\n",
      "batch 3305: loss 0.050025\n",
      "batch 3306: loss 0.027844\n",
      "batch 3307: loss 0.117303\n",
      "batch 3308: loss 0.190597\n",
      "batch 3309: loss 0.015833\n",
      "batch 3310: loss 0.070934\n",
      "batch 3311: loss 0.112389\n",
      "batch 3312: loss 0.051099\n",
      "batch 3313: loss 0.121146\n",
      "batch 3314: loss 0.142740\n",
      "batch 3315: loss 0.262015\n",
      "batch 3316: loss 0.032246\n",
      "batch 3317: loss 0.043612\n",
      "batch 3318: loss 0.051525\n",
      "batch 3319: loss 0.059098\n",
      "batch 3320: loss 0.123015\n",
      "batch 3321: loss 0.176820\n",
      "batch 3322: loss 0.026149\n",
      "batch 3323: loss 0.018789\n",
      "batch 3324: loss 0.104151\n",
      "batch 3325: loss 0.042601\n",
      "batch 3326: loss 0.076855\n",
      "batch 3327: loss 0.033395\n",
      "batch 3328: loss 0.051900\n",
      "batch 3329: loss 0.098874\n",
      "batch 3330: loss 0.047288\n",
      "batch 3331: loss 0.088063\n",
      "batch 3332: loss 0.052186\n",
      "batch 3333: loss 0.039370\n",
      "batch 3334: loss 0.057218\n",
      "batch 3335: loss 0.076294\n",
      "batch 3336: loss 0.078358\n",
      "batch 3337: loss 0.100586\n",
      "batch 3338: loss 0.102437\n",
      "batch 3339: loss 0.052216\n",
      "batch 3340: loss 0.042307\n",
      "batch 3341: loss 0.105913\n",
      "batch 3342: loss 0.073748\n",
      "batch 3343: loss 0.252884\n",
      "batch 3344: loss 0.121825\n",
      "batch 3345: loss 0.016371\n",
      "batch 3346: loss 0.088160\n",
      "batch 3347: loss 0.099598\n",
      "batch 3348: loss 0.105653\n",
      "batch 3349: loss 0.155851\n",
      "batch 3350: loss 0.065513\n",
      "batch 3351: loss 0.071787\n",
      "batch 3352: loss 0.053771\n",
      "batch 3353: loss 0.128098\n",
      "batch 3354: loss 0.252284\n",
      "batch 3355: loss 0.128644\n",
      "batch 3356: loss 0.018513\n",
      "batch 3357: loss 0.034421\n",
      "batch 3358: loss 0.188952\n",
      "batch 3359: loss 0.030839\n",
      "batch 3360: loss 0.041500\n",
      "batch 3361: loss 0.024740\n",
      "batch 3362: loss 0.059802\n",
      "batch 3363: loss 0.068853\n",
      "batch 3364: loss 0.122515\n",
      "batch 3365: loss 0.051195\n",
      "batch 3366: loss 0.124069\n",
      "batch 3367: loss 0.198719\n",
      "batch 3368: loss 0.099266\n",
      "batch 3369: loss 0.048544\n",
      "batch 3370: loss 0.108500\n",
      "batch 3371: loss 0.063887\n",
      "batch 3372: loss 0.047642\n",
      "batch 3373: loss 0.060590\n",
      "batch 3374: loss 0.119758\n",
      "batch 3375: loss 0.212820\n",
      "batch 3376: loss 0.064579\n",
      "batch 3377: loss 0.053043\n",
      "batch 3378: loss 0.034289\n",
      "batch 3379: loss 0.109164\n",
      "batch 3380: loss 0.060581\n",
      "batch 3381: loss 0.021072\n",
      "batch 3382: loss 0.147592\n",
      "batch 3383: loss 0.144210\n",
      "batch 3384: loss 0.100518\n",
      "batch 3385: loss 0.027537\n",
      "batch 3386: loss 0.037665\n",
      "batch 3387: loss 0.051151\n",
      "batch 3388: loss 0.102158\n",
      "batch 3389: loss 0.083000\n",
      "batch 3390: loss 0.076709\n",
      "batch 3391: loss 0.068006\n",
      "batch 3392: loss 0.022871\n",
      "batch 3393: loss 0.054429\n",
      "batch 3394: loss 0.041549\n",
      "batch 3395: loss 0.020580\n",
      "batch 3396: loss 0.020110\n",
      "batch 3397: loss 0.076269\n",
      "batch 3398: loss 0.117786\n",
      "batch 3399: loss 0.030685\n",
      "batch 3400: loss 0.080846\n",
      "batch 3401: loss 0.119356\n",
      "batch 3402: loss 0.146497\n",
      "batch 3403: loss 0.038865\n",
      "batch 3404: loss 0.021308\n",
      "batch 3405: loss 0.118585\n",
      "batch 3406: loss 0.013881\n",
      "batch 3407: loss 0.066284\n",
      "batch 3408: loss 0.064742\n",
      "batch 3409: loss 0.055236\n",
      "batch 3410: loss 0.019077\n",
      "batch 3411: loss 0.155345\n",
      "batch 3412: loss 0.041752\n",
      "batch 3413: loss 0.161408\n",
      "batch 3414: loss 0.140419\n",
      "batch 3415: loss 0.069939\n",
      "batch 3416: loss 0.015831\n",
      "batch 3417: loss 0.160343\n",
      "batch 3418: loss 0.022067\n",
      "batch 3419: loss 0.017118\n",
      "batch 3420: loss 0.138498\n",
      "batch 3421: loss 0.172608\n",
      "batch 3422: loss 0.076815\n",
      "batch 3423: loss 0.053240\n",
      "batch 3424: loss 0.032443\n",
      "batch 3425: loss 0.044300\n",
      "batch 3426: loss 0.013116\n",
      "batch 3427: loss 0.014211\n",
      "batch 3428: loss 0.017304\n",
      "batch 3429: loss 0.043018\n",
      "batch 3430: loss 0.022193\n",
      "batch 3431: loss 0.030576\n",
      "batch 3432: loss 0.097873\n",
      "batch 3433: loss 0.091091\n",
      "batch 3434: loss 0.032164\n",
      "batch 3435: loss 0.040105\n",
      "batch 3436: loss 0.073666\n",
      "batch 3437: loss 0.037052\n",
      "batch 3438: loss 0.090964\n",
      "batch 3439: loss 0.048232\n",
      "batch 3440: loss 0.043196\n",
      "batch 3441: loss 0.023161\n",
      "batch 3442: loss 0.034435\n",
      "batch 3443: loss 0.028656\n",
      "batch 3444: loss 0.039530\n",
      "batch 3445: loss 0.048829\n",
      "batch 3446: loss 0.009104\n",
      "batch 3447: loss 0.075134\n",
      "batch 3448: loss 0.061725\n",
      "batch 3449: loss 0.140666\n",
      "batch 3450: loss 0.042230\n",
      "batch 3451: loss 0.018369\n",
      "batch 3452: loss 0.067185\n",
      "batch 3453: loss 0.063568\n",
      "batch 3454: loss 0.148413\n",
      "batch 3455: loss 0.068639\n",
      "batch 3456: loss 0.091755\n",
      "batch 3457: loss 0.032524\n",
      "batch 3458: loss 0.054576\n",
      "batch 3459: loss 0.040605\n",
      "batch 3460: loss 0.041282\n",
      "batch 3461: loss 0.041977\n",
      "batch 3462: loss 0.119710\n",
      "batch 3463: loss 0.152188\n",
      "batch 3464: loss 0.029817\n",
      "batch 3465: loss 0.068824\n",
      "batch 3466: loss 0.015362\n",
      "batch 3467: loss 0.039539\n",
      "batch 3468: loss 0.035029\n",
      "batch 3469: loss 0.040496\n",
      "batch 3470: loss 0.010329\n",
      "batch 3471: loss 0.033802\n",
      "batch 3472: loss 0.046559\n",
      "batch 3473: loss 0.060653\n",
      "batch 3474: loss 0.059107\n",
      "batch 3475: loss 0.047390\n",
      "batch 3476: loss 0.017858\n",
      "batch 3477: loss 0.082478\n",
      "batch 3478: loss 0.015483\n",
      "batch 3479: loss 0.025911\n",
      "batch 3480: loss 0.135741\n",
      "batch 3481: loss 0.060980\n",
      "batch 3482: loss 0.010651\n",
      "batch 3483: loss 0.008071\n",
      "batch 3484: loss 0.024813\n",
      "batch 3485: loss 0.043575\n",
      "batch 3486: loss 0.039217\n",
      "batch 3487: loss 0.040620\n",
      "batch 3488: loss 0.064111\n",
      "batch 3489: loss 0.020365\n",
      "batch 3490: loss 0.097866\n",
      "batch 3491: loss 0.016593\n",
      "batch 3492: loss 0.019784\n",
      "batch 3493: loss 0.020376\n",
      "batch 3494: loss 0.041960\n",
      "batch 3495: loss 0.062328\n",
      "batch 3496: loss 0.055622\n",
      "batch 3497: loss 0.213599\n",
      "batch 3498: loss 0.138839\n",
      "batch 3499: loss 0.151551\n",
      "batch 3500: loss 0.105405\n",
      "batch 3501: loss 0.048729\n",
      "batch 3502: loss 0.022407\n",
      "batch 3503: loss 0.017021\n",
      "batch 3504: loss 0.064141\n",
      "batch 3505: loss 0.034791\n",
      "batch 3506: loss 0.065172\n",
      "batch 3507: loss 0.133575\n",
      "batch 3508: loss 0.060337\n",
      "batch 3509: loss 0.023719\n",
      "batch 3510: loss 0.146997\n",
      "batch 3511: loss 0.030646\n",
      "batch 3512: loss 0.099489\n",
      "batch 3513: loss 0.062196\n",
      "batch 3514: loss 0.018960\n",
      "batch 3515: loss 0.007944\n",
      "batch 3516: loss 0.041253\n",
      "batch 3517: loss 0.019875\n",
      "batch 3518: loss 0.026711\n",
      "batch 3519: loss 0.011575\n",
      "batch 3520: loss 0.081437\n",
      "batch 3521: loss 0.045362\n",
      "batch 3522: loss 0.055143\n",
      "batch 3523: loss 0.130531\n",
      "batch 3524: loss 0.145485\n",
      "batch 3525: loss 0.016597\n",
      "batch 3526: loss 0.172210\n",
      "batch 3527: loss 0.047575\n",
      "batch 3528: loss 0.049616\n",
      "batch 3529: loss 0.171348\n",
      "batch 3530: loss 0.060434\n",
      "batch 3531: loss 0.037995\n",
      "batch 3532: loss 0.087096\n",
      "batch 3533: loss 0.150911\n",
      "batch 3534: loss 0.049075\n",
      "batch 3535: loss 0.024175\n",
      "batch 3536: loss 0.251559\n",
      "batch 3537: loss 0.016444\n",
      "batch 3538: loss 0.006636\n",
      "batch 3539: loss 0.063272\n",
      "batch 3540: loss 0.302995\n",
      "batch 3541: loss 0.075753\n",
      "batch 3542: loss 0.111697\n",
      "batch 3543: loss 0.082823\n",
      "batch 3544: loss 0.180803\n",
      "batch 3545: loss 0.057912\n",
      "batch 3546: loss 0.007262\n",
      "batch 3547: loss 0.090260\n",
      "batch 3548: loss 0.018442\n",
      "batch 3549: loss 0.037825\n",
      "batch 3550: loss 0.096934\n",
      "batch 3551: loss 0.114612\n",
      "batch 3552: loss 0.038210\n",
      "batch 3553: loss 0.101834\n",
      "batch 3554: loss 0.040217\n",
      "batch 3555: loss 0.334877\n",
      "batch 3556: loss 0.126040\n",
      "batch 3557: loss 0.136783\n",
      "batch 3558: loss 0.098675\n",
      "batch 3559: loss 0.054244\n",
      "batch 3560: loss 0.096100\n",
      "batch 3561: loss 0.073544\n",
      "batch 3562: loss 0.088774\n",
      "batch 3563: loss 0.022183\n",
      "batch 3564: loss 0.108017\n",
      "batch 3565: loss 0.204392\n",
      "batch 3566: loss 0.135781\n",
      "batch 3567: loss 0.121106\n",
      "batch 3568: loss 0.050095\n",
      "batch 3569: loss 0.053590\n",
      "batch 3570: loss 0.188707\n",
      "batch 3571: loss 0.073963\n",
      "batch 3572: loss 0.061491\n",
      "batch 3573: loss 0.036389\n",
      "batch 3574: loss 0.040813\n",
      "batch 3575: loss 0.013476\n",
      "batch 3576: loss 0.058164\n",
      "batch 3577: loss 0.038320\n",
      "batch 3578: loss 0.038971\n",
      "batch 3579: loss 0.010777\n",
      "batch 3580: loss 0.264521\n",
      "batch 3581: loss 0.174982\n",
      "batch 3582: loss 0.048884\n",
      "batch 3583: loss 0.105575\n",
      "batch 3584: loss 0.108820\n",
      "batch 3585: loss 0.263310\n",
      "batch 3586: loss 0.025984\n",
      "batch 3587: loss 0.010797\n",
      "batch 3588: loss 0.117171\n",
      "batch 3589: loss 0.080315\n",
      "batch 3590: loss 0.026125\n",
      "batch 3591: loss 0.210102\n",
      "batch 3592: loss 0.056392\n",
      "batch 3593: loss 0.229039\n",
      "batch 3594: loss 0.069440\n",
      "batch 3595: loss 0.056097\n",
      "batch 3596: loss 0.050911\n",
      "batch 3597: loss 0.161955\n",
      "batch 3598: loss 0.013075\n",
      "batch 3599: loss 0.092088\n",
      "batch 3600: loss 0.031260\n",
      "batch 3601: loss 0.044571\n",
      "batch 3602: loss 0.061475\n",
      "batch 3603: loss 0.049144\n",
      "batch 3604: loss 0.086611\n",
      "batch 3605: loss 0.021699\n",
      "batch 3606: loss 0.045118\n",
      "batch 3607: loss 0.106274\n",
      "batch 3608: loss 0.023670\n",
      "batch 3609: loss 0.044491\n",
      "batch 3610: loss 0.045423\n",
      "batch 3611: loss 0.035068\n",
      "batch 3612: loss 0.027574\n",
      "batch 3613: loss 0.012321\n",
      "batch 3614: loss 0.034547\n",
      "batch 3615: loss 0.087004\n",
      "batch 3616: loss 0.077161\n",
      "batch 3617: loss 0.097719\n",
      "batch 3618: loss 0.013824\n",
      "batch 3619: loss 0.091695\n",
      "batch 3620: loss 0.048649\n",
      "batch 3621: loss 0.026726\n",
      "batch 3622: loss 0.022414\n",
      "batch 3623: loss 0.029290\n",
      "batch 3624: loss 0.089517\n",
      "batch 3625: loss 0.071250\n",
      "batch 3626: loss 0.043736\n",
      "batch 3627: loss 0.074633\n",
      "batch 3628: loss 0.057967\n",
      "batch 3629: loss 0.063591\n",
      "batch 3630: loss 0.131667\n",
      "batch 3631: loss 0.035155\n",
      "batch 3632: loss 0.092539\n",
      "batch 3633: loss 0.012447\n",
      "batch 3634: loss 0.022643\n",
      "batch 3635: loss 0.044624\n",
      "batch 3636: loss 0.022446\n",
      "batch 3637: loss 0.036157\n",
      "batch 3638: loss 0.025949\n",
      "batch 3639: loss 0.107916\n",
      "batch 3640: loss 0.050483\n",
      "batch 3641: loss 0.031223\n",
      "batch 3642: loss 0.097264\n",
      "batch 3643: loss 0.049511\n",
      "batch 3644: loss 0.114848\n",
      "batch 3645: loss 0.022845\n",
      "batch 3646: loss 0.068553\n",
      "batch 3647: loss 0.135203\n",
      "batch 3648: loss 0.115058\n",
      "batch 3649: loss 0.096670\n",
      "batch 3650: loss 0.147922\n",
      "batch 3651: loss 0.080757\n",
      "batch 3652: loss 0.044046\n",
      "batch 3653: loss 0.040929\n",
      "batch 3654: loss 0.006277\n",
      "batch 3655: loss 0.084337\n",
      "batch 3656: loss 0.104446\n",
      "batch 3657: loss 0.076340\n",
      "batch 3658: loss 0.011336\n",
      "batch 3659: loss 0.092535\n",
      "batch 3660: loss 0.011127\n",
      "batch 3661: loss 0.030510\n",
      "batch 3662: loss 0.109855\n",
      "batch 3663: loss 0.166271\n",
      "batch 3664: loss 0.225303\n",
      "batch 3665: loss 0.046266\n",
      "batch 3666: loss 0.071244\n",
      "batch 3667: loss 0.050823\n",
      "batch 3668: loss 0.049653\n",
      "batch 3669: loss 0.095101\n",
      "batch 3670: loss 0.058037\n",
      "batch 3671: loss 0.031147\n",
      "batch 3672: loss 0.057037\n",
      "batch 3673: loss 0.111573\n",
      "batch 3674: loss 0.024140\n",
      "batch 3675: loss 0.028372\n",
      "batch 3676: loss 0.070466\n",
      "batch 3677: loss 0.044584\n",
      "batch 3678: loss 0.084852\n",
      "batch 3679: loss 0.080247\n",
      "batch 3680: loss 0.122510\n",
      "batch 3681: loss 0.088311\n",
      "batch 3682: loss 0.059953\n",
      "batch 3683: loss 0.121151\n",
      "batch 3684: loss 0.023137\n",
      "batch 3685: loss 0.030856\n",
      "batch 3686: loss 0.074344\n",
      "batch 3687: loss 0.221260\n",
      "batch 3688: loss 0.015636\n",
      "batch 3689: loss 0.072109\n",
      "batch 3690: loss 0.058114\n",
      "batch 3691: loss 0.053627\n",
      "batch 3692: loss 0.119927\n",
      "batch 3693: loss 0.042055\n",
      "batch 3694: loss 0.119300\n",
      "batch 3695: loss 0.048735\n",
      "batch 3696: loss 0.046165\n",
      "batch 3697: loss 0.045660\n",
      "batch 3698: loss 0.047763\n",
      "batch 3699: loss 0.012358\n",
      "batch 3700: loss 0.055969\n",
      "batch 3701: loss 0.100564\n",
      "batch 3702: loss 0.012243\n",
      "batch 3703: loss 0.038604\n",
      "batch 3704: loss 0.049687\n",
      "batch 3705: loss 0.075449\n",
      "batch 3706: loss 0.028216\n",
      "batch 3707: loss 0.083344\n",
      "batch 3708: loss 0.085820\n",
      "batch 3709: loss 0.027634\n",
      "batch 3710: loss 0.048608\n",
      "batch 3711: loss 0.123394\n",
      "batch 3712: loss 0.058612\n",
      "batch 3713: loss 0.057743\n",
      "batch 3714: loss 0.027819\n",
      "batch 3715: loss 0.033053\n",
      "batch 3716: loss 0.058147\n",
      "batch 3717: loss 0.083653\n",
      "batch 3718: loss 0.166660\n",
      "batch 3719: loss 0.041806\n",
      "batch 3720: loss 0.358642\n",
      "batch 3721: loss 0.099149\n",
      "batch 3722: loss 0.059625\n",
      "batch 3723: loss 0.026927\n",
      "batch 3724: loss 0.025029\n",
      "batch 3725: loss 0.025339\n",
      "batch 3726: loss 0.045245\n",
      "batch 3727: loss 0.044307\n",
      "batch 3728: loss 0.028329\n",
      "batch 3729: loss 0.029833\n",
      "batch 3730: loss 0.108803\n",
      "batch 3731: loss 0.029321\n",
      "batch 3732: loss 0.026273\n",
      "batch 3733: loss 0.035227\n",
      "batch 3734: loss 0.054226\n",
      "batch 3735: loss 0.016826\n",
      "batch 3736: loss 0.173472\n",
      "batch 3737: loss 0.168727\n",
      "batch 3738: loss 0.011048\n",
      "batch 3739: loss 0.015421\n",
      "batch 3740: loss 0.030003\n",
      "batch 3741: loss 0.015518\n",
      "batch 3742: loss 0.084091\n",
      "batch 3743: loss 0.013925\n",
      "batch 3744: loss 0.204260\n",
      "batch 3745: loss 0.015525\n",
      "batch 3746: loss 0.016933\n",
      "batch 3747: loss 0.099093\n",
      "batch 3748: loss 0.057399\n",
      "batch 3749: loss 0.023785\n",
      "batch 3750: loss 0.067418\n",
      "batch 3751: loss 0.175360\n",
      "batch 3752: loss 0.150544\n",
      "batch 3753: loss 0.079714\n",
      "batch 3754: loss 0.025847\n",
      "batch 3755: loss 0.092377\n",
      "batch 3756: loss 0.039069\n",
      "batch 3757: loss 0.210347\n",
      "batch 3758: loss 0.047656\n",
      "batch 3759: loss 0.130331\n",
      "batch 3760: loss 0.139743\n",
      "batch 3761: loss 0.074146\n",
      "batch 3762: loss 0.019984\n",
      "batch 3763: loss 0.062298\n",
      "batch 3764: loss 0.047521\n",
      "batch 3765: loss 0.036067\n",
      "batch 3766: loss 0.078893\n",
      "batch 3767: loss 0.078555\n",
      "batch 3768: loss 0.076768\n",
      "batch 3769: loss 0.097173\n",
      "batch 3770: loss 0.016234\n",
      "batch 3771: loss 0.056626\n",
      "batch 3772: loss 0.036186\n",
      "batch 3773: loss 0.025289\n",
      "batch 3774: loss 0.155579\n",
      "batch 3775: loss 0.053843\n",
      "batch 3776: loss 0.048379\n",
      "batch 3777: loss 0.115369\n",
      "batch 3778: loss 0.013758\n",
      "batch 3779: loss 0.314810\n",
      "batch 3780: loss 0.049395\n",
      "batch 3781: loss 0.125577\n",
      "batch 3782: loss 0.134794\n",
      "batch 3783: loss 0.035005\n",
      "batch 3784: loss 0.126100\n",
      "batch 3785: loss 0.059076\n",
      "batch 3786: loss 0.076839\n",
      "batch 3787: loss 0.100653\n",
      "batch 3788: loss 0.063170\n",
      "batch 3789: loss 0.079057\n",
      "batch 3790: loss 0.055485\n",
      "batch 3791: loss 0.022404\n",
      "batch 3792: loss 0.149087\n",
      "batch 3793: loss 0.040758\n",
      "batch 3794: loss 0.091298\n",
      "batch 3795: loss 0.159644\n",
      "batch 3796: loss 0.039331\n",
      "batch 3797: loss 0.077319\n",
      "batch 3798: loss 0.061653\n",
      "batch 3799: loss 0.117428\n",
      "batch 3800: loss 0.084933\n",
      "batch 3801: loss 0.069317\n",
      "batch 3802: loss 0.043456\n",
      "batch 3803: loss 0.086311\n",
      "batch 3804: loss 0.012008\n",
      "batch 3805: loss 0.141295\n",
      "batch 3806: loss 0.032932\n",
      "batch 3807: loss 0.039600\n",
      "batch 3808: loss 0.022135\n",
      "batch 3809: loss 0.025542\n",
      "batch 3810: loss 0.090114\n",
      "batch 3811: loss 0.084926\n",
      "batch 3812: loss 0.091895\n",
      "batch 3813: loss 0.061109\n",
      "batch 3814: loss 0.033436\n",
      "batch 3815: loss 0.277166\n",
      "batch 3816: loss 0.018958\n",
      "batch 3817: loss 0.057930\n",
      "batch 3818: loss 0.081360\n",
      "batch 3819: loss 0.045740\n",
      "batch 3820: loss 0.110462\n",
      "batch 3821: loss 0.053003\n",
      "batch 3822: loss 0.025625\n",
      "batch 3823: loss 0.017220\n",
      "batch 3824: loss 0.057970\n",
      "batch 3825: loss 0.071990\n",
      "batch 3826: loss 0.051882\n",
      "batch 3827: loss 0.017353\n",
      "batch 3828: loss 0.071084\n",
      "batch 3829: loss 0.168418\n",
      "batch 3830: loss 0.110689\n",
      "batch 3831: loss 0.069875\n",
      "batch 3832: loss 0.016868\n",
      "batch 3833: loss 0.166574\n",
      "batch 3834: loss 0.085570\n",
      "batch 3835: loss 0.037621\n",
      "batch 3836: loss 0.097973\n",
      "batch 3837: loss 0.020526\n",
      "batch 3838: loss 0.077782\n",
      "batch 3839: loss 0.084306\n",
      "batch 3840: loss 0.025936\n",
      "batch 3841: loss 0.096575\n",
      "batch 3842: loss 0.049285\n",
      "batch 3843: loss 0.115200\n",
      "batch 3844: loss 0.024317\n",
      "batch 3845: loss 0.083294\n",
      "batch 3846: loss 0.123954\n",
      "batch 3847: loss 0.044255\n",
      "batch 3848: loss 0.107455\n",
      "batch 3849: loss 0.035930\n",
      "batch 3850: loss 0.040244\n",
      "batch 3851: loss 0.016108\n",
      "batch 3852: loss 0.055388\n",
      "batch 3853: loss 0.021389\n",
      "batch 3854: loss 0.093623\n",
      "batch 3855: loss 0.039263\n",
      "batch 3856: loss 0.025780\n",
      "batch 3857: loss 0.017744\n",
      "batch 3858: loss 0.121749\n",
      "batch 3859: loss 0.087216\n",
      "batch 3860: loss 0.105978\n",
      "batch 3861: loss 0.043470\n",
      "batch 3862: loss 0.127942\n",
      "batch 3863: loss 0.053064\n",
      "batch 3864: loss 0.035386\n",
      "batch 3865: loss 0.013712\n",
      "batch 3866: loss 0.054753\n",
      "batch 3867: loss 0.037163\n",
      "batch 3868: loss 0.118692\n",
      "batch 3869: loss 0.030174\n",
      "batch 3870: loss 0.035699\n",
      "batch 3871: loss 0.062538\n",
      "batch 3872: loss 0.088162\n",
      "batch 3873: loss 0.058105\n",
      "batch 3874: loss 0.015859\n",
      "batch 3875: loss 0.043948\n",
      "batch 3876: loss 0.017925\n",
      "batch 3877: loss 0.024466\n",
      "batch 3878: loss 0.027671\n",
      "batch 3879: loss 0.036313\n",
      "batch 3880: loss 0.028930\n",
      "batch 3881: loss 0.084780\n",
      "batch 3882: loss 0.019277\n",
      "batch 3883: loss 0.037452\n",
      "batch 3884: loss 0.046081\n",
      "batch 3885: loss 0.074553\n",
      "batch 3886: loss 0.048204\n",
      "batch 3887: loss 0.272454\n",
      "batch 3888: loss 0.008667\n",
      "batch 3889: loss 0.165173\n",
      "batch 3890: loss 0.045083\n",
      "batch 3891: loss 0.026973\n",
      "batch 3892: loss 0.170703\n",
      "batch 3893: loss 0.061283\n",
      "batch 3894: loss 0.077122\n",
      "batch 3895: loss 0.135538\n",
      "batch 3896: loss 0.050700\n",
      "batch 3897: loss 0.178163\n",
      "batch 3898: loss 0.033879\n",
      "batch 3899: loss 0.056681\n",
      "batch 3900: loss 0.016596\n",
      "batch 3901: loss 0.055774\n",
      "batch 3902: loss 0.034529\n",
      "batch 3903: loss 0.077331\n",
      "batch 3904: loss 0.039862\n",
      "batch 3905: loss 0.022222\n",
      "batch 3906: loss 0.022944\n",
      "batch 3907: loss 0.050664\n",
      "batch 3908: loss 0.038060\n",
      "batch 3909: loss 0.022467\n",
      "batch 3910: loss 0.150879\n",
      "batch 3911: loss 0.059875\n",
      "batch 3912: loss 0.047041\n",
      "batch 3913: loss 0.081613\n",
      "batch 3914: loss 0.010618\n",
      "batch 3915: loss 0.053928\n",
      "batch 3916: loss 0.333880\n",
      "batch 3917: loss 0.079093\n",
      "batch 3918: loss 0.149894\n",
      "batch 3919: loss 0.183020\n",
      "batch 3920: loss 0.065070\n",
      "batch 3921: loss 0.094745\n",
      "batch 3922: loss 0.165652\n",
      "batch 3923: loss 0.067056\n",
      "batch 3924: loss 0.086640\n",
      "batch 3925: loss 0.006616\n",
      "batch 3926: loss 0.021084\n",
      "batch 3927: loss 0.128445\n",
      "batch 3928: loss 0.035012\n",
      "batch 3929: loss 0.036013\n",
      "batch 3930: loss 0.035004\n",
      "batch 3931: loss 0.038378\n",
      "batch 3932: loss 0.052764\n",
      "batch 3933: loss 0.020957\n",
      "batch 3934: loss 0.032251\n",
      "batch 3935: loss 0.012504\n",
      "batch 3936: loss 0.120195\n",
      "batch 3937: loss 0.081732\n",
      "batch 3938: loss 0.052171\n",
      "batch 3939: loss 0.017540\n",
      "batch 3940: loss 0.124503\n",
      "batch 3941: loss 0.081731\n",
      "batch 3942: loss 0.073808\n",
      "batch 3943: loss 0.076625\n",
      "batch 3944: loss 0.049501\n",
      "batch 3945: loss 0.010160\n",
      "batch 3946: loss 0.026311\n",
      "batch 3947: loss 0.022515\n",
      "batch 3948: loss 0.119111\n",
      "batch 3949: loss 0.046295\n",
      "batch 3950: loss 0.030484\n",
      "batch 3951: loss 0.092772\n",
      "batch 3952: loss 0.021270\n",
      "batch 3953: loss 0.070298\n",
      "batch 3954: loss 0.158820\n",
      "batch 3955: loss 0.149658\n",
      "batch 3956: loss 0.059547\n",
      "batch 3957: loss 0.016224\n",
      "batch 3958: loss 0.079543\n",
      "batch 3959: loss 0.216227\n",
      "batch 3960: loss 0.075027\n",
      "batch 3961: loss 0.057185\n",
      "batch 3962: loss 0.087647\n",
      "batch 3963: loss 0.015022\n",
      "batch 3964: loss 0.194107\n",
      "batch 3965: loss 0.091802\n",
      "batch 3966: loss 0.017259\n",
      "batch 3967: loss 0.109996\n",
      "batch 3968: loss 0.124887\n",
      "batch 3969: loss 0.073024\n",
      "batch 3970: loss 0.078204\n",
      "batch 3971: loss 0.080968\n",
      "batch 3972: loss 0.057598\n",
      "batch 3973: loss 0.033711\n",
      "batch 3974: loss 0.014869\n",
      "batch 3975: loss 0.015071\n",
      "batch 3976: loss 0.076243\n",
      "batch 3977: loss 0.018454\n",
      "batch 3978: loss 0.062977\n",
      "batch 3979: loss 0.038269\n",
      "batch 3980: loss 0.052737\n",
      "batch 3981: loss 0.065599\n",
      "batch 3982: loss 0.179132\n",
      "batch 3983: loss 0.088099\n",
      "batch 3984: loss 0.054645\n",
      "batch 3985: loss 0.034277\n",
      "batch 3986: loss 0.245559\n",
      "batch 3987: loss 0.045396\n",
      "batch 3988: loss 0.021231\n",
      "batch 3989: loss 0.246547\n",
      "batch 3990: loss 0.035407\n",
      "batch 3991: loss 0.099613\n",
      "batch 3992: loss 0.013488\n",
      "batch 3993: loss 0.053422\n",
      "batch 3994: loss 0.112638\n",
      "batch 3995: loss 0.122240\n",
      "batch 3996: loss 0.055617\n",
      "batch 3997: loss 0.044395\n",
      "batch 3998: loss 0.150558\n",
      "batch 3999: loss 0.073422\n",
      "batch 4000: loss 0.017591\n",
      "batch 4001: loss 0.067021\n",
      "batch 4002: loss 0.052273\n",
      "batch 4003: loss 0.380758\n",
      "batch 4004: loss 0.059131\n",
      "batch 4005: loss 0.017138\n",
      "batch 4006: loss 0.065113\n",
      "batch 4007: loss 0.059301\n",
      "batch 4008: loss 0.078345\n",
      "batch 4009: loss 0.040454\n",
      "batch 4010: loss 0.071041\n",
      "batch 4011: loss 0.033103\n",
      "batch 4012: loss 0.014139\n",
      "batch 4013: loss 0.023372\n",
      "batch 4014: loss 0.088525\n",
      "batch 4015: loss 0.070403\n",
      "batch 4016: loss 0.009699\n",
      "batch 4017: loss 0.101632\n",
      "batch 4018: loss 0.096307\n",
      "batch 4019: loss 0.029755\n",
      "batch 4020: loss 0.033700\n",
      "batch 4021: loss 0.067779\n",
      "batch 4022: loss 0.019850\n",
      "batch 4023: loss 0.045717\n",
      "batch 4024: loss 0.053992\n",
      "batch 4025: loss 0.111000\n",
      "batch 4026: loss 0.115379\n",
      "batch 4027: loss 0.015647\n",
      "batch 4028: loss 0.092434\n",
      "batch 4029: loss 0.134266\n",
      "batch 4030: loss 0.059134\n",
      "batch 4031: loss 0.073759\n",
      "batch 4032: loss 0.021196\n",
      "batch 4033: loss 0.020839\n",
      "batch 4034: loss 0.028675\n",
      "batch 4035: loss 0.088525\n",
      "batch 4036: loss 0.021339\n",
      "batch 4037: loss 0.034002\n",
      "batch 4038: loss 0.024281\n",
      "batch 4039: loss 0.227337\n",
      "batch 4040: loss 0.022011\n",
      "batch 4041: loss 0.074674\n",
      "batch 4042: loss 0.300275\n",
      "batch 4043: loss 0.215480\n",
      "batch 4044: loss 0.038860\n",
      "batch 4045: loss 0.086583\n",
      "batch 4046: loss 0.049547\n",
      "batch 4047: loss 0.023523\n",
      "batch 4048: loss 0.007898\n",
      "batch 4049: loss 0.073095\n",
      "batch 4050: loss 0.149744\n",
      "batch 4051: loss 0.049693\n",
      "batch 4052: loss 0.073857\n",
      "batch 4053: loss 0.099788\n",
      "batch 4054: loss 0.157736\n",
      "batch 4055: loss 0.060577\n",
      "batch 4056: loss 0.096808\n",
      "batch 4057: loss 0.026568\n",
      "batch 4058: loss 0.024002\n",
      "batch 4059: loss 0.042722\n",
      "batch 4060: loss 0.024304\n",
      "batch 4061: loss 0.090913\n",
      "batch 4062: loss 0.067201\n",
      "batch 4063: loss 0.300122\n",
      "batch 4064: loss 0.038459\n",
      "batch 4065: loss 0.166016\n",
      "batch 4066: loss 0.054831\n",
      "batch 4067: loss 0.044093\n",
      "batch 4068: loss 0.011594\n",
      "batch 4069: loss 0.045104\n",
      "batch 4070: loss 0.032636\n",
      "batch 4071: loss 0.090279\n",
      "batch 4072: loss 0.025669\n",
      "batch 4073: loss 0.101452\n",
      "batch 4074: loss 0.082996\n",
      "batch 4075: loss 0.082488\n",
      "batch 4076: loss 0.070336\n",
      "batch 4077: loss 0.050547\n",
      "batch 4078: loss 0.069476\n",
      "batch 4079: loss 0.073908\n",
      "batch 4080: loss 0.228687\n",
      "batch 4081: loss 0.166420\n",
      "batch 4082: loss 0.015198\n",
      "batch 4083: loss 0.019086\n",
      "batch 4084: loss 0.026443\n",
      "batch 4085: loss 0.088163\n",
      "batch 4086: loss 0.021487\n",
      "batch 4087: loss 0.007655\n",
      "batch 4088: loss 0.009437\n",
      "batch 4089: loss 0.014155\n",
      "batch 4090: loss 0.047615\n",
      "batch 4091: loss 0.133865\n",
      "batch 4092: loss 0.045239\n",
      "batch 4093: loss 0.114487\n",
      "batch 4094: loss 0.078379\n",
      "batch 4095: loss 0.104240\n",
      "batch 4096: loss 0.100411\n",
      "batch 4097: loss 0.036404\n",
      "batch 4098: loss 0.092412\n",
      "batch 4099: loss 0.100741\n",
      "batch 4100: loss 0.017111\n",
      "batch 4101: loss 0.056044\n",
      "batch 4102: loss 0.144070\n",
      "batch 4103: loss 0.023231\n",
      "batch 4104: loss 0.042061\n",
      "batch 4105: loss 0.162910\n",
      "batch 4106: loss 0.036450\n",
      "batch 4107: loss 0.039170\n",
      "batch 4108: loss 0.094966\n",
      "batch 4109: loss 0.056543\n",
      "batch 4110: loss 0.121602\n",
      "batch 4111: loss 0.062050\n",
      "batch 4112: loss 0.051918\n",
      "batch 4113: loss 0.012724\n",
      "batch 4114: loss 0.036436\n",
      "batch 4115: loss 0.053485\n",
      "batch 4116: loss 0.071513\n",
      "batch 4117: loss 0.033389\n",
      "batch 4118: loss 0.134312\n",
      "batch 4119: loss 0.066849\n",
      "batch 4120: loss 0.039687\n",
      "batch 4121: loss 0.102526\n",
      "batch 4122: loss 0.010904\n",
      "batch 4123: loss 0.032321\n",
      "batch 4124: loss 0.024271\n",
      "batch 4125: loss 0.168036\n",
      "batch 4126: loss 0.016586\n",
      "batch 4127: loss 0.047648\n",
      "batch 4128: loss 0.116430\n",
      "batch 4129: loss 0.044601\n",
      "batch 4130: loss 0.039677\n",
      "batch 4131: loss 0.067799\n",
      "batch 4132: loss 0.013617\n",
      "batch 4133: loss 0.034453\n",
      "batch 4134: loss 0.028623\n",
      "batch 4135: loss 0.046050\n",
      "batch 4136: loss 0.039168\n",
      "batch 4137: loss 0.036895\n",
      "batch 4138: loss 0.018243\n",
      "batch 4139: loss 0.017423\n",
      "batch 4140: loss 0.107397\n",
      "batch 4141: loss 0.100852\n",
      "batch 4142: loss 0.361515\n",
      "batch 4143: loss 0.069496\n",
      "batch 4144: loss 0.067729\n",
      "batch 4145: loss 0.065839\n",
      "batch 4146: loss 0.091066\n",
      "batch 4147: loss 0.055352\n",
      "batch 4148: loss 0.046829\n",
      "batch 4149: loss 0.078750\n",
      "batch 4150: loss 0.081574\n",
      "batch 4151: loss 0.028553\n",
      "batch 4152: loss 0.054221\n",
      "batch 4153: loss 0.039990\n",
      "batch 4154: loss 0.015424\n",
      "batch 4155: loss 0.065726\n",
      "batch 4156: loss 0.022685\n",
      "batch 4157: loss 0.068162\n",
      "batch 4158: loss 0.007386\n",
      "batch 4159: loss 0.023631\n",
      "batch 4160: loss 0.025120\n",
      "batch 4161: loss 0.019701\n",
      "batch 4162: loss 0.056709\n",
      "batch 4163: loss 0.038985\n",
      "batch 4164: loss 0.009331\n",
      "batch 4165: loss 0.092558\n",
      "batch 4166: loss 0.179998\n",
      "batch 4167: loss 0.016047\n",
      "batch 4168: loss 0.029034\n",
      "batch 4169: loss 0.075350\n",
      "batch 4170: loss 0.065789\n",
      "batch 4171: loss 0.081856\n",
      "batch 4172: loss 0.029695\n",
      "batch 4173: loss 0.035597\n",
      "batch 4174: loss 0.014976\n",
      "batch 4175: loss 0.049198\n",
      "batch 4176: loss 0.013032\n",
      "batch 4177: loss 0.036105\n",
      "batch 4178: loss 0.044781\n",
      "batch 4179: loss 0.011292\n",
      "batch 4180: loss 0.011011\n",
      "batch 4181: loss 0.129826\n",
      "batch 4182: loss 0.128778\n",
      "batch 4183: loss 0.018952\n",
      "batch 4184: loss 0.026107\n",
      "batch 4185: loss 0.006317\n",
      "batch 4186: loss 0.033792\n",
      "batch 4187: loss 0.025936\n",
      "batch 4188: loss 0.190942\n",
      "batch 4189: loss 0.194369\n",
      "batch 4190: loss 0.080860\n",
      "batch 4191: loss 0.041057\n",
      "batch 4192: loss 0.068316\n",
      "batch 4193: loss 0.115452\n",
      "batch 4194: loss 0.145747\n",
      "batch 4195: loss 0.025740\n",
      "batch 4196: loss 0.020928\n",
      "batch 4197: loss 0.071212\n",
      "batch 4198: loss 0.048627\n",
      "batch 4199: loss 0.036330\n",
      "batch 4200: loss 0.031358\n",
      "batch 4201: loss 0.060672\n",
      "batch 4202: loss 0.160076\n",
      "batch 4203: loss 0.081614\n",
      "batch 4204: loss 0.183672\n",
      "batch 4205: loss 0.032090\n",
      "batch 4206: loss 0.017430\n",
      "batch 4207: loss 0.171844\n",
      "batch 4208: loss 0.059799\n",
      "batch 4209: loss 0.056332\n",
      "batch 4210: loss 0.167256\n",
      "batch 4211: loss 0.014850\n",
      "batch 4212: loss 0.032149\n",
      "batch 4213: loss 0.031200\n",
      "batch 4214: loss 0.097031\n",
      "batch 4215: loss 0.269703\n",
      "batch 4216: loss 0.066789\n",
      "batch 4217: loss 0.008572\n",
      "batch 4218: loss 0.019601\n",
      "batch 4219: loss 0.013572\n",
      "batch 4220: loss 0.030094\n",
      "batch 4221: loss 0.025519\n",
      "batch 4222: loss 0.049886\n",
      "batch 4223: loss 0.245412\n",
      "batch 4224: loss 0.038409\n",
      "batch 4225: loss 0.049886\n",
      "batch 4226: loss 0.085887\n",
      "batch 4227: loss 0.051139\n",
      "batch 4228: loss 0.063636\n",
      "batch 4229: loss 0.014489\n",
      "batch 4230: loss 0.158700\n",
      "batch 4231: loss 0.076742\n",
      "batch 4232: loss 0.051405\n",
      "batch 4233: loss 0.061033\n",
      "batch 4234: loss 0.074951\n",
      "batch 4235: loss 0.039788\n",
      "batch 4236: loss 0.017301\n",
      "batch 4237: loss 0.004995\n",
      "batch 4238: loss 0.066509\n",
      "batch 4239: loss 0.028844\n",
      "batch 4240: loss 0.044132\n",
      "batch 4241: loss 0.035377\n",
      "batch 4242: loss 0.194624\n",
      "batch 4243: loss 0.126513\n",
      "batch 4244: loss 0.020122\n",
      "batch 4245: loss 0.030413\n",
      "batch 4246: loss 0.079772\n",
      "batch 4247: loss 0.043836\n",
      "batch 4248: loss 0.032568\n",
      "batch 4249: loss 0.024018\n",
      "batch 4250: loss 0.019413\n",
      "batch 4251: loss 0.050910\n",
      "batch 4252: loss 0.030060\n",
      "batch 4253: loss 0.030056\n",
      "batch 4254: loss 0.090516\n",
      "batch 4255: loss 0.052170\n",
      "batch 4256: loss 0.011065\n",
      "batch 4257: loss 0.047756\n",
      "batch 4258: loss 0.058900\n",
      "batch 4259: loss 0.122241\n",
      "batch 4260: loss 0.187439\n",
      "batch 4261: loss 0.082271\n",
      "batch 4262: loss 0.067679\n",
      "batch 4263: loss 0.103905\n",
      "batch 4264: loss 0.098909\n",
      "batch 4265: loss 0.023242\n",
      "batch 4266: loss 0.170274\n",
      "batch 4267: loss 0.055309\n",
      "batch 4268: loss 0.055910\n",
      "batch 4269: loss 0.072110\n",
      "batch 4270: loss 0.066987\n",
      "batch 4271: loss 0.021312\n",
      "batch 4272: loss 0.116874\n",
      "batch 4273: loss 0.180571\n",
      "batch 4274: loss 0.061306\n",
      "batch 4275: loss 0.031092\n",
      "batch 4276: loss 0.055057\n",
      "batch 4277: loss 0.150210\n",
      "batch 4278: loss 0.148377\n",
      "batch 4279: loss 0.049000\n",
      "batch 4280: loss 0.069027\n",
      "batch 4281: loss 0.030599\n",
      "batch 4282: loss 0.013387\n",
      "batch 4283: loss 0.032614\n",
      "batch 4284: loss 0.061732\n",
      "batch 4285: loss 0.155003\n",
      "batch 4286: loss 0.125243\n",
      "batch 4287: loss 0.086460\n",
      "batch 4288: loss 0.252820\n",
      "batch 4289: loss 0.064620\n",
      "batch 4290: loss 0.048958\n",
      "batch 4291: loss 0.079475\n",
      "batch 4292: loss 0.177968\n",
      "batch 4293: loss 0.021516\n",
      "batch 4294: loss 0.073941\n",
      "batch 4295: loss 0.124522\n",
      "batch 4296: loss 0.058719\n",
      "batch 4297: loss 0.053441\n",
      "batch 4298: loss 0.149146\n",
      "batch 4299: loss 0.059563\n",
      "batch 4300: loss 0.145976\n",
      "batch 4301: loss 0.091083\n",
      "batch 4302: loss 0.071868\n",
      "batch 4303: loss 0.114353\n",
      "batch 4304: loss 0.062331\n",
      "batch 4305: loss 0.048042\n",
      "batch 4306: loss 0.006039\n",
      "batch 4307: loss 0.024849\n",
      "batch 4308: loss 0.014308\n",
      "batch 4309: loss 0.011255\n",
      "batch 4310: loss 0.128081\n",
      "batch 4311: loss 0.139410\n",
      "batch 4312: loss 0.116730\n",
      "batch 4313: loss 0.107086\n",
      "batch 4314: loss 0.089844\n",
      "batch 4315: loss 0.051933\n",
      "batch 4316: loss 0.027415\n",
      "batch 4317: loss 0.037453\n",
      "batch 4318: loss 0.146547\n",
      "batch 4319: loss 0.023213\n",
      "batch 4320: loss 0.120871\n",
      "batch 4321: loss 0.062725\n",
      "batch 4322: loss 0.023388\n",
      "batch 4323: loss 0.069913\n",
      "batch 4324: loss 0.012221\n",
      "batch 4325: loss 0.053965\n",
      "batch 4326: loss 0.011589\n",
      "batch 4327: loss 0.079842\n",
      "batch 4328: loss 0.015222\n",
      "batch 4329: loss 0.232853\n",
      "batch 4330: loss 0.028130\n",
      "batch 4331: loss 0.056029\n",
      "batch 4332: loss 0.087134\n",
      "batch 4333: loss 0.039807\n",
      "batch 4334: loss 0.054182\n",
      "batch 4335: loss 0.115269\n",
      "batch 4336: loss 0.049247\n",
      "batch 4337: loss 0.279280\n",
      "batch 4338: loss 0.029513\n",
      "batch 4339: loss 0.062185\n",
      "batch 4340: loss 0.017501\n",
      "batch 4341: loss 0.070731\n",
      "batch 4342: loss 0.077320\n",
      "batch 4343: loss 0.052027\n",
      "batch 4344: loss 0.012292\n",
      "batch 4345: loss 0.165329\n",
      "batch 4346: loss 0.051789\n",
      "batch 4347: loss 0.144278\n",
      "batch 4348: loss 0.057298\n",
      "batch 4349: loss 0.028329\n",
      "batch 4350: loss 0.017522\n",
      "batch 4351: loss 0.051674\n",
      "batch 4352: loss 0.073199\n",
      "batch 4353: loss 0.039500\n",
      "batch 4354: loss 0.016960\n",
      "batch 4355: loss 0.044989\n",
      "batch 4356: loss 0.049380\n",
      "batch 4357: loss 0.042109\n",
      "batch 4358: loss 0.079438\n",
      "batch 4359: loss 0.033767\n",
      "batch 4360: loss 0.017205\n",
      "batch 4361: loss 0.022951\n",
      "batch 4362: loss 0.203798\n",
      "batch 4363: loss 0.018696\n",
      "batch 4364: loss 0.104994\n",
      "batch 4365: loss 0.071634\n",
      "batch 4366: loss 0.024831\n",
      "batch 4367: loss 0.050574\n",
      "batch 4368: loss 0.136616\n",
      "batch 4369: loss 0.015933\n",
      "batch 4370: loss 0.111131\n",
      "batch 4371: loss 0.056756\n",
      "batch 4372: loss 0.065496\n",
      "batch 4373: loss 0.065795\n",
      "batch 4374: loss 0.019125\n",
      "batch 4375: loss 0.291957\n",
      "batch 4376: loss 0.033195\n",
      "batch 4377: loss 0.162981\n",
      "batch 4378: loss 0.111873\n",
      "batch 4379: loss 0.069119\n",
      "batch 4380: loss 0.026322\n",
      "batch 4381: loss 0.024641\n",
      "batch 4382: loss 0.038800\n",
      "batch 4383: loss 0.078990\n",
      "batch 4384: loss 0.049520\n",
      "batch 4385: loss 0.018522\n",
      "batch 4386: loss 0.025768\n",
      "batch 4387: loss 0.056268\n",
      "batch 4388: loss 0.032873\n",
      "batch 4389: loss 0.045944\n",
      "batch 4390: loss 0.175939\n",
      "batch 4391: loss 0.089210\n",
      "batch 4392: loss 0.039840\n",
      "batch 4393: loss 0.064373\n",
      "batch 4394: loss 0.148305\n",
      "batch 4395: loss 0.107551\n",
      "batch 4396: loss 0.069887\n",
      "batch 4397: loss 0.028596\n",
      "batch 4398: loss 0.048313\n",
      "batch 4399: loss 0.040577\n",
      "batch 4400: loss 0.098558\n",
      "batch 4401: loss 0.028931\n",
      "batch 4402: loss 0.054368\n",
      "batch 4403: loss 0.061393\n",
      "batch 4404: loss 0.047327\n",
      "batch 4405: loss 0.084730\n",
      "batch 4406: loss 0.101574\n",
      "batch 4407: loss 0.138024\n",
      "batch 4408: loss 0.180977\n",
      "batch 4409: loss 0.130426\n",
      "batch 4410: loss 0.131639\n",
      "batch 4411: loss 0.026239\n",
      "batch 4412: loss 0.040957\n",
      "batch 4413: loss 0.043447\n",
      "batch 4414: loss 0.008675\n",
      "batch 4415: loss 0.129061\n",
      "batch 4416: loss 0.041231\n",
      "batch 4417: loss 0.015361\n",
      "batch 4418: loss 0.010731\n",
      "batch 4419: loss 0.060086\n",
      "batch 4420: loss 0.066277\n",
      "batch 4421: loss 0.069552\n",
      "batch 4422: loss 0.024766\n",
      "batch 4423: loss 0.048380\n",
      "batch 4424: loss 0.013876\n",
      "batch 4425: loss 0.047658\n",
      "batch 4426: loss 0.022968\n",
      "batch 4427: loss 0.023786\n",
      "batch 4428: loss 0.044626\n",
      "batch 4429: loss 0.045176\n",
      "batch 4430: loss 0.069390\n",
      "batch 4431: loss 0.040742\n",
      "batch 4432: loss 0.021855\n",
      "batch 4433: loss 0.041167\n",
      "batch 4434: loss 0.040875\n",
      "batch 4435: loss 0.066589\n",
      "batch 4436: loss 0.053529\n",
      "batch 4437: loss 0.023537\n",
      "batch 4438: loss 0.042436\n",
      "batch 4439: loss 0.065400\n",
      "batch 4440: loss 0.011929\n",
      "batch 4441: loss 0.014566\n",
      "batch 4442: loss 0.068384\n",
      "batch 4443: loss 0.101122\n",
      "batch 4444: loss 0.106975\n",
      "batch 4445: loss 0.034185\n",
      "batch 4446: loss 0.166505\n",
      "batch 4447: loss 0.019613\n",
      "batch 4448: loss 0.182625\n",
      "batch 4449: loss 0.040449\n",
      "batch 4450: loss 0.044216\n",
      "batch 4451: loss 0.074966\n",
      "batch 4452: loss 0.117671\n",
      "batch 4453: loss 0.018740\n",
      "batch 4454: loss 0.013191\n",
      "batch 4455: loss 0.021130\n",
      "batch 4456: loss 0.036231\n",
      "batch 4457: loss 0.034371\n",
      "batch 4458: loss 0.045338\n",
      "batch 4459: loss 0.134772\n",
      "batch 4460: loss 0.037987\n",
      "batch 4461: loss 0.061183\n",
      "batch 4462: loss 0.046653\n",
      "batch 4463: loss 0.028684\n",
      "batch 4464: loss 0.006708\n",
      "batch 4465: loss 0.034120\n",
      "batch 4466: loss 0.024851\n",
      "batch 4467: loss 0.065136\n",
      "batch 4468: loss 0.038330\n",
      "batch 4469: loss 0.015115\n",
      "batch 4470: loss 0.033676\n",
      "batch 4471: loss 0.023605\n",
      "batch 4472: loss 0.023177\n",
      "batch 4473: loss 0.040106\n",
      "batch 4474: loss 0.012319\n",
      "batch 4475: loss 0.019011\n",
      "batch 4476: loss 0.011073\n",
      "batch 4477: loss 0.012982\n",
      "batch 4478: loss 0.222308\n",
      "batch 4479: loss 0.023866\n",
      "batch 4480: loss 0.037699\n",
      "batch 4481: loss 0.037864\n",
      "batch 4482: loss 0.049464\n",
      "batch 4483: loss 0.041041\n",
      "batch 4484: loss 0.017448\n",
      "batch 4485: loss 0.064038\n",
      "batch 4486: loss 0.038414\n",
      "batch 4487: loss 0.005767\n",
      "batch 4488: loss 0.048141\n",
      "batch 4489: loss 0.124351\n",
      "batch 4490: loss 0.107717\n",
      "batch 4491: loss 0.023028\n",
      "batch 4492: loss 0.033039\n",
      "batch 4493: loss 0.083631\n",
      "batch 4494: loss 0.132812\n",
      "batch 4495: loss 0.014979\n",
      "batch 4496: loss 0.011355\n",
      "batch 4497: loss 0.105781\n",
      "batch 4498: loss 0.029458\n",
      "batch 4499: loss 0.050132\n",
      "batch 4500: loss 0.054668\n",
      "batch 4501: loss 0.020691\n",
      "batch 4502: loss 0.078266\n",
      "batch 4503: loss 0.142080\n",
      "batch 4504: loss 0.125963\n",
      "batch 4505: loss 0.012313\n",
      "batch 4506: loss 0.019604\n",
      "batch 4507: loss 0.063178\n",
      "batch 4508: loss 0.019048\n",
      "batch 4509: loss 0.142340\n",
      "batch 4510: loss 0.102708\n",
      "batch 4511: loss 0.032469\n",
      "batch 4512: loss 0.034188\n",
      "batch 4513: loss 0.009667\n",
      "batch 4514: loss 0.014989\n",
      "batch 4515: loss 0.051937\n",
      "batch 4516: loss 0.025159\n",
      "batch 4517: loss 0.025266\n",
      "batch 4518: loss 0.074473\n",
      "batch 4519: loss 0.024110\n",
      "batch 4520: loss 0.062098\n",
      "batch 4521: loss 0.066439\n",
      "batch 4522: loss 0.060639\n",
      "batch 4523: loss 0.092365\n",
      "batch 4524: loss 0.007867\n",
      "batch 4525: loss 0.026375\n",
      "batch 4526: loss 0.065671\n",
      "batch 4527: loss 0.130685\n",
      "batch 4528: loss 0.100839\n",
      "batch 4529: loss 0.106079\n",
      "batch 4530: loss 0.066113\n",
      "batch 4531: loss 0.024738\n",
      "batch 4532: loss 0.237812\n",
      "batch 4533: loss 0.051229\n",
      "batch 4534: loss 0.219015\n",
      "batch 4535: loss 0.183392\n",
      "batch 4536: loss 0.069903\n",
      "batch 4537: loss 0.066122\n",
      "batch 4538: loss 0.038780\n",
      "batch 4539: loss 0.115533\n",
      "batch 4540: loss 0.027312\n",
      "batch 4541: loss 0.179556\n",
      "batch 4542: loss 0.126625\n",
      "batch 4543: loss 0.040394\n",
      "batch 4544: loss 0.025696\n",
      "batch 4545: loss 0.023914\n",
      "batch 4546: loss 0.103070\n",
      "batch 4547: loss 0.156043\n",
      "batch 4548: loss 0.049041\n",
      "batch 4549: loss 0.104315\n",
      "batch 4550: loss 0.104741\n",
      "batch 4551: loss 0.150252\n",
      "batch 4552: loss 0.083856\n",
      "batch 4553: loss 0.036822\n",
      "batch 4554: loss 0.084161\n",
      "batch 4555: loss 0.073759\n",
      "batch 4556: loss 0.175639\n",
      "batch 4557: loss 0.013185\n",
      "batch 4558: loss 0.034894\n",
      "batch 4559: loss 0.012587\n",
      "batch 4560: loss 0.112738\n",
      "batch 4561: loss 0.105572\n",
      "batch 4562: loss 0.026043\n",
      "batch 4563: loss 0.155900\n",
      "batch 4564: loss 0.047509\n",
      "batch 4565: loss 0.067662\n",
      "batch 4566: loss 0.059735\n",
      "batch 4567: loss 0.048744\n",
      "batch 4568: loss 0.131005\n",
      "batch 4569: loss 0.144561\n",
      "batch 4570: loss 0.074950\n",
      "batch 4571: loss 0.040930\n",
      "batch 4572: loss 0.020019\n",
      "batch 4573: loss 0.073986\n",
      "batch 4574: loss 0.006286\n",
      "batch 4575: loss 0.094154\n",
      "batch 4576: loss 0.052271\n",
      "batch 4577: loss 0.036186\n",
      "batch 4578: loss 0.052842\n",
      "batch 4579: loss 0.046147\n",
      "batch 4580: loss 0.068288\n",
      "batch 4581: loss 0.030368\n",
      "batch 4582: loss 0.035960\n",
      "batch 4583: loss 0.086891\n",
      "batch 4584: loss 0.014386\n",
      "batch 4585: loss 0.076747\n",
      "batch 4586: loss 0.031753\n",
      "batch 4587: loss 0.152756\n",
      "batch 4588: loss 0.011232\n",
      "batch 4589: loss 0.042649\n",
      "batch 4590: loss 0.009950\n",
      "batch 4591: loss 0.170064\n",
      "batch 4592: loss 0.015275\n",
      "batch 4593: loss 0.020730\n",
      "batch 4594: loss 0.011653\n",
      "batch 4595: loss 0.069556\n",
      "batch 4596: loss 0.047534\n",
      "batch 4597: loss 0.034956\n",
      "batch 4598: loss 0.107118\n",
      "batch 4599: loss 0.037053\n",
      "batch 4600: loss 0.006806\n",
      "batch 4601: loss 0.022758\n",
      "batch 4602: loss 0.027291\n",
      "batch 4603: loss 0.092477\n",
      "batch 4604: loss 0.017525\n",
      "batch 4605: loss 0.072634\n",
      "batch 4606: loss 0.078567\n",
      "batch 4607: loss 0.038126\n",
      "batch 4608: loss 0.012021\n",
      "batch 4609: loss 0.011635\n",
      "batch 4610: loss 0.038259\n",
      "batch 4611: loss 0.136679\n",
      "batch 4612: loss 0.029331\n",
      "batch 4613: loss 0.007304\n",
      "batch 4614: loss 0.176665\n",
      "batch 4615: loss 0.006567\n",
      "batch 4616: loss 0.010471\n",
      "batch 4617: loss 0.011287\n",
      "batch 4618: loss 0.026266\n",
      "batch 4619: loss 0.093668\n",
      "batch 4620: loss 0.017353\n",
      "batch 4621: loss 0.050132\n",
      "batch 4622: loss 0.121984\n",
      "batch 4623: loss 0.064235\n",
      "batch 4624: loss 0.053253\n",
      "batch 4625: loss 0.039864\n",
      "batch 4626: loss 0.100309\n",
      "batch 4627: loss 0.181365\n",
      "batch 4628: loss 0.044959\n",
      "batch 4629: loss 0.026653\n",
      "batch 4630: loss 0.039438\n",
      "batch 4631: loss 0.028382\n",
      "batch 4632: loss 0.036184\n",
      "batch 4633: loss 0.038727\n",
      "batch 4634: loss 0.080488\n",
      "batch 4635: loss 0.100727\n",
      "batch 4636: loss 0.025644\n",
      "batch 4637: loss 0.023908\n",
      "batch 4638: loss 0.113378\n",
      "batch 4639: loss 0.165936\n",
      "batch 4640: loss 0.158013\n",
      "batch 4641: loss 0.062225\n",
      "batch 4642: loss 0.058289\n",
      "batch 4643: loss 0.022064\n",
      "batch 4644: loss 0.032002\n",
      "batch 4645: loss 0.076215\n",
      "batch 4646: loss 0.103130\n",
      "batch 4647: loss 0.051573\n",
      "batch 4648: loss 0.025783\n",
      "batch 4649: loss 0.032919\n",
      "batch 4650: loss 0.149311\n",
      "batch 4651: loss 0.020155\n",
      "batch 4652: loss 0.043545\n",
      "batch 4653: loss 0.015280\n",
      "batch 4654: loss 0.079056\n",
      "batch 4655: loss 0.020506\n",
      "batch 4656: loss 0.016875\n",
      "batch 4657: loss 0.038917\n",
      "batch 4658: loss 0.122584\n",
      "batch 4659: loss 0.069598\n",
      "batch 4660: loss 0.062875\n",
      "batch 4661: loss 0.047234\n",
      "batch 4662: loss 0.075446\n",
      "batch 4663: loss 0.007335\n",
      "batch 4664: loss 0.084659\n",
      "batch 4665: loss 0.038371\n",
      "batch 4666: loss 0.019917\n",
      "batch 4667: loss 0.122487\n",
      "batch 4668: loss 0.067397\n",
      "batch 4669: loss 0.026241\n",
      "batch 4670: loss 0.099017\n",
      "batch 4671: loss 0.073293\n",
      "batch 4672: loss 0.055367\n",
      "batch 4673: loss 0.057922\n",
      "batch 4674: loss 0.017916\n",
      "batch 4675: loss 0.077764\n",
      "batch 4676: loss 0.011807\n",
      "batch 4677: loss 0.029544\n",
      "batch 4678: loss 0.077355\n",
      "batch 4679: loss 0.218368\n",
      "batch 4680: loss 0.019344\n",
      "batch 4681: loss 0.043377\n",
      "batch 4682: loss 0.060136\n",
      "batch 4683: loss 0.077041\n",
      "batch 4684: loss 0.007246\n",
      "batch 4685: loss 0.145611\n",
      "batch 4686: loss 0.032745\n",
      "batch 4687: loss 0.052565\n",
      "batch 4688: loss 0.072147\n",
      "batch 4689: loss 0.039860\n",
      "batch 4690: loss 0.037252\n",
      "batch 4691: loss 0.055555\n",
      "batch 4692: loss 0.077306\n",
      "batch 4693: loss 0.050170\n",
      "batch 4694: loss 0.016007\n",
      "batch 4695: loss 0.016089\n",
      "batch 4696: loss 0.021931\n",
      "batch 4697: loss 0.036728\n",
      "batch 4698: loss 0.020498\n",
      "batch 4699: loss 0.012889\n",
      "batch 4700: loss 0.009165\n",
      "batch 4701: loss 0.054189\n",
      "batch 4702: loss 0.022066\n",
      "batch 4703: loss 0.008264\n",
      "batch 4704: loss 0.009522\n",
      "batch 4705: loss 0.027774\n",
      "batch 4706: loss 0.079964\n",
      "batch 4707: loss 0.112182\n",
      "batch 4708: loss 0.052347\n",
      "batch 4709: loss 0.162316\n",
      "batch 4710: loss 0.097940\n",
      "batch 4711: loss 0.046096\n",
      "batch 4712: loss 0.059359\n",
      "batch 4713: loss 0.018931\n",
      "batch 4714: loss 0.073211\n",
      "batch 4715: loss 0.030265\n",
      "batch 4716: loss 0.017876\n",
      "batch 4717: loss 0.102070\n",
      "batch 4718: loss 0.130975\n",
      "batch 4719: loss 0.014672\n",
      "batch 4720: loss 0.024870\n",
      "batch 4721: loss 0.007105\n",
      "batch 4722: loss 0.053933\n",
      "batch 4723: loss 0.032043\n",
      "batch 4724: loss 0.014188\n",
      "batch 4725: loss 0.099563\n",
      "batch 4726: loss 0.080909\n",
      "batch 4727: loss 0.076265\n",
      "batch 4728: loss 0.022567\n",
      "batch 4729: loss 0.053303\n",
      "batch 4730: loss 0.016210\n",
      "batch 4731: loss 0.206535\n",
      "batch 4732: loss 0.085558\n",
      "batch 4733: loss 0.035900\n",
      "batch 4734: loss 0.021172\n",
      "batch 4735: loss 0.047801\n",
      "batch 4736: loss 0.073667\n",
      "batch 4737: loss 0.101098\n",
      "batch 4738: loss 0.059575\n",
      "batch 4739: loss 0.042422\n",
      "batch 4740: loss 0.027333\n",
      "batch 4741: loss 0.023442\n",
      "batch 4742: loss 0.047525\n",
      "batch 4743: loss 0.133357\n",
      "batch 4744: loss 0.353892\n",
      "batch 4745: loss 0.070952\n",
      "batch 4746: loss 0.095590\n",
      "batch 4747: loss 0.021719\n",
      "batch 4748: loss 0.088431\n",
      "batch 4749: loss 0.040166\n",
      "batch 4750: loss 0.019182\n",
      "batch 4751: loss 0.065779\n",
      "batch 4752: loss 0.018319\n",
      "batch 4753: loss 0.060078\n",
      "batch 4754: loss 0.058524\n",
      "batch 4755: loss 0.046464\n",
      "batch 4756: loss 0.050488\n",
      "batch 4757: loss 0.060156\n",
      "batch 4758: loss 0.020950\n",
      "batch 4759: loss 0.025232\n",
      "batch 4760: loss 0.119795\n",
      "batch 4761: loss 0.075454\n",
      "batch 4762: loss 0.044793\n",
      "batch 4763: loss 0.037878\n",
      "batch 4764: loss 0.058053\n",
      "batch 4765: loss 0.153664\n",
      "batch 4766: loss 0.005921\n",
      "batch 4767: loss 0.057033\n",
      "batch 4768: loss 0.040092\n",
      "batch 4769: loss 0.187195\n",
      "batch 4770: loss 0.104369\n",
      "batch 4771: loss 0.180240\n",
      "batch 4772: loss 0.039722\n",
      "batch 4773: loss 0.106524\n",
      "batch 4774: loss 0.101138\n",
      "batch 4775: loss 0.157639\n",
      "batch 4776: loss 0.093461\n",
      "batch 4777: loss 0.151708\n",
      "batch 4778: loss 0.021909\n",
      "batch 4779: loss 0.143557\n",
      "batch 4780: loss 0.017772\n",
      "batch 4781: loss 0.127789\n",
      "batch 4782: loss 0.067030\n",
      "batch 4783: loss 0.130883\n",
      "batch 4784: loss 0.015420\n",
      "batch 4785: loss 0.009995\n",
      "batch 4786: loss 0.057037\n",
      "batch 4787: loss 0.052264\n",
      "batch 4788: loss 0.033546\n",
      "batch 4789: loss 0.012784\n",
      "batch 4790: loss 0.038660\n",
      "batch 4791: loss 0.334722\n",
      "batch 4792: loss 0.063947\n",
      "batch 4793: loss 0.043602\n",
      "batch 4794: loss 0.070142\n",
      "batch 4795: loss 0.029840\n",
      "batch 4796: loss 0.025599\n",
      "batch 4797: loss 0.066348\n",
      "batch 4798: loss 0.155192\n",
      "batch 4799: loss 0.063831\n",
      "batch 4800: loss 0.104617\n",
      "batch 4801: loss 0.055060\n",
      "batch 4802: loss 0.070865\n",
      "batch 4803: loss 0.016408\n",
      "batch 4804: loss 0.083277\n",
      "batch 4805: loss 0.045117\n",
      "batch 4806: loss 0.059807\n",
      "batch 4807: loss 0.022038\n",
      "batch 4808: loss 0.070723\n",
      "batch 4809: loss 0.026296\n",
      "batch 4810: loss 0.016652\n",
      "batch 4811: loss 0.084449\n",
      "batch 4812: loss 0.034072\n",
      "batch 4813: loss 0.104817\n",
      "batch 4814: loss 0.016027\n",
      "batch 4815: loss 0.021219\n",
      "batch 4816: loss 0.163280\n",
      "batch 4817: loss 0.015328\n",
      "batch 4818: loss 0.028293\n",
      "batch 4819: loss 0.049861\n",
      "batch 4820: loss 0.082018\n",
      "batch 4821: loss 0.041315\n",
      "batch 4822: loss 0.031648\n",
      "batch 4823: loss 0.031872\n",
      "batch 4824: loss 0.037239\n",
      "batch 4825: loss 0.039079\n",
      "batch 4826: loss 0.104429\n",
      "batch 4827: loss 0.082293\n",
      "batch 4828: loss 0.077597\n",
      "batch 4829: loss 0.063191\n",
      "batch 4830: loss 0.103964\n",
      "batch 4831: loss 0.079263\n",
      "batch 4832: loss 0.015999\n",
      "batch 4833: loss 0.019071\n",
      "batch 4834: loss 0.232321\n",
      "batch 4835: loss 0.034459\n",
      "batch 4836: loss 0.020397\n",
      "batch 4837: loss 0.017928\n",
      "batch 4838: loss 0.113622\n",
      "batch 4839: loss 0.053502\n",
      "batch 4840: loss 0.012315\n",
      "batch 4841: loss 0.010105\n",
      "batch 4842: loss 0.062310\n",
      "batch 4843: loss 0.029912\n",
      "batch 4844: loss 0.016613\n",
      "batch 4845: loss 0.054324\n",
      "batch 4846: loss 0.169292\n",
      "batch 4847: loss 0.034769\n",
      "batch 4848: loss 0.012099\n",
      "batch 4849: loss 0.067802\n",
      "batch 4850: loss 0.022132\n",
      "batch 4851: loss 0.032373\n",
      "batch 4852: loss 0.207068\n",
      "batch 4853: loss 0.016765\n",
      "batch 4854: loss 0.044304\n",
      "batch 4855: loss 0.023534\n",
      "batch 4856: loss 0.038609\n",
      "batch 4857: loss 0.060663\n",
      "batch 4858: loss 0.011631\n",
      "batch 4859: loss 0.060154\n",
      "batch 4860: loss 0.029002\n",
      "batch 4861: loss 0.153944\n",
      "batch 4862: loss 0.045617\n",
      "batch 4863: loss 0.133108\n",
      "batch 4864: loss 0.069511\n",
      "batch 4865: loss 0.034733\n",
      "batch 4866: loss 0.096919\n",
      "batch 4867: loss 0.034170\n",
      "batch 4868: loss 0.055140\n",
      "batch 4869: loss 0.031722\n",
      "batch 4870: loss 0.025551\n",
      "batch 4871: loss 0.016814\n",
      "batch 4872: loss 0.005549\n",
      "batch 4873: loss 0.009488\n",
      "batch 4874: loss 0.135995\n",
      "batch 4875: loss 0.120790\n",
      "batch 4876: loss 0.104921\n",
      "batch 4877: loss 0.036141\n",
      "batch 4878: loss 0.077363\n",
      "batch 4879: loss 0.011918\n",
      "batch 4880: loss 0.037617\n",
      "batch 4881: loss 0.019760\n",
      "batch 4882: loss 0.019857\n",
      "batch 4883: loss 0.069628\n",
      "batch 4884: loss 0.025526\n",
      "batch 4885: loss 0.057017\n",
      "batch 4886: loss 0.047088\n",
      "batch 4887: loss 0.007269\n",
      "batch 4888: loss 0.027381\n",
      "batch 4889: loss 0.142188\n",
      "batch 4890: loss 0.019180\n",
      "batch 4891: loss 0.014230\n",
      "batch 4892: loss 0.012929\n",
      "batch 4893: loss 0.100618\n",
      "batch 4894: loss 0.016172\n",
      "batch 4895: loss 0.084874\n",
      "batch 4896: loss 0.027942\n",
      "batch 4897: loss 0.004061\n",
      "batch 4898: loss 0.069667\n",
      "batch 4899: loss 0.013943\n",
      "batch 4900: loss 0.073129\n",
      "batch 4901: loss 0.089338\n",
      "batch 4902: loss 0.003247\n",
      "batch 4903: loss 0.054479\n",
      "batch 4904: loss 0.014942\n",
      "batch 4905: loss 0.025612\n",
      "batch 4906: loss 0.107432\n",
      "batch 4907: loss 0.129152\n",
      "batch 4908: loss 0.026604\n",
      "batch 4909: loss 0.017774\n",
      "batch 4910: loss 0.035179\n",
      "batch 4911: loss 0.137822\n",
      "batch 4912: loss 0.022109\n",
      "batch 4913: loss 0.191878\n",
      "batch 4914: loss 0.034555\n",
      "batch 4915: loss 0.034437\n",
      "batch 4916: loss 0.076696\n",
      "batch 4917: loss 0.030347\n",
      "batch 4918: loss 0.144242\n",
      "batch 4919: loss 0.070131\n",
      "batch 4920: loss 0.063928\n",
      "batch 4921: loss 0.045329\n",
      "batch 4922: loss 0.030430\n",
      "batch 4923: loss 0.060751\n",
      "batch 4924: loss 0.120654\n",
      "batch 4925: loss 0.027274\n",
      "batch 4926: loss 0.112397\n",
      "batch 4927: loss 0.115292\n",
      "batch 4928: loss 0.015943\n",
      "batch 4929: loss 0.043218\n",
      "batch 4930: loss 0.046954\n",
      "batch 4931: loss 0.003628\n",
      "batch 4932: loss 0.015004\n",
      "batch 4933: loss 0.020304\n",
      "batch 4934: loss 0.074328\n",
      "batch 4935: loss 0.008449\n",
      "batch 4936: loss 0.034765\n",
      "batch 4937: loss 0.146470\n",
      "batch 4938: loss 0.031456\n",
      "batch 4939: loss 0.019139\n",
      "batch 4940: loss 0.023056\n",
      "batch 4941: loss 0.013335\n",
      "batch 4942: loss 0.079760\n",
      "batch 4943: loss 0.042251\n",
      "batch 4944: loss 0.027997\n",
      "batch 4945: loss 0.020438\n",
      "batch 4946: loss 0.015167\n",
      "batch 4947: loss 0.077047\n",
      "batch 4948: loss 0.004723\n",
      "batch 4949: loss 0.049270\n",
      "batch 4950: loss 0.020895\n",
      "batch 4951: loss 0.057619\n",
      "batch 4952: loss 0.017201\n",
      "batch 4953: loss 0.145739\n",
      "batch 4954: loss 0.068004\n",
      "batch 4955: loss 0.069775\n",
      "batch 4956: loss 0.034557\n",
      "batch 4957: loss 0.017787\n",
      "batch 4958: loss 0.010824\n",
      "batch 4959: loss 0.019055\n",
      "batch 4960: loss 0.045517\n",
      "batch 4961: loss 0.089055\n",
      "batch 4962: loss 0.016008\n",
      "batch 4963: loss 0.072794\n",
      "batch 4964: loss 0.059704\n",
      "batch 4965: loss 0.013313\n",
      "batch 4966: loss 0.042312\n",
      "batch 4967: loss 0.063303\n",
      "batch 4968: loss 0.113544\n",
      "batch 4969: loss 0.089530\n",
      "batch 4970: loss 0.020032\n",
      "batch 4971: loss 0.147054\n",
      "batch 4972: loss 0.136388\n",
      "batch 4973: loss 0.092098\n",
      "batch 4974: loss 0.023030\n",
      "batch 4975: loss 0.043212\n",
      "batch 4976: loss 0.012226\n",
      "batch 4977: loss 0.018107\n",
      "batch 4978: loss 0.025304\n",
      "batch 4979: loss 0.037241\n",
      "batch 4980: loss 0.151612\n",
      "batch 4981: loss 0.086922\n",
      "batch 4982: loss 0.049750\n",
      "batch 4983: loss 0.061489\n",
      "batch 4984: loss 0.089957\n",
      "batch 4985: loss 0.058168\n",
      "batch 4986: loss 0.035466\n",
      "batch 4987: loss 0.044653\n",
      "batch 4988: loss 0.032353\n",
      "batch 4989: loss 0.012303\n",
      "batch 4990: loss 0.056126\n",
      "batch 4991: loss 0.061650\n",
      "batch 4992: loss 0.038444\n",
      "batch 4993: loss 0.049814\n",
      "batch 4994: loss 0.043962\n",
      "batch 4995: loss 0.019821\n",
      "batch 4996: loss 0.004679\n",
      "batch 4997: loss 0.023070\n",
      "batch 4998: loss 0.082116\n",
      "batch 4999: loss 0.171267\n",
      "batch 5000: loss 0.011705\n",
      "batch 5001: loss 0.025905\n",
      "batch 5002: loss 0.044822\n",
      "batch 5003: loss 0.073117\n",
      "batch 5004: loss 0.005426\n",
      "batch 5005: loss 0.019722\n",
      "batch 5006: loss 0.020266\n",
      "batch 5007: loss 0.093932\n",
      "batch 5008: loss 0.011752\n",
      "batch 5009: loss 0.037639\n",
      "batch 5010: loss 0.071300\n",
      "batch 5011: loss 0.019573\n",
      "batch 5012: loss 0.067830\n",
      "batch 5013: loss 0.096345\n",
      "batch 5014: loss 0.056702\n",
      "batch 5015: loss 0.040292\n",
      "batch 5016: loss 0.035426\n",
      "batch 5017: loss 0.117247\n",
      "batch 5018: loss 0.014831\n",
      "batch 5019: loss 0.074752\n",
      "batch 5020: loss 0.138536\n",
      "batch 5021: loss 0.082349\n",
      "batch 5022: loss 0.034800\n",
      "batch 5023: loss 0.047766\n",
      "batch 5024: loss 0.015931\n",
      "batch 5025: loss 0.044803\n",
      "batch 5026: loss 0.049539\n",
      "batch 5027: loss 0.025398\n",
      "batch 5028: loss 0.039179\n",
      "batch 5029: loss 0.005678\n",
      "batch 5030: loss 0.020436\n",
      "batch 5031: loss 0.242835\n",
      "batch 5032: loss 0.036358\n",
      "batch 5033: loss 0.010098\n",
      "batch 5034: loss 0.041578\n",
      "batch 5035: loss 0.020334\n",
      "batch 5036: loss 0.064087\n",
      "batch 5037: loss 0.026395\n",
      "batch 5038: loss 0.073395\n",
      "batch 5039: loss 0.032919\n",
      "batch 5040: loss 0.081769\n",
      "batch 5041: loss 0.017178\n",
      "batch 5042: loss 0.148308\n",
      "batch 5043: loss 0.146834\n",
      "batch 5044: loss 0.067085\n",
      "batch 5045: loss 0.071196\n",
      "batch 5046: loss 0.058802\n",
      "batch 5047: loss 0.011786\n",
      "batch 5048: loss 0.015406\n",
      "batch 5049: loss 0.020202\n",
      "batch 5050: loss 0.077849\n",
      "batch 5051: loss 0.053689\n",
      "batch 5052: loss 0.021022\n",
      "batch 5053: loss 0.066116\n",
      "batch 5054: loss 0.018244\n",
      "batch 5055: loss 0.049086\n",
      "batch 5056: loss 0.026904\n",
      "batch 5057: loss 0.100286\n",
      "batch 5058: loss 0.017164\n",
      "batch 5059: loss 0.035748\n",
      "batch 5060: loss 0.099095\n",
      "batch 5061: loss 0.070785\n",
      "batch 5062: loss 0.013727\n",
      "batch 5063: loss 0.165114\n",
      "batch 5064: loss 0.013708\n",
      "batch 5065: loss 0.050621\n",
      "batch 5066: loss 0.109392\n",
      "batch 5067: loss 0.158023\n",
      "batch 5068: loss 0.009921\n",
      "batch 5069: loss 0.030153\n",
      "batch 5070: loss 0.010386\n",
      "batch 5071: loss 0.027080\n",
      "batch 5072: loss 0.013414\n",
      "batch 5073: loss 0.136256\n",
      "batch 5074: loss 0.133763\n",
      "batch 5075: loss 0.031036\n",
      "batch 5076: loss 0.020914\n",
      "batch 5077: loss 0.012238\n",
      "batch 5078: loss 0.020030\n",
      "batch 5079: loss 0.110893\n",
      "batch 5080: loss 0.032100\n",
      "batch 5081: loss 0.033876\n",
      "batch 5082: loss 0.101124\n",
      "batch 5083: loss 0.128802\n",
      "batch 5084: loss 0.130242\n",
      "batch 5085: loss 0.051785\n",
      "batch 5086: loss 0.004062\n",
      "batch 5087: loss 0.050076\n",
      "batch 5088: loss 0.010068\n",
      "batch 5089: loss 0.010583\n",
      "batch 5090: loss 0.067449\n",
      "batch 5091: loss 0.056318\n",
      "batch 5092: loss 0.074562\n",
      "batch 5093: loss 0.043263\n",
      "batch 5094: loss 0.056244\n",
      "batch 5095: loss 0.016750\n",
      "batch 5096: loss 0.161156\n",
      "batch 5097: loss 0.082940\n",
      "batch 5098: loss 0.019184\n",
      "batch 5099: loss 0.010588\n",
      "batch 5100: loss 0.084188\n",
      "batch 5101: loss 0.039346\n",
      "batch 5102: loss 0.050809\n",
      "batch 5103: loss 0.040939\n",
      "batch 5104: loss 0.055889\n",
      "batch 5105: loss 0.020523\n",
      "batch 5106: loss 0.101756\n",
      "batch 5107: loss 0.035929\n",
      "batch 5108: loss 0.052143\n",
      "batch 5109: loss 0.024122\n",
      "batch 5110: loss 0.155421\n",
      "batch 5111: loss 0.045137\n",
      "batch 5112: loss 0.036087\n",
      "batch 5113: loss 0.124434\n",
      "batch 5114: loss 0.059579\n",
      "batch 5115: loss 0.015346\n",
      "batch 5116: loss 0.053143\n",
      "batch 5117: loss 0.012856\n",
      "batch 5118: loss 0.025465\n",
      "batch 5119: loss 0.019673\n",
      "batch 5120: loss 0.157523\n",
      "batch 5121: loss 0.114641\n",
      "batch 5122: loss 0.182376\n",
      "batch 5123: loss 0.173694\n",
      "batch 5124: loss 0.017341\n",
      "batch 5125: loss 0.009533\n",
      "batch 5126: loss 0.046993\n",
      "batch 5127: loss 0.025466\n",
      "batch 5128: loss 0.019791\n",
      "batch 5129: loss 0.007073\n",
      "batch 5130: loss 0.014217\n",
      "batch 5131: loss 0.028618\n",
      "batch 5132: loss 0.024975\n",
      "batch 5133: loss 0.054938\n",
      "batch 5134: loss 0.244269\n",
      "batch 5135: loss 0.066575\n",
      "batch 5136: loss 0.074921\n",
      "batch 5137: loss 0.046873\n",
      "batch 5138: loss 0.028093\n",
      "batch 5139: loss 0.034001\n",
      "batch 5140: loss 0.040148\n",
      "batch 5141: loss 0.051111\n",
      "batch 5142: loss 0.027296\n",
      "batch 5143: loss 0.055705\n",
      "batch 5144: loss 0.012273\n",
      "batch 5145: loss 0.093468\n",
      "batch 5146: loss 0.030339\n",
      "batch 5147: loss 0.173283\n",
      "batch 5148: loss 0.023016\n",
      "batch 5149: loss 0.010839\n",
      "batch 5150: loss 0.082502\n",
      "batch 5151: loss 0.094621\n",
      "batch 5152: loss 0.055604\n",
      "batch 5153: loss 0.064165\n",
      "batch 5154: loss 0.052168\n",
      "batch 5155: loss 0.037485\n",
      "batch 5156: loss 0.049342\n",
      "batch 5157: loss 0.045621\n",
      "batch 5158: loss 0.082041\n",
      "batch 5159: loss 0.022264\n",
      "batch 5160: loss 0.022407\n",
      "batch 5161: loss 0.092462\n",
      "batch 5162: loss 0.024377\n",
      "batch 5163: loss 0.008327\n",
      "batch 5164: loss 0.048413\n",
      "batch 5165: loss 0.050213\n",
      "batch 5166: loss 0.012663\n",
      "batch 5167: loss 0.040994\n",
      "batch 5168: loss 0.032310\n",
      "batch 5169: loss 0.108156\n",
      "batch 5170: loss 0.043918\n",
      "batch 5171: loss 0.013295\n",
      "batch 5172: loss 0.109550\n",
      "batch 5173: loss 0.184271\n",
      "batch 5174: loss 0.125772\n",
      "batch 5175: loss 0.029141\n",
      "batch 5176: loss 0.052915\n",
      "batch 5177: loss 0.028334\n",
      "batch 5178: loss 0.023028\n",
      "batch 5179: loss 0.112909\n",
      "batch 5180: loss 0.037961\n",
      "batch 5181: loss 0.076428\n",
      "batch 5182: loss 0.044816\n",
      "batch 5183: loss 0.032040\n",
      "batch 5184: loss 0.048392\n",
      "batch 5185: loss 0.045750\n",
      "batch 5186: loss 0.055505\n",
      "batch 5187: loss 0.040958\n",
      "batch 5188: loss 0.065783\n",
      "batch 5189: loss 0.058841\n",
      "batch 5190: loss 0.054133\n",
      "batch 5191: loss 0.124118\n",
      "batch 5192: loss 0.043889\n",
      "batch 5193: loss 0.013222\n",
      "batch 5194: loss 0.027710\n",
      "batch 5195: loss 0.154726\n",
      "batch 5196: loss 0.118177\n",
      "batch 5197: loss 0.041995\n",
      "batch 5198: loss 0.031121\n",
      "batch 5199: loss 0.025709\n",
      "batch 5200: loss 0.011797\n",
      "batch 5201: loss 0.154975\n",
      "batch 5202: loss 0.169196\n",
      "batch 5203: loss 0.074895\n",
      "batch 5204: loss 0.057892\n",
      "batch 5205: loss 0.021797\n",
      "batch 5206: loss 0.255211\n",
      "batch 5207: loss 0.029698\n",
      "batch 5208: loss 0.083469\n",
      "batch 5209: loss 0.005325\n",
      "batch 5210: loss 0.133021\n",
      "batch 5211: loss 0.041138\n",
      "batch 5212: loss 0.150622\n",
      "batch 5213: loss 0.063053\n",
      "batch 5214: loss 0.149424\n",
      "batch 5215: loss 0.020379\n",
      "batch 5216: loss 0.089014\n",
      "batch 5217: loss 0.097532\n",
      "batch 5218: loss 0.274730\n",
      "batch 5219: loss 0.059977\n",
      "batch 5220: loss 0.012842\n",
      "batch 5221: loss 0.004967\n",
      "batch 5222: loss 0.085110\n",
      "batch 5223: loss 0.133973\n",
      "batch 5224: loss 0.031932\n",
      "batch 5225: loss 0.035205\n",
      "batch 5226: loss 0.009539\n",
      "batch 5227: loss 0.017534\n",
      "batch 5228: loss 0.050003\n",
      "batch 5229: loss 0.007158\n",
      "batch 5230: loss 0.081770\n",
      "batch 5231: loss 0.041300\n",
      "batch 5232: loss 0.023895\n",
      "batch 5233: loss 0.039503\n",
      "batch 5234: loss 0.051892\n",
      "batch 5235: loss 0.058021\n",
      "batch 5236: loss 0.020800\n",
      "batch 5237: loss 0.067543\n",
      "batch 5238: loss 0.035805\n",
      "batch 5239: loss 0.099463\n",
      "batch 5240: loss 0.108141\n",
      "batch 5241: loss 0.090556\n",
      "batch 5242: loss 0.052049\n",
      "batch 5243: loss 0.048158\n",
      "batch 5244: loss 0.024462\n",
      "batch 5245: loss 0.054367\n",
      "batch 5246: loss 0.072018\n",
      "batch 5247: loss 0.033535\n",
      "batch 5248: loss 0.147233\n",
      "batch 5249: loss 0.017132\n",
      "batch 5250: loss 0.011778\n",
      "batch 5251: loss 0.026371\n",
      "batch 5252: loss 0.009145\n",
      "batch 5253: loss 0.043194\n",
      "batch 5254: loss 0.070192\n",
      "batch 5255: loss 0.128594\n",
      "batch 5256: loss 0.031361\n",
      "batch 5257: loss 0.021681\n",
      "batch 5258: loss 0.054107\n",
      "batch 5259: loss 0.017694\n",
      "batch 5260: loss 0.017678\n",
      "batch 5261: loss 0.223857\n",
      "batch 5262: loss 0.084237\n",
      "batch 5263: loss 0.008063\n",
      "batch 5264: loss 0.013422\n",
      "batch 5265: loss 0.086201\n",
      "batch 5266: loss 0.029709\n",
      "batch 5267: loss 0.095194\n",
      "batch 5268: loss 0.140665\n",
      "batch 5269: loss 0.039678\n",
      "batch 5270: loss 0.055885\n",
      "batch 5271: loss 0.025450\n",
      "batch 5272: loss 0.095080\n",
      "batch 5273: loss 0.152730\n",
      "batch 5274: loss 0.093558\n",
      "batch 5275: loss 0.007690\n",
      "batch 5276: loss 0.034291\n",
      "batch 5277: loss 0.041409\n",
      "batch 5278: loss 0.043307\n",
      "batch 5279: loss 0.047510\n",
      "batch 5280: loss 0.055922\n",
      "batch 5281: loss 0.139959\n",
      "batch 5282: loss 0.014799\n",
      "batch 5283: loss 0.020880\n",
      "batch 5284: loss 0.021532\n",
      "batch 5285: loss 0.074571\n",
      "batch 5286: loss 0.043704\n",
      "batch 5287: loss 0.070294\n",
      "batch 5288: loss 0.040526\n",
      "batch 5289: loss 0.018293\n",
      "batch 5290: loss 0.124323\n",
      "batch 5291: loss 0.029192\n",
      "batch 5292: loss 0.098677\n",
      "batch 5293: loss 0.010618\n",
      "batch 5294: loss 0.036855\n",
      "batch 5295: loss 0.009850\n",
      "batch 5296: loss 0.028848\n",
      "batch 5297: loss 0.214413\n",
      "batch 5298: loss 0.186020\n",
      "batch 5299: loss 0.035429\n",
      "batch 5300: loss 0.024898\n",
      "batch 5301: loss 0.122395\n",
      "batch 5302: loss 0.104327\n",
      "batch 5303: loss 0.045903\n",
      "batch 5304: loss 0.031983\n",
      "batch 5305: loss 0.054481\n",
      "batch 5306: loss 0.012426\n",
      "batch 5307: loss 0.010537\n",
      "batch 5308: loss 0.043799\n",
      "batch 5309: loss 0.015793\n",
      "batch 5310: loss 0.014380\n",
      "batch 5311: loss 0.117736\n",
      "batch 5312: loss 0.147006\n",
      "batch 5313: loss 0.263696\n",
      "batch 5314: loss 0.078642\n",
      "batch 5315: loss 0.044574\n",
      "batch 5316: loss 0.023934\n",
      "batch 5317: loss 0.052996\n",
      "batch 5318: loss 0.283946\n",
      "batch 5319: loss 0.140763\n",
      "batch 5320: loss 0.043346\n",
      "batch 5321: loss 0.024239\n",
      "batch 5322: loss 0.053281\n",
      "batch 5323: loss 0.064706\n",
      "batch 5324: loss 0.048638\n",
      "batch 5325: loss 0.009467\n",
      "batch 5326: loss 0.025360\n",
      "batch 5327: loss 0.141215\n",
      "batch 5328: loss 0.031429\n",
      "batch 5329: loss 0.029627\n",
      "batch 5330: loss 0.035519\n",
      "batch 5331: loss 0.043995\n",
      "batch 5332: loss 0.025617\n",
      "batch 5333: loss 0.038474\n",
      "batch 5334: loss 0.017185\n",
      "batch 5335: loss 0.058584\n",
      "batch 5336: loss 0.126275\n",
      "batch 5337: loss 0.026478\n",
      "batch 5338: loss 0.050620\n",
      "batch 5339: loss 0.081076\n",
      "batch 5340: loss 0.089326\n",
      "batch 5341: loss 0.030693\n",
      "batch 5342: loss 0.015834\n",
      "batch 5343: loss 0.068135\n",
      "batch 5344: loss 0.119338\n",
      "batch 5345: loss 0.016214\n",
      "batch 5346: loss 0.065291\n",
      "batch 5347: loss 0.074700\n",
      "batch 5348: loss 0.025294\n",
      "batch 5349: loss 0.074679\n",
      "batch 5350: loss 0.149242\n",
      "batch 5351: loss 0.018211\n",
      "batch 5352: loss 0.034867\n",
      "batch 5353: loss 0.045317\n",
      "batch 5354: loss 0.195099\n",
      "batch 5355: loss 0.118151\n",
      "batch 5356: loss 0.029033\n",
      "batch 5357: loss 0.035346\n",
      "batch 5358: loss 0.050242\n",
      "batch 5359: loss 0.057820\n",
      "batch 5360: loss 0.032183\n",
      "batch 5361: loss 0.039917\n",
      "batch 5362: loss 0.012037\n",
      "batch 5363: loss 0.008715\n",
      "batch 5364: loss 0.021551\n",
      "batch 5365: loss 0.028597\n",
      "batch 5366: loss 0.007382\n",
      "batch 5367: loss 0.070257\n",
      "batch 5368: loss 0.014911\n",
      "batch 5369: loss 0.098586\n",
      "batch 5370: loss 0.028625\n",
      "batch 5371: loss 0.048016\n",
      "batch 5372: loss 0.038893\n",
      "batch 5373: loss 0.031462\n",
      "batch 5374: loss 0.006483\n",
      "batch 5375: loss 0.012058\n",
      "batch 5376: loss 0.012083\n",
      "batch 5377: loss 0.009885\n",
      "batch 5378: loss 0.054114\n",
      "batch 5379: loss 0.025366\n",
      "batch 5380: loss 0.065106\n",
      "batch 5381: loss 0.048579\n",
      "batch 5382: loss 0.076020\n",
      "batch 5383: loss 0.109247\n",
      "batch 5384: loss 0.121588\n",
      "batch 5385: loss 0.046922\n",
      "batch 5386: loss 0.037828\n",
      "batch 5387: loss 0.009339\n",
      "batch 5388: loss 0.054624\n",
      "batch 5389: loss 0.004962\n",
      "batch 5390: loss 0.066677\n",
      "batch 5391: loss 0.124602\n",
      "batch 5392: loss 0.028987\n",
      "batch 5393: loss 0.028740\n",
      "batch 5394: loss 0.012110\n",
      "batch 5395: loss 0.053643\n",
      "batch 5396: loss 0.144747\n",
      "batch 5397: loss 0.024494\n",
      "batch 5398: loss 0.039141\n",
      "batch 5399: loss 0.033892\n",
      "batch 5400: loss 0.292079\n",
      "batch 5401: loss 0.008154\n",
      "batch 5402: loss 0.051921\n",
      "batch 5403: loss 0.025666\n",
      "batch 5404: loss 0.060289\n",
      "batch 5405: loss 0.051388\n",
      "batch 5406: loss 0.051965\n",
      "batch 5407: loss 0.006368\n",
      "batch 5408: loss 0.037902\n",
      "batch 5409: loss 0.051708\n",
      "batch 5410: loss 0.016079\n",
      "batch 5411: loss 0.005606\n",
      "batch 5412: loss 0.024607\n",
      "batch 5413: loss 0.019715\n",
      "batch 5414: loss 0.083238\n",
      "batch 5415: loss 0.040950\n",
      "batch 5416: loss 0.070370\n",
      "batch 5417: loss 0.046094\n",
      "batch 5418: loss 0.080043\n",
      "batch 5419: loss 0.032914\n",
      "batch 5420: loss 0.004527\n",
      "batch 5421: loss 0.028294\n",
      "batch 5422: loss 0.027096\n",
      "batch 5423: loss 0.065935\n",
      "batch 5424: loss 0.018570\n",
      "batch 5425: loss 0.021683\n",
      "batch 5426: loss 0.050919\n",
      "batch 5427: loss 0.078343\n",
      "batch 5428: loss 0.076047\n",
      "batch 5429: loss 0.038715\n",
      "batch 5430: loss 0.009768\n",
      "batch 5431: loss 0.257737\n",
      "batch 5432: loss 0.161554\n",
      "batch 5433: loss 0.065406\n",
      "batch 5434: loss 0.069120\n",
      "batch 5435: loss 0.044211\n",
      "batch 5436: loss 0.075546\n",
      "batch 5437: loss 0.135528\n",
      "batch 5438: loss 0.037353\n",
      "batch 5439: loss 0.171489\n",
      "batch 5440: loss 0.019292\n",
      "batch 5441: loss 0.005319\n",
      "batch 5442: loss 0.088405\n",
      "batch 5443: loss 0.117248\n",
      "batch 5444: loss 0.032188\n",
      "batch 5445: loss 0.028118\n",
      "batch 5446: loss 0.030233\n",
      "batch 5447: loss 0.048528\n",
      "batch 5448: loss 0.012658\n",
      "batch 5449: loss 0.040701\n",
      "batch 5450: loss 0.061526\n",
      "batch 5451: loss 0.120791\n",
      "batch 5452: loss 0.031426\n",
      "batch 5453: loss 0.147736\n",
      "batch 5454: loss 0.036937\n",
      "batch 5455: loss 0.039753\n",
      "batch 5456: loss 0.029533\n",
      "batch 5457: loss 0.014767\n",
      "batch 5458: loss 0.008032\n",
      "batch 5459: loss 0.074300\n",
      "batch 5460: loss 0.041725\n",
      "batch 5461: loss 0.276951\n",
      "batch 5462: loss 0.040298\n",
      "batch 5463: loss 0.100381\n",
      "batch 5464: loss 0.026259\n",
      "batch 5465: loss 0.056482\n",
      "batch 5466: loss 0.081296\n",
      "batch 5467: loss 0.039730\n",
      "batch 5468: loss 0.020349\n",
      "batch 5469: loss 0.047776\n",
      "batch 5470: loss 0.032385\n",
      "batch 5471: loss 0.052851\n",
      "batch 5472: loss 0.025471\n",
      "batch 5473: loss 0.058288\n",
      "batch 5474: loss 0.179197\n",
      "batch 5475: loss 0.010031\n",
      "batch 5476: loss 0.018936\n",
      "batch 5477: loss 0.024998\n",
      "batch 5478: loss 0.013895\n",
      "batch 5479: loss 0.049071\n",
      "batch 5480: loss 0.113372\n",
      "batch 5481: loss 0.037374\n",
      "batch 5482: loss 0.136741\n",
      "batch 5483: loss 0.048660\n",
      "batch 5484: loss 0.012909\n",
      "batch 5485: loss 0.077119\n",
      "batch 5486: loss 0.042921\n",
      "batch 5487: loss 0.063835\n",
      "batch 5488: loss 0.130414\n",
      "batch 5489: loss 0.013863\n",
      "batch 5490: loss 0.021008\n",
      "batch 5491: loss 0.035358\n",
      "batch 5492: loss 0.008227\n",
      "batch 5493: loss 0.018070\n",
      "batch 5494: loss 0.151018\n",
      "batch 5495: loss 0.085833\n",
      "batch 5496: loss 0.004800\n",
      "batch 5497: loss 0.079382\n",
      "batch 5498: loss 0.015507\n",
      "batch 5499: loss 0.060511\n",
      "batch 5500: loss 0.024612\n",
      "batch 5501: loss 0.195101\n",
      "batch 5502: loss 0.015376\n",
      "batch 5503: loss 0.116002\n",
      "batch 5504: loss 0.019954\n",
      "batch 5505: loss 0.106444\n",
      "batch 5506: loss 0.022376\n",
      "batch 5507: loss 0.038593\n",
      "batch 5508: loss 0.061564\n",
      "batch 5509: loss 0.007803\n",
      "batch 5510: loss 0.023827\n",
      "batch 5511: loss 0.022594\n",
      "batch 5512: loss 0.021976\n",
      "batch 5513: loss 0.057882\n",
      "batch 5514: loss 0.090028\n",
      "batch 5515: loss 0.083512\n",
      "batch 5516: loss 0.026907\n",
      "batch 5517: loss 0.036814\n",
      "batch 5518: loss 0.077576\n",
      "batch 5519: loss 0.043932\n",
      "batch 5520: loss 0.010521\n",
      "batch 5521: loss 0.042033\n",
      "batch 5522: loss 0.057113\n",
      "batch 5523: loss 0.009326\n",
      "batch 5524: loss 0.020975\n",
      "batch 5525: loss 0.067868\n",
      "batch 5526: loss 0.109247\n",
      "batch 5527: loss 0.035903\n",
      "batch 5528: loss 0.033650\n",
      "batch 5529: loss 0.051071\n",
      "batch 5530: loss 0.030300\n",
      "batch 5531: loss 0.069515\n",
      "batch 5532: loss 0.014886\n",
      "batch 5533: loss 0.053207\n",
      "batch 5534: loss 0.194708\n",
      "batch 5535: loss 0.009626\n",
      "batch 5536: loss 0.065269\n",
      "batch 5537: loss 0.025704\n",
      "batch 5538: loss 0.013007\n",
      "batch 5539: loss 0.087599\n",
      "batch 5540: loss 0.019054\n",
      "batch 5541: loss 0.016550\n",
      "batch 5542: loss 0.026285\n",
      "batch 5543: loss 0.054596\n",
      "batch 5544: loss 0.005476\n",
      "batch 5545: loss 0.038420\n",
      "batch 5546: loss 0.112738\n",
      "batch 5547: loss 0.048209\n",
      "batch 5548: loss 0.021113\n",
      "batch 5549: loss 0.141378\n",
      "batch 5550: loss 0.066064\n",
      "batch 5551: loss 0.174852\n",
      "batch 5552: loss 0.080952\n",
      "batch 5553: loss 0.044471\n",
      "batch 5554: loss 0.015113\n",
      "batch 5555: loss 0.027704\n",
      "batch 5556: loss 0.014491\n",
      "batch 5557: loss 0.051857\n",
      "batch 5558: loss 0.013592\n",
      "batch 5559: loss 0.023106\n",
      "batch 5560: loss 0.020927\n",
      "batch 5561: loss 0.058762\n",
      "batch 5562: loss 0.014348\n",
      "batch 5563: loss 0.117643\n",
      "batch 5564: loss 0.089539\n",
      "batch 5565: loss 0.008124\n",
      "batch 5566: loss 0.056613\n",
      "batch 5567: loss 0.054164\n",
      "batch 5568: loss 0.018669\n",
      "batch 5569: loss 0.052274\n",
      "batch 5570: loss 0.021818\n",
      "batch 5571: loss 0.078894\n",
      "batch 5572: loss 0.019661\n",
      "batch 5573: loss 0.039104\n",
      "batch 5574: loss 0.064271\n",
      "batch 5575: loss 0.047910\n",
      "batch 5576: loss 0.022182\n",
      "batch 5577: loss 0.014983\n",
      "batch 5578: loss 0.089699\n",
      "batch 5579: loss 0.111077\n",
      "batch 5580: loss 0.008164\n",
      "batch 5581: loss 0.138000\n",
      "batch 5582: loss 0.057133\n",
      "batch 5583: loss 0.037184\n",
      "batch 5584: loss 0.026002\n",
      "batch 5585: loss 0.056265\n",
      "batch 5586: loss 0.032754\n",
      "batch 5587: loss 0.063549\n",
      "batch 5588: loss 0.027873\n",
      "batch 5589: loss 0.024608\n",
      "batch 5590: loss 0.015492\n",
      "batch 5591: loss 0.073219\n",
      "batch 5592: loss 0.207490\n",
      "batch 5593: loss 0.059757\n",
      "batch 5594: loss 0.028022\n",
      "batch 5595: loss 0.028581\n",
      "batch 5596: loss 0.205855\n",
      "batch 5597: loss 0.099669\n",
      "batch 5598: loss 0.065129\n",
      "batch 5599: loss 0.016999\n",
      "batch 5600: loss 0.006263\n",
      "batch 5601: loss 0.025102\n",
      "batch 5602: loss 0.175774\n",
      "batch 5603: loss 0.060402\n",
      "batch 5604: loss 0.027722\n",
      "batch 5605: loss 0.022590\n",
      "batch 5606: loss 0.062177\n",
      "batch 5607: loss 0.039573\n",
      "batch 5608: loss 0.042642\n",
      "batch 5609: loss 0.027668\n",
      "batch 5610: loss 0.055154\n",
      "batch 5611: loss 0.017775\n",
      "batch 5612: loss 0.024451\n",
      "batch 5613: loss 0.009825\n",
      "batch 5614: loss 0.024102\n",
      "batch 5615: loss 0.082003\n",
      "batch 5616: loss 0.051247\n",
      "batch 5617: loss 0.021864\n",
      "batch 5618: loss 0.137294\n",
      "batch 5619: loss 0.034449\n",
      "batch 5620: loss 0.061840\n",
      "batch 5621: loss 0.010604\n",
      "batch 5622: loss 0.008192\n",
      "batch 5623: loss 0.174303\n",
      "batch 5624: loss 0.102020\n",
      "batch 5625: loss 0.026302\n",
      "batch 5626: loss 0.030823\n",
      "batch 5627: loss 0.031575\n",
      "batch 5628: loss 0.107601\n",
      "batch 5629: loss 0.094629\n",
      "batch 5630: loss 0.012508\n",
      "batch 5631: loss 0.121033\n",
      "batch 5632: loss 0.038396\n",
      "batch 5633: loss 0.042915\n",
      "batch 5634: loss 0.010614\n",
      "batch 5635: loss 0.140533\n",
      "batch 5636: loss 0.006739\n",
      "batch 5637: loss 0.141912\n",
      "batch 5638: loss 0.014254\n",
      "batch 5639: loss 0.053243\n",
      "batch 5640: loss 0.019965\n",
      "batch 5641: loss 0.195788\n",
      "batch 5642: loss 0.026577\n",
      "batch 5643: loss 0.034265\n",
      "batch 5644: loss 0.065050\n",
      "batch 5645: loss 0.014588\n",
      "batch 5646: loss 0.112572\n",
      "batch 5647: loss 0.053579\n",
      "batch 5648: loss 0.048627\n",
      "batch 5649: loss 0.014040\n",
      "batch 5650: loss 0.021057\n",
      "batch 5651: loss 0.071461\n",
      "batch 5652: loss 0.013863\n",
      "batch 5653: loss 0.026890\n",
      "batch 5654: loss 0.034144\n",
      "batch 5655: loss 0.098295\n",
      "batch 5656: loss 0.017859\n",
      "batch 5657: loss 0.009808\n",
      "batch 5658: loss 0.045508\n",
      "batch 5659: loss 0.210897\n",
      "batch 5660: loss 0.020185\n",
      "batch 5661: loss 0.055274\n",
      "batch 5662: loss 0.086628\n",
      "batch 5663: loss 0.081671\n",
      "batch 5664: loss 0.007602\n",
      "batch 5665: loss 0.024151\n",
      "batch 5666: loss 0.102883\n",
      "batch 5667: loss 0.196889\n",
      "batch 5668: loss 0.012325\n",
      "batch 5669: loss 0.110605\n",
      "batch 5670: loss 0.021107\n",
      "batch 5671: loss 0.022892\n",
      "batch 5672: loss 0.021917\n",
      "batch 5673: loss 0.007225\n",
      "batch 5674: loss 0.010769\n",
      "batch 5675: loss 0.086495\n",
      "batch 5676: loss 0.078321\n",
      "batch 5677: loss 0.015743\n",
      "batch 5678: loss 0.175733\n",
      "batch 5679: loss 0.019643\n",
      "batch 5680: loss 0.011794\n",
      "batch 5681: loss 0.032962\n",
      "batch 5682: loss 0.048498\n",
      "batch 5683: loss 0.060030\n",
      "batch 5684: loss 0.022920\n",
      "batch 5685: loss 0.004675\n",
      "batch 5686: loss 0.008676\n",
      "batch 5687: loss 0.059780\n",
      "batch 5688: loss 0.071058\n",
      "batch 5689: loss 0.038520\n",
      "batch 5690: loss 0.025543\n",
      "batch 5691: loss 0.050875\n",
      "batch 5692: loss 0.086442\n",
      "batch 5693: loss 0.031994\n",
      "batch 5694: loss 0.185087\n",
      "batch 5695: loss 0.011329\n",
      "batch 5696: loss 0.087100\n",
      "batch 5697: loss 0.020529\n",
      "batch 5698: loss 0.050694\n",
      "batch 5699: loss 0.043144\n",
      "batch 5700: loss 0.029488\n",
      "batch 5701: loss 0.025826\n",
      "batch 5702: loss 0.016370\n",
      "batch 5703: loss 0.020495\n",
      "batch 5704: loss 0.143472\n",
      "batch 5705: loss 0.019996\n",
      "batch 5706: loss 0.021154\n",
      "batch 5707: loss 0.094094\n",
      "batch 5708: loss 0.005221\n",
      "batch 5709: loss 0.097052\n",
      "batch 5710: loss 0.090033\n",
      "batch 5711: loss 0.022325\n",
      "batch 5712: loss 0.006521\n",
      "batch 5713: loss 0.116472\n",
      "batch 5714: loss 0.022967\n",
      "batch 5715: loss 0.042564\n",
      "batch 5716: loss 0.114223\n",
      "batch 5717: loss 0.008741\n",
      "batch 5718: loss 0.019309\n",
      "batch 5719: loss 0.064567\n",
      "batch 5720: loss 0.045301\n",
      "batch 5721: loss 0.018945\n",
      "batch 5722: loss 0.054314\n",
      "batch 5723: loss 0.085119\n",
      "batch 5724: loss 0.024376\n",
      "batch 5725: loss 0.091383\n",
      "batch 5726: loss 0.041406\n",
      "batch 5727: loss 0.014817\n",
      "batch 5728: loss 0.015428\n",
      "batch 5729: loss 0.014346\n",
      "batch 5730: loss 0.015318\n",
      "batch 5731: loss 0.011906\n",
      "batch 5732: loss 0.063282\n",
      "batch 5733: loss 0.170551\n",
      "batch 5734: loss 0.003130\n",
      "batch 5735: loss 0.015784\n",
      "batch 5736: loss 0.094213\n",
      "batch 5737: loss 0.038183\n",
      "batch 5738: loss 0.016346\n",
      "batch 5739: loss 0.016768\n",
      "batch 5740: loss 0.029576\n",
      "batch 5741: loss 0.002198\n",
      "batch 5742: loss 0.030965\n",
      "batch 5743: loss 0.016170\n",
      "batch 5744: loss 0.057732\n",
      "batch 5745: loss 0.046007\n",
      "batch 5746: loss 0.022551\n",
      "batch 5747: loss 0.023967\n",
      "batch 5748: loss 0.013949\n",
      "batch 5749: loss 0.052300\n",
      "batch 5750: loss 0.237870\n",
      "batch 5751: loss 0.153779\n",
      "batch 5752: loss 0.018541\n",
      "batch 5753: loss 0.095509\n",
      "batch 5754: loss 0.136485\n",
      "batch 5755: loss 0.009888\n",
      "batch 5756: loss 0.009902\n",
      "batch 5757: loss 0.020958\n",
      "batch 5758: loss 0.032489\n",
      "batch 5759: loss 0.018374\n",
      "batch 5760: loss 0.067350\n",
      "batch 5761: loss 0.181739\n",
      "batch 5762: loss 0.042927\n",
      "batch 5763: loss 0.023242\n",
      "batch 5764: loss 0.037538\n",
      "batch 5765: loss 0.020885\n",
      "batch 5766: loss 0.047027\n",
      "batch 5767: loss 0.019877\n",
      "batch 5768: loss 0.027216\n",
      "batch 5769: loss 0.008846\n",
      "batch 5770: loss 0.080161\n",
      "batch 5771: loss 0.027378\n",
      "batch 5772: loss 0.085388\n",
      "batch 5773: loss 0.011461\n",
      "batch 5774: loss 0.023329\n",
      "batch 5775: loss 0.100551\n",
      "batch 5776: loss 0.074159\n",
      "batch 5777: loss 0.007468\n",
      "batch 5778: loss 0.024238\n",
      "batch 5779: loss 0.013779\n",
      "batch 5780: loss 0.043505\n",
      "batch 5781: loss 0.164560\n",
      "batch 5782: loss 0.017081\n",
      "batch 5783: loss 0.041465\n",
      "batch 5784: loss 0.006792\n",
      "batch 5785: loss 0.115434\n",
      "batch 5786: loss 0.028738\n",
      "batch 5787: loss 0.016175\n",
      "batch 5788: loss 0.085908\n",
      "batch 5789: loss 0.044538\n",
      "batch 5790: loss 0.040035\n",
      "batch 5791: loss 0.025433\n",
      "batch 5792: loss 0.075835\n",
      "batch 5793: loss 0.010352\n",
      "batch 5794: loss 0.015697\n",
      "batch 5795: loss 0.015847\n",
      "batch 5796: loss 0.089258\n",
      "batch 5797: loss 0.015239\n",
      "batch 5798: loss 0.070194\n",
      "batch 5799: loss 0.036587\n",
      "batch 5800: loss 0.011567\n",
      "batch 5801: loss 0.020439\n",
      "batch 5802: loss 0.070135\n",
      "batch 5803: loss 0.022973\n",
      "batch 5804: loss 0.034950\n",
      "batch 5805: loss 0.214175\n",
      "batch 5806: loss 0.078700\n",
      "batch 5807: loss 0.026790\n",
      "batch 5808: loss 0.018758\n",
      "batch 5809: loss 0.027240\n",
      "batch 5810: loss 0.019886\n",
      "batch 5811: loss 0.014519\n",
      "batch 5812: loss 0.014694\n",
      "batch 5813: loss 0.017670\n",
      "batch 5814: loss 0.026125\n",
      "batch 5815: loss 0.107219\n",
      "batch 5816: loss 0.053225\n",
      "batch 5817: loss 0.091367\n",
      "batch 5818: loss 0.005311\n",
      "batch 5819: loss 0.018343\n",
      "batch 5820: loss 0.042763\n",
      "batch 5821: loss 0.072800\n",
      "batch 5822: loss 0.023674\n",
      "batch 5823: loss 0.012462\n",
      "batch 5824: loss 0.103053\n",
      "batch 5825: loss 0.008255\n",
      "batch 5826: loss 0.006176\n",
      "batch 5827: loss 0.006451\n",
      "batch 5828: loss 0.093980\n",
      "batch 5829: loss 0.086596\n",
      "batch 5830: loss 0.014311\n",
      "batch 5831: loss 0.013620\n",
      "batch 5832: loss 0.092321\n",
      "batch 5833: loss 0.008867\n",
      "batch 5834: loss 0.030240\n",
      "batch 5835: loss 0.065896\n",
      "batch 5836: loss 0.026838\n",
      "batch 5837: loss 0.049042\n",
      "batch 5838: loss 0.032039\n",
      "batch 5839: loss 0.086935\n",
      "batch 5840: loss 0.020139\n",
      "batch 5841: loss 0.015493\n",
      "batch 5842: loss 0.033602\n",
      "batch 5843: loss 0.124287\n",
      "batch 5844: loss 0.025746\n",
      "batch 5845: loss 0.013119\n",
      "batch 5846: loss 0.119627\n",
      "batch 5847: loss 0.117779\n",
      "batch 5848: loss 0.035842\n",
      "batch 5849: loss 0.012801\n",
      "batch 5850: loss 0.016528\n",
      "batch 5851: loss 0.108995\n",
      "batch 5852: loss 0.270903\n",
      "batch 5853: loss 0.043700\n",
      "batch 5854: loss 0.021042\n",
      "batch 5855: loss 0.026848\n",
      "batch 5856: loss 0.019286\n",
      "batch 5857: loss 0.077673\n",
      "batch 5858: loss 0.009875\n",
      "batch 5859: loss 0.027073\n",
      "batch 5860: loss 0.147437\n",
      "batch 5861: loss 0.004798\n",
      "batch 5862: loss 0.011459\n",
      "batch 5863: loss 0.008842\n",
      "batch 5864: loss 0.135568\n",
      "batch 5865: loss 0.020198\n",
      "batch 5866: loss 0.009636\n",
      "batch 5867: loss 0.084396\n",
      "batch 5868: loss 0.146662\n",
      "batch 5869: loss 0.065364\n",
      "batch 5870: loss 0.117870\n",
      "batch 5871: loss 0.077594\n",
      "batch 5872: loss 0.002762\n",
      "batch 5873: loss 0.046725\n",
      "batch 5874: loss 0.016225\n",
      "batch 5875: loss 0.094194\n",
      "batch 5876: loss 0.053721\n",
      "batch 5877: loss 0.042585\n",
      "batch 5878: loss 0.021877\n",
      "batch 5879: loss 0.058392\n",
      "batch 5880: loss 0.009217\n",
      "batch 5881: loss 0.020707\n",
      "batch 5882: loss 0.020128\n",
      "batch 5883: loss 0.008337\n",
      "batch 5884: loss 0.028553\n",
      "batch 5885: loss 0.123889\n",
      "batch 5886: loss 0.004291\n",
      "batch 5887: loss 0.085501\n",
      "batch 5888: loss 0.031853\n",
      "batch 5889: loss 0.003670\n",
      "batch 5890: loss 0.021996\n",
      "batch 5891: loss 0.007759\n",
      "batch 5892: loss 0.085314\n",
      "batch 5893: loss 0.060912\n",
      "batch 5894: loss 0.032748\n",
      "batch 5895: loss 0.057755\n",
      "batch 5896: loss 0.163502\n",
      "batch 5897: loss 0.025312\n",
      "batch 5898: loss 0.014346\n",
      "batch 5899: loss 0.041461\n",
      "batch 5900: loss 0.188938\n",
      "batch 5901: loss 0.007289\n",
      "batch 5902: loss 0.031488\n",
      "batch 5903: loss 0.080950\n",
      "batch 5904: loss 0.021852\n",
      "batch 5905: loss 0.025898\n",
      "batch 5906: loss 0.014259\n",
      "batch 5907: loss 0.087514\n",
      "batch 5908: loss 0.067620\n",
      "batch 5909: loss 0.068579\n",
      "batch 5910: loss 0.068912\n",
      "batch 5911: loss 0.057903\n",
      "batch 5912: loss 0.030360\n",
      "batch 5913: loss 0.008523\n",
      "batch 5914: loss 0.116479\n",
      "batch 5915: loss 0.004862\n",
      "batch 5916: loss 0.022405\n",
      "batch 5917: loss 0.006154\n",
      "batch 5918: loss 0.039166\n",
      "batch 5919: loss 0.036785\n",
      "batch 5920: loss 0.018694\n",
      "batch 5921: loss 0.039079\n",
      "batch 5922: loss 0.013045\n",
      "batch 5923: loss 0.029507\n",
      "batch 5924: loss 0.044258\n",
      "batch 5925: loss 0.029230\n",
      "batch 5926: loss 0.016706\n",
      "batch 5927: loss 0.014156\n",
      "batch 5928: loss 0.008608\n",
      "batch 5929: loss 0.082908\n",
      "batch 5930: loss 0.034899\n",
      "batch 5931: loss 0.029163\n",
      "batch 5932: loss 0.050836\n",
      "batch 5933: loss 0.019735\n",
      "batch 5934: loss 0.045613\n",
      "batch 5935: loss 0.049368\n",
      "batch 5936: loss 0.103957\n",
      "batch 5937: loss 0.017651\n",
      "batch 5938: loss 0.008497\n",
      "batch 5939: loss 0.036205\n",
      "batch 5940: loss 0.004781\n",
      "batch 5941: loss 0.036323\n",
      "batch 5942: loss 0.006027\n",
      "batch 5943: loss 0.014895\n",
      "batch 5944: loss 0.032126\n",
      "batch 5945: loss 0.106612\n",
      "batch 5946: loss 0.025402\n",
      "batch 5947: loss 0.025499\n",
      "batch 5948: loss 0.056218\n",
      "batch 5949: loss 0.027475\n",
      "batch 5950: loss 0.077987\n",
      "batch 5951: loss 0.021892\n",
      "batch 5952: loss 0.031828\n",
      "batch 5953: loss 0.039965\n",
      "batch 5954: loss 0.016445\n",
      "batch 5955: loss 0.103404\n",
      "batch 5956: loss 0.052866\n",
      "batch 5957: loss 0.021696\n",
      "batch 5958: loss 0.048338\n",
      "batch 5959: loss 0.015704\n",
      "batch 5960: loss 0.049523\n",
      "batch 5961: loss 0.050085\n",
      "batch 5962: loss 0.011738\n",
      "batch 5963: loss 0.085833\n",
      "batch 5964: loss 0.025718\n",
      "batch 5965: loss 0.021133\n",
      "batch 5966: loss 0.070118\n",
      "batch 5967: loss 0.061697\n",
      "batch 5968: loss 0.049841\n",
      "batch 5969: loss 0.161554\n",
      "batch 5970: loss 0.010920\n",
      "batch 5971: loss 0.025413\n",
      "batch 5972: loss 0.027072\n",
      "batch 5973: loss 0.017893\n",
      "batch 5974: loss 0.010813\n",
      "batch 5975: loss 0.007097\n",
      "batch 5976: loss 0.036347\n",
      "batch 5977: loss 0.076908\n",
      "batch 5978: loss 0.023793\n",
      "batch 5979: loss 0.017295\n",
      "batch 5980: loss 0.033616\n",
      "batch 5981: loss 0.007658\n",
      "batch 5982: loss 0.041696\n",
      "batch 5983: loss 0.034761\n",
      "batch 5984: loss 0.018021\n",
      "batch 5985: loss 0.040195\n",
      "batch 5986: loss 0.044029\n",
      "batch 5987: loss 0.017827\n",
      "batch 5988: loss 0.139807\n",
      "batch 5989: loss 0.037861\n",
      "batch 5990: loss 0.132170\n",
      "batch 5991: loss 0.009161\n",
      "batch 5992: loss 0.095932\n",
      "batch 5993: loss 0.032661\n",
      "batch 5994: loss 0.014899\n",
      "batch 5995: loss 0.076534\n",
      "batch 5996: loss 0.035871\n",
      "batch 5997: loss 0.015092\n",
      "batch 5998: loss 0.013779\n",
      "batch 5999: loss 0.027770\n"
     ]
    }
   ],
   "source": [
    "    num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "    for batch_index in range(num_batches):\n",
    "        X, y = data_loader.get_batch(batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的評估： tf.keras.metrics\n",
    "最後，我們使用測試集評估模型的性能。這裡，我們使用 tf.keras.metrics 中的 SparseCategoricalAccuracy 評量器來評估模型在測試集上的性能，該評量器能夠對模型預測的結果與真實結果進行比較，並輸出預測正確的樣本數占總樣本數的比例。我們疊代測試資料集，每次通過 update_state() 方法向評量器輸入兩個參數： y_pred 和 y_true ，即模型預測出的結果和真實結果。評量器具有內部變數來保存當前評估指標相關的參數數值（例如當前已傳入的累計樣本數和當前預測正確的樣本數）。疊代結束後，我們使用 result() 方法輸出最終的評量指標值（預測正確的樣本數占總樣本數的比例）。\n",
    "\n",
    "在以下評量器程式碼中，我們提出了一個實例 tf.keras.metrics.SparseCategoricalAccuracy，並使用 For 循環疊代分批次傳入了測試集資料的預測結果與真實結果，並輸出訓練後的模型在測試資料集上的準確率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.974000\n"
     ]
    }
   ],
   "source": [
    "    sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    num_batches = int(data_loader.num_test_data // batch_size)\n",
    "    for batch_index in range(num_batches):\n",
    "        start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "        y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "        sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "    print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷積神經網路（CNN）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷積層神經元（卷積核）數目\n",
    "            kernel_size=[5, 5],     # 接受區的大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.301937\n",
      "batch 1: loss 2.222834\n",
      "batch 2: loss 1.975661\n",
      "batch 3: loss 1.982270\n",
      "batch 4: loss 1.738940\n",
      "batch 5: loss 1.592233\n",
      "batch 6: loss 1.353394\n",
      "batch 7: loss 1.416510\n",
      "batch 8: loss 1.063882\n",
      "batch 9: loss 0.811517\n",
      "batch 10: loss 0.917445\n",
      "batch 11: loss 0.622289\n",
      "batch 12: loss 0.894822\n",
      "batch 13: loss 0.627155\n",
      "batch 14: loss 0.578612\n",
      "batch 15: loss 0.482645\n",
      "batch 16: loss 0.422136\n",
      "batch 17: loss 0.427089\n",
      "batch 18: loss 0.404176\n",
      "batch 19: loss 0.491314\n",
      "batch 20: loss 0.450208\n",
      "batch 21: loss 0.590942\n",
      "batch 22: loss 0.461819\n",
      "batch 23: loss 0.648759\n",
      "batch 24: loss 0.450375\n",
      "batch 25: loss 0.508517\n",
      "batch 26: loss 0.560419\n",
      "batch 27: loss 0.187707\n",
      "batch 28: loss 0.291256\n",
      "batch 29: loss 0.611106\n",
      "batch 30: loss 0.593267\n",
      "batch 31: loss 0.287954\n",
      "batch 32: loss 0.425068\n",
      "batch 33: loss 0.464779\n",
      "batch 34: loss 0.278601\n",
      "batch 35: loss 0.442968\n",
      "batch 36: loss 0.399355\n",
      "batch 37: loss 0.264028\n",
      "batch 38: loss 0.257739\n",
      "batch 39: loss 0.261392\n",
      "batch 40: loss 0.222210\n",
      "batch 41: loss 0.399148\n",
      "batch 42: loss 0.246160\n",
      "batch 43: loss 0.299980\n",
      "batch 44: loss 0.266799\n",
      "batch 45: loss 0.206273\n",
      "batch 46: loss 0.143523\n",
      "batch 47: loss 0.325959\n",
      "batch 48: loss 0.293247\n",
      "batch 49: loss 0.320074\n",
      "batch 50: loss 0.177101\n",
      "batch 51: loss 0.101269\n",
      "batch 52: loss 0.116483\n",
      "batch 53: loss 0.266170\n",
      "batch 54: loss 0.688305\n",
      "batch 55: loss 0.383396\n",
      "batch 56: loss 0.121014\n",
      "batch 57: loss 0.267638\n",
      "batch 58: loss 0.182264\n",
      "batch 59: loss 0.742792\n",
      "batch 60: loss 0.104149\n",
      "batch 61: loss 0.068875\n",
      "batch 62: loss 0.335548\n",
      "batch 63: loss 0.088099\n",
      "batch 64: loss 0.361139\n",
      "batch 65: loss 0.248344\n",
      "batch 66: loss 0.266128\n",
      "batch 67: loss 0.306916\n",
      "batch 68: loss 0.208246\n",
      "batch 69: loss 0.329787\n",
      "batch 70: loss 0.087924\n",
      "batch 71: loss 0.199602\n",
      "batch 72: loss 0.153894\n",
      "batch 73: loss 0.154752\n",
      "batch 74: loss 0.155650\n",
      "batch 75: loss 0.070770\n",
      "batch 76: loss 0.103641\n",
      "batch 77: loss 0.259456\n",
      "batch 78: loss 0.159423\n",
      "batch 79: loss 0.228646\n",
      "batch 80: loss 0.050573\n",
      "batch 81: loss 0.088425\n",
      "batch 82: loss 0.286529\n",
      "batch 83: loss 0.134964\n",
      "batch 84: loss 0.063927\n",
      "batch 85: loss 0.242739\n",
      "batch 86: loss 0.102311\n",
      "batch 87: loss 0.121376\n",
      "batch 88: loss 0.309959\n",
      "batch 89: loss 0.106060\n",
      "batch 90: loss 0.275993\n",
      "batch 91: loss 0.148571\n",
      "batch 92: loss 0.076868\n",
      "batch 93: loss 0.100798\n",
      "batch 94: loss 0.163384\n",
      "batch 95: loss 0.066649\n",
      "batch 96: loss 0.109030\n",
      "batch 97: loss 0.165288\n",
      "batch 98: loss 0.029388\n",
      "batch 99: loss 0.094114\n",
      "batch 100: loss 0.175340\n",
      "batch 101: loss 0.211408\n",
      "batch 102: loss 0.225642\n",
      "batch 103: loss 0.104347\n",
      "batch 104: loss 0.190746\n",
      "batch 105: loss 0.110662\n",
      "batch 106: loss 0.231168\n",
      "batch 107: loss 0.122186\n",
      "batch 108: loss 0.302732\n",
      "batch 109: loss 0.164082\n",
      "batch 110: loss 0.137936\n",
      "batch 111: loss 0.095905\n",
      "batch 112: loss 0.092352\n",
      "batch 113: loss 0.101199\n",
      "batch 114: loss 0.226170\n",
      "batch 115: loss 0.179916\n",
      "batch 116: loss 0.080514\n",
      "batch 117: loss 0.052740\n",
      "batch 118: loss 0.115712\n",
      "batch 119: loss 0.189564\n",
      "batch 120: loss 0.113619\n",
      "batch 121: loss 0.071271\n",
      "batch 122: loss 0.201397\n",
      "batch 123: loss 0.163814\n",
      "batch 124: loss 0.095332\n",
      "batch 125: loss 0.138886\n",
      "batch 126: loss 0.080214\n",
      "batch 127: loss 0.109488\n",
      "batch 128: loss 0.118221\n",
      "batch 129: loss 0.108497\n",
      "batch 130: loss 0.259924\n",
      "batch 131: loss 0.140604\n",
      "batch 132: loss 0.163482\n",
      "batch 133: loss 0.174150\n",
      "batch 134: loss 0.074779\n",
      "batch 135: loss 0.246570\n",
      "batch 136: loss 0.178049\n",
      "batch 137: loss 0.124878\n",
      "batch 138: loss 0.060170\n",
      "batch 139: loss 0.029664\n",
      "batch 140: loss 0.242060\n",
      "batch 141: loss 0.040073\n",
      "batch 142: loss 0.107842\n",
      "batch 143: loss 0.176942\n",
      "batch 144: loss 0.106383\n",
      "batch 145: loss 0.059837\n",
      "batch 146: loss 0.072895\n",
      "batch 147: loss 0.224365\n",
      "batch 148: loss 0.097791\n",
      "batch 149: loss 0.102331\n",
      "batch 150: loss 0.156714\n",
      "batch 151: loss 0.172168\n",
      "batch 152: loss 0.095824\n",
      "batch 153: loss 0.146662\n",
      "batch 154: loss 0.193085\n",
      "batch 155: loss 0.206233\n",
      "batch 156: loss 0.130532\n",
      "batch 157: loss 0.125590\n",
      "batch 158: loss 0.204931\n",
      "batch 159: loss 0.117679\n",
      "batch 160: loss 0.020590\n",
      "batch 161: loss 0.109086\n",
      "batch 162: loss 0.107918\n",
      "batch 163: loss 0.270650\n",
      "batch 164: loss 0.141477\n",
      "batch 165: loss 0.125110\n",
      "batch 166: loss 0.122888\n",
      "batch 167: loss 0.038419\n",
      "batch 168: loss 0.135533\n",
      "batch 169: loss 0.260291\n",
      "batch 170: loss 0.113129\n",
      "batch 171: loss 0.080230\n",
      "batch 172: loss 0.137721\n",
      "batch 173: loss 0.103362\n",
      "batch 174: loss 0.132789\n",
      "batch 175: loss 0.100086\n",
      "batch 176: loss 0.078440\n",
      "batch 177: loss 0.115253\n",
      "batch 178: loss 0.112769\n",
      "batch 179: loss 0.154337\n",
      "batch 180: loss 0.107072\n",
      "batch 181: loss 0.170245\n",
      "batch 182: loss 0.132757\n",
      "batch 183: loss 0.059501\n",
      "batch 184: loss 0.137005\n",
      "batch 185: loss 0.085517\n",
      "batch 186: loss 0.066247\n",
      "batch 187: loss 0.151012\n",
      "batch 188: loss 0.034815\n",
      "batch 189: loss 0.118946\n",
      "batch 190: loss 0.034617\n",
      "batch 191: loss 0.049754\n",
      "batch 192: loss 0.109648\n",
      "batch 193: loss 0.045663\n",
      "batch 194: loss 0.028330\n",
      "batch 195: loss 0.154003\n",
      "batch 196: loss 0.089531\n",
      "batch 197: loss 0.064668\n",
      "batch 198: loss 0.103779\n",
      "batch 199: loss 0.114582\n",
      "batch 200: loss 0.102832\n",
      "batch 201: loss 0.017547\n",
      "batch 202: loss 0.280516\n",
      "batch 203: loss 0.022276\n",
      "batch 204: loss 0.103710\n",
      "batch 205: loss 0.159607\n",
      "batch 206: loss 0.056481\n",
      "batch 207: loss 0.068882\n",
      "batch 208: loss 0.215598\n",
      "batch 209: loss 0.163971\n",
      "batch 210: loss 0.165150\n",
      "batch 211: loss 0.151229\n",
      "batch 212: loss 0.184884\n",
      "batch 213: loss 0.186646\n",
      "batch 214: loss 0.077552\n",
      "batch 215: loss 0.093944\n",
      "batch 216: loss 0.114002\n",
      "batch 217: loss 0.050828\n",
      "batch 218: loss 0.150413\n",
      "batch 219: loss 0.163751\n",
      "batch 220: loss 0.163143\n",
      "batch 221: loss 0.213281\n",
      "batch 222: loss 0.157650\n",
      "batch 223: loss 0.155980\n",
      "batch 224: loss 0.058743\n",
      "batch 225: loss 0.030537\n",
      "batch 226: loss 0.196031\n",
      "batch 227: loss 0.024534\n",
      "batch 228: loss 0.020453\n",
      "batch 229: loss 0.175221\n",
      "batch 230: loss 0.157919\n",
      "batch 231: loss 0.073033\n",
      "batch 232: loss 0.212905\n",
      "batch 233: loss 0.072792\n",
      "batch 234: loss 0.018683\n",
      "batch 235: loss 0.069923\n",
      "batch 236: loss 0.132743\n",
      "batch 237: loss 0.154714\n",
      "batch 238: loss 0.277896\n",
      "batch 239: loss 0.063429\n",
      "batch 240: loss 0.042415\n",
      "batch 241: loss 0.020635\n",
      "batch 242: loss 0.026627\n",
      "batch 243: loss 0.228356\n",
      "batch 244: loss 0.033375\n",
      "batch 245: loss 0.097568\n",
      "batch 246: loss 0.031103\n",
      "batch 247: loss 0.039480\n",
      "batch 248: loss 0.104081\n",
      "batch 249: loss 0.088260\n",
      "batch 250: loss 0.110704\n",
      "batch 251: loss 0.174623\n",
      "batch 252: loss 0.045664\n",
      "batch 253: loss 0.117044\n",
      "batch 254: loss 0.019693\n",
      "batch 255: loss 0.040064\n",
      "batch 256: loss 0.193117\n",
      "batch 257: loss 0.140911\n",
      "batch 258: loss 0.093236\n",
      "batch 259: loss 0.126496\n",
      "batch 260: loss 0.087286\n",
      "batch 261: loss 0.060614\n",
      "batch 262: loss 0.027522\n",
      "batch 263: loss 0.160466\n",
      "batch 264: loss 0.114158\n",
      "batch 265: loss 0.097330\n",
      "batch 266: loss 0.210260\n",
      "batch 267: loss 0.039735\n",
      "batch 268: loss 0.080619\n",
      "batch 269: loss 0.110955\n",
      "batch 270: loss 0.079075\n",
      "batch 271: loss 0.078948\n",
      "batch 272: loss 0.027129\n",
      "batch 273: loss 0.127010\n",
      "batch 274: loss 0.148418\n",
      "batch 275: loss 0.065602\n",
      "batch 276: loss 0.045549\n",
      "batch 277: loss 0.050391\n",
      "batch 278: loss 0.033410\n",
      "batch 279: loss 0.146938\n",
      "batch 280: loss 0.107673\n",
      "batch 281: loss 0.020061\n",
      "batch 282: loss 0.062985\n",
      "batch 283: loss 0.072939\n",
      "batch 284: loss 0.035103\n",
      "batch 285: loss 0.152622\n",
      "batch 286: loss 0.138564\n",
      "batch 287: loss 0.030810\n",
      "batch 288: loss 0.006345\n",
      "batch 289: loss 0.064291\n",
      "batch 290: loss 0.055304\n",
      "batch 291: loss 0.018793\n",
      "batch 292: loss 0.175071\n",
      "batch 293: loss 0.012064\n",
      "batch 294: loss 0.138870\n",
      "batch 295: loss 0.046092\n",
      "batch 296: loss 0.117794\n",
      "batch 297: loss 0.120970\n",
      "batch 298: loss 0.039168\n",
      "batch 299: loss 0.050750\n",
      "batch 300: loss 0.051260\n",
      "batch 301: loss 0.122446\n",
      "batch 302: loss 0.162676\n",
      "batch 303: loss 0.015788\n",
      "batch 304: loss 0.010674\n",
      "batch 305: loss 0.010888\n",
      "batch 306: loss 0.073359\n",
      "batch 307: loss 0.013854\n",
      "batch 308: loss 0.015979\n",
      "batch 309: loss 0.178030\n",
      "batch 310: loss 0.009105\n",
      "batch 311: loss 0.039201\n",
      "batch 312: loss 0.054645\n",
      "batch 313: loss 0.008196\n",
      "batch 314: loss 0.349069\n",
      "batch 315: loss 0.102643\n",
      "batch 316: loss 0.045034\n",
      "batch 317: loss 0.020289\n",
      "batch 318: loss 0.030565\n",
      "batch 319: loss 0.019801\n",
      "batch 320: loss 0.073675\n",
      "batch 321: loss 0.057921\n",
      "batch 322: loss 0.048775\n",
      "batch 323: loss 0.283605\n",
      "batch 324: loss 0.010305\n",
      "batch 325: loss 0.038380\n",
      "batch 326: loss 0.080752\n",
      "batch 327: loss 0.036687\n",
      "batch 328: loss 0.014499\n",
      "batch 329: loss 0.010555\n",
      "batch 330: loss 0.087723\n",
      "batch 331: loss 0.087885\n",
      "batch 332: loss 0.006017\n",
      "batch 333: loss 0.047633\n",
      "batch 334: loss 0.074694\n",
      "batch 335: loss 0.023742\n",
      "batch 336: loss 0.028847\n",
      "batch 337: loss 0.136024\n",
      "batch 338: loss 0.224896\n",
      "batch 339: loss 0.255322\n",
      "batch 340: loss 0.008833\n",
      "batch 341: loss 0.026100\n",
      "batch 342: loss 0.027541\n",
      "batch 343: loss 0.006709\n",
      "batch 344: loss 0.086607\n",
      "batch 345: loss 0.196591\n",
      "batch 346: loss 0.124975\n",
      "batch 347: loss 0.104974\n",
      "batch 348: loss 0.012460\n",
      "batch 349: loss 0.034316\n",
      "batch 350: loss 0.042748\n",
      "batch 351: loss 0.040844\n",
      "batch 352: loss 0.059689\n",
      "batch 353: loss 0.072292\n",
      "batch 354: loss 0.079375\n",
      "batch 355: loss 0.101208\n",
      "batch 356: loss 0.164196\n",
      "batch 357: loss 0.077017\n",
      "batch 358: loss 0.012298\n",
      "batch 359: loss 0.184139\n",
      "batch 360: loss 0.030634\n",
      "batch 361: loss 0.056522\n",
      "batch 362: loss 0.096994\n",
      "batch 363: loss 0.070932\n",
      "batch 364: loss 0.101824\n",
      "batch 365: loss 0.103980\n",
      "batch 366: loss 0.060428\n",
      "batch 367: loss 0.074128\n",
      "batch 368: loss 0.102687\n",
      "batch 369: loss 0.154632\n",
      "batch 370: loss 0.068305\n",
      "batch 371: loss 0.009765\n",
      "batch 372: loss 0.005022\n",
      "batch 373: loss 0.014907\n",
      "batch 374: loss 0.069919\n",
      "batch 375: loss 0.069140\n",
      "batch 376: loss 0.079978\n",
      "batch 377: loss 0.097250\n",
      "batch 378: loss 0.021970\n",
      "batch 379: loss 0.097620\n",
      "batch 380: loss 0.056478\n",
      "batch 381: loss 0.105952\n",
      "batch 382: loss 0.064733\n",
      "batch 383: loss 0.011176\n",
      "batch 384: loss 0.055308\n",
      "batch 385: loss 0.267742\n",
      "batch 386: loss 0.018034\n",
      "batch 387: loss 0.010955\n",
      "batch 388: loss 0.145029\n",
      "batch 389: loss 0.259760\n",
      "batch 390: loss 0.015438\n",
      "batch 391: loss 0.025215\n",
      "batch 392: loss 0.058585\n",
      "batch 393: loss 0.021008\n",
      "batch 394: loss 0.042694\n",
      "batch 395: loss 0.018339\n",
      "batch 396: loss 0.099460\n",
      "batch 397: loss 0.038495\n",
      "batch 398: loss 0.023015\n",
      "batch 399: loss 0.057855\n",
      "batch 400: loss 0.021583\n",
      "batch 401: loss 0.179956\n",
      "batch 402: loss 0.049687\n",
      "batch 403: loss 0.021364\n",
      "batch 404: loss 0.167222\n",
      "batch 405: loss 0.047677\n",
      "batch 406: loss 0.019931\n",
      "batch 407: loss 0.004059\n",
      "batch 408: loss 0.120534\n",
      "batch 409: loss 0.058569\n",
      "batch 410: loss 0.059251\n",
      "batch 411: loss 0.091770\n",
      "batch 412: loss 0.097232\n",
      "batch 413: loss 0.101298\n",
      "batch 414: loss 0.076896\n",
      "batch 415: loss 0.104448\n",
      "batch 416: loss 0.111195\n",
      "batch 417: loss 0.013446\n",
      "batch 418: loss 0.016543\n",
      "batch 419: loss 0.005283\n",
      "batch 420: loss 0.008097\n",
      "batch 421: loss 0.026349\n",
      "batch 422: loss 0.020604\n",
      "batch 423: loss 0.092507\n",
      "batch 424: loss 0.002360\n",
      "batch 425: loss 0.025106\n",
      "batch 426: loss 0.082091\n",
      "batch 427: loss 0.130162\n",
      "batch 428: loss 0.014711\n",
      "batch 429: loss 0.037435\n",
      "batch 430: loss 0.009572\n",
      "batch 431: loss 0.007457\n",
      "batch 432: loss 0.247312\n",
      "batch 433: loss 0.093376\n",
      "batch 434: loss 0.022104\n",
      "batch 435: loss 0.063913\n",
      "batch 436: loss 0.008480\n",
      "batch 437: loss 0.139363\n",
      "batch 438: loss 0.018794\n",
      "batch 439: loss 0.081236\n",
      "batch 440: loss 0.133301\n",
      "batch 441: loss 0.042613\n",
      "batch 442: loss 0.012174\n",
      "batch 443: loss 0.097720\n",
      "batch 444: loss 0.065555\n",
      "batch 445: loss 0.132899\n",
      "batch 446: loss 0.111800\n",
      "batch 447: loss 0.090287\n",
      "batch 448: loss 0.026235\n",
      "batch 449: loss 0.089083\n",
      "batch 450: loss 0.054917\n",
      "batch 451: loss 0.013993\n",
      "batch 452: loss 0.092541\n",
      "batch 453: loss 0.022124\n",
      "batch 454: loss 0.012661\n",
      "batch 455: loss 0.034897\n",
      "batch 456: loss 0.166365\n",
      "batch 457: loss 0.190372\n",
      "batch 458: loss 0.121445\n",
      "batch 459: loss 0.209256\n",
      "batch 460: loss 0.068338\n",
      "batch 461: loss 0.006169\n",
      "batch 462: loss 0.065224\n",
      "batch 463: loss 0.171124\n",
      "batch 464: loss 0.021632\n",
      "batch 465: loss 0.126072\n",
      "batch 466: loss 0.027272\n",
      "batch 467: loss 0.142995\n",
      "batch 468: loss 0.133810\n",
      "batch 469: loss 0.012504\n",
      "batch 470: loss 0.055101\n",
      "batch 471: loss 0.149656\n",
      "batch 472: loss 0.018394\n",
      "batch 473: loss 0.065890\n",
      "batch 474: loss 0.174706\n",
      "batch 475: loss 0.119975\n",
      "batch 476: loss 0.216596\n",
      "batch 477: loss 0.079168\n",
      "batch 478: loss 0.122013\n",
      "batch 479: loss 0.236284\n",
      "batch 480: loss 0.031867\n",
      "batch 481: loss 0.067344\n",
      "batch 482: loss 0.012449\n",
      "batch 483: loss 0.022029\n",
      "batch 484: loss 0.041579\n",
      "batch 485: loss 0.029985\n",
      "batch 486: loss 0.049968\n",
      "batch 487: loss 0.106845\n",
      "batch 488: loss 0.052485\n",
      "batch 489: loss 0.089169\n",
      "batch 490: loss 0.014065\n",
      "batch 491: loss 0.028794\n",
      "batch 492: loss 0.019741\n",
      "batch 493: loss 0.013481\n",
      "batch 494: loss 0.007551\n",
      "batch 495: loss 0.061739\n",
      "batch 496: loss 0.189841\n",
      "batch 497: loss 0.060635\n",
      "batch 498: loss 0.004100\n",
      "batch 499: loss 0.037122\n",
      "batch 500: loss 0.110086\n",
      "batch 501: loss 0.062641\n",
      "batch 502: loss 0.061866\n",
      "batch 503: loss 0.213915\n",
      "batch 504: loss 0.025820\n",
      "batch 505: loss 0.020786\n",
      "batch 506: loss 0.224983\n",
      "batch 507: loss 0.048294\n",
      "batch 508: loss 0.131600\n",
      "batch 509: loss 0.176039\n",
      "batch 510: loss 0.183946\n",
      "batch 511: loss 0.209933\n",
      "batch 512: loss 0.007382\n",
      "batch 513: loss 0.042517\n",
      "batch 514: loss 0.176171\n",
      "batch 515: loss 0.042834\n",
      "batch 516: loss 0.058367\n",
      "batch 517: loss 0.078245\n",
      "batch 518: loss 0.052388\n",
      "batch 519: loss 0.186977\n",
      "batch 520: loss 0.069016\n",
      "batch 521: loss 0.020940\n",
      "batch 522: loss 0.052646\n",
      "batch 523: loss 0.003900\n",
      "batch 524: loss 0.149981\n",
      "batch 525: loss 0.059833\n",
      "batch 526: loss 0.027844\n",
      "batch 527: loss 0.028520\n",
      "batch 528: loss 0.043479\n",
      "batch 529: loss 0.033930\n",
      "batch 530: loss 0.020619\n",
      "batch 531: loss 0.119608\n",
      "batch 532: loss 0.045843\n",
      "batch 533: loss 0.013146\n",
      "batch 534: loss 0.021626\n",
      "batch 535: loss 0.063082\n",
      "batch 536: loss 0.019363\n",
      "batch 537: loss 0.018134\n",
      "batch 538: loss 0.040537\n",
      "batch 539: loss 0.193484\n",
      "batch 540: loss 0.035746\n",
      "batch 541: loss 0.016291\n",
      "batch 542: loss 0.012193\n",
      "batch 543: loss 0.006815\n",
      "batch 544: loss 0.004870\n",
      "batch 545: loss 0.009089\n",
      "batch 546: loss 0.006933\n",
      "batch 547: loss 0.014738\n",
      "batch 548: loss 0.045124\n",
      "batch 549: loss 0.020741\n",
      "batch 550: loss 0.088804\n",
      "batch 551: loss 0.120729\n",
      "batch 552: loss 0.009796\n",
      "batch 553: loss 0.012703\n",
      "batch 554: loss 0.304237\n",
      "batch 555: loss 0.193373\n",
      "batch 556: loss 0.024909\n",
      "batch 557: loss 0.036365\n",
      "batch 558: loss 0.053325\n",
      "batch 559: loss 0.030133\n",
      "batch 560: loss 0.006322\n",
      "batch 561: loss 0.027189\n",
      "batch 562: loss 0.113015\n",
      "batch 563: loss 0.040428\n",
      "batch 564: loss 0.043108\n",
      "batch 565: loss 0.032282\n",
      "batch 566: loss 0.103189\n",
      "batch 567: loss 0.015729\n",
      "batch 568: loss 0.054581\n",
      "batch 569: loss 0.027037\n",
      "batch 570: loss 0.014478\n",
      "batch 571: loss 0.037505\n",
      "batch 572: loss 0.075381\n",
      "batch 573: loss 0.100455\n",
      "batch 574: loss 0.002976\n",
      "batch 575: loss 0.007234\n",
      "batch 576: loss 0.025959\n",
      "batch 577: loss 0.108063\n",
      "batch 578: loss 0.052627\n",
      "batch 579: loss 0.247748\n",
      "batch 580: loss 0.081580\n",
      "batch 581: loss 0.074339\n",
      "batch 582: loss 0.023127\n",
      "batch 583: loss 0.014948\n",
      "batch 584: loss 0.007953\n",
      "batch 585: loss 0.018991\n",
      "batch 586: loss 0.010885\n",
      "batch 587: loss 0.119511\n",
      "batch 588: loss 0.085568\n",
      "batch 589: loss 0.012938\n",
      "batch 590: loss 0.091489\n",
      "batch 591: loss 0.018761\n",
      "batch 592: loss 0.243010\n",
      "batch 593: loss 0.007075\n",
      "batch 594: loss 0.013038\n",
      "batch 595: loss 0.046827\n",
      "batch 596: loss 0.063636\n",
      "batch 597: loss 0.038793\n",
      "batch 598: loss 0.041814\n",
      "batch 599: loss 0.033062\n",
      "batch 600: loss 0.010075\n",
      "batch 601: loss 0.012913\n",
      "batch 602: loss 0.005527\n",
      "batch 603: loss 0.019344\n",
      "batch 604: loss 0.061285\n",
      "batch 605: loss 0.219681\n",
      "batch 606: loss 0.016463\n",
      "batch 607: loss 0.019108\n",
      "batch 608: loss 0.004892\n",
      "batch 609: loss 0.017991\n",
      "batch 610: loss 0.010560\n",
      "batch 611: loss 0.241307\n",
      "batch 612: loss 0.014179\n",
      "batch 613: loss 0.043841\n",
      "batch 614: loss 0.029304\n",
      "batch 615: loss 0.109011\n",
      "batch 616: loss 0.048569\n",
      "batch 617: loss 0.009632\n",
      "batch 618: loss 0.004418\n",
      "batch 619: loss 0.016204\n",
      "batch 620: loss 0.028738\n",
      "batch 621: loss 0.035548\n",
      "batch 622: loss 0.011116\n",
      "batch 623: loss 0.026580\n",
      "batch 624: loss 0.033862\n",
      "batch 625: loss 0.058796\n",
      "batch 626: loss 0.084753\n",
      "batch 627: loss 0.019777\n",
      "batch 628: loss 0.020495\n",
      "batch 629: loss 0.025723\n",
      "batch 630: loss 0.068722\n",
      "batch 631: loss 0.066577\n",
      "batch 632: loss 0.018567\n",
      "batch 633: loss 0.047193\n",
      "batch 634: loss 0.032362\n",
      "batch 635: loss 0.038187\n",
      "batch 636: loss 0.010577\n",
      "batch 637: loss 0.035990\n",
      "batch 638: loss 0.005120\n",
      "batch 639: loss 0.137521\n",
      "batch 640: loss 0.030140\n",
      "batch 641: loss 0.041646\n",
      "batch 642: loss 0.010988\n",
      "batch 643: loss 0.052613\n",
      "batch 644: loss 0.043193\n",
      "batch 645: loss 0.025052\n",
      "batch 646: loss 0.008195\n",
      "batch 647: loss 0.014835\n",
      "batch 648: loss 0.010306\n",
      "batch 649: loss 0.038950\n",
      "batch 650: loss 0.106492\n",
      "batch 651: loss 0.091555\n",
      "batch 652: loss 0.011262\n",
      "batch 653: loss 0.008935\n",
      "batch 654: loss 0.096604\n",
      "batch 655: loss 0.008223\n",
      "batch 656: loss 0.109816\n",
      "batch 657: loss 0.060463\n",
      "batch 658: loss 0.038729\n",
      "batch 659: loss 0.095579\n",
      "batch 660: loss 0.007058\n",
      "batch 661: loss 0.105651\n",
      "batch 662: loss 0.071355\n",
      "batch 663: loss 0.012270\n",
      "batch 664: loss 0.019380\n",
      "batch 665: loss 0.031308\n",
      "batch 666: loss 0.079351\n",
      "batch 667: loss 0.045820\n",
      "batch 668: loss 0.279816\n",
      "batch 669: loss 0.036235\n",
      "batch 670: loss 0.018432\n",
      "batch 671: loss 0.002362\n",
      "batch 672: loss 0.013078\n",
      "batch 673: loss 0.015787\n",
      "batch 674: loss 0.026085\n",
      "batch 675: loss 0.030179\n",
      "batch 676: loss 0.007783\n",
      "batch 677: loss 0.035787\n",
      "batch 678: loss 0.043196\n",
      "batch 679: loss 0.026222\n",
      "batch 680: loss 0.024261\n",
      "batch 681: loss 0.050041\n",
      "batch 682: loss 0.052576\n",
      "batch 683: loss 0.043604\n",
      "batch 684: loss 0.167033\n",
      "batch 685: loss 0.119026\n",
      "batch 686: loss 0.132860\n",
      "batch 687: loss 0.057119\n",
      "batch 688: loss 0.228616\n",
      "batch 689: loss 0.058401\n",
      "batch 690: loss 0.160575\n",
      "batch 691: loss 0.070293\n",
      "batch 692: loss 0.010353\n",
      "batch 693: loss 0.107115\n",
      "batch 694: loss 0.072628\n",
      "batch 695: loss 0.018911\n",
      "batch 696: loss 0.027455\n",
      "batch 697: loss 0.027936\n",
      "batch 698: loss 0.026849\n",
      "batch 699: loss 0.009715\n",
      "batch 700: loss 0.009797\n",
      "batch 701: loss 0.038071\n",
      "batch 702: loss 0.113363\n",
      "batch 703: loss 0.049351\n",
      "batch 704: loss 0.015313\n",
      "batch 705: loss 0.019793\n",
      "batch 706: loss 0.016763\n",
      "batch 707: loss 0.026763\n",
      "batch 708: loss 0.184186\n",
      "batch 709: loss 0.003350\n",
      "batch 710: loss 0.013180\n",
      "batch 711: loss 0.022527\n",
      "batch 712: loss 0.038749\n",
      "batch 713: loss 0.030255\n",
      "batch 714: loss 0.070349\n",
      "batch 715: loss 0.142828\n",
      "batch 716: loss 0.003863\n",
      "batch 717: loss 0.091571\n",
      "batch 718: loss 0.023031\n",
      "batch 719: loss 0.041887\n",
      "batch 720: loss 0.004340\n",
      "batch 721: loss 0.020679\n",
      "batch 722: loss 0.051447\n",
      "batch 723: loss 0.038042\n",
      "batch 724: loss 0.022214\n",
      "batch 725: loss 0.002739\n",
      "batch 726: loss 0.020647\n",
      "batch 727: loss 0.165471\n",
      "batch 728: loss 0.053185\n",
      "batch 729: loss 0.096067\n",
      "batch 730: loss 0.152459\n",
      "batch 731: loss 0.064946\n",
      "batch 732: loss 0.050274\n",
      "batch 733: loss 0.016347\n",
      "batch 734: loss 0.115358\n",
      "batch 735: loss 0.008520\n",
      "batch 736: loss 0.045449\n",
      "batch 737: loss 0.026284\n",
      "batch 738: loss 0.099700\n",
      "batch 739: loss 0.106744\n",
      "batch 740: loss 0.028083\n",
      "batch 741: loss 0.274622\n",
      "batch 742: loss 0.012562\n",
      "batch 743: loss 0.020225\n",
      "batch 744: loss 0.031862\n",
      "batch 745: loss 0.004878\n",
      "batch 746: loss 0.015771\n",
      "batch 747: loss 0.035092\n",
      "batch 748: loss 0.248235\n",
      "batch 749: loss 0.027294\n",
      "batch 750: loss 0.049056\n",
      "batch 751: loss 0.010350\n",
      "batch 752: loss 0.139594\n",
      "batch 753: loss 0.140749\n",
      "batch 754: loss 0.084699\n",
      "batch 755: loss 0.095716\n",
      "batch 756: loss 0.021369\n",
      "batch 757: loss 0.032755\n",
      "batch 758: loss 0.094775\n",
      "batch 759: loss 0.089063\n",
      "batch 760: loss 0.033526\n",
      "batch 761: loss 0.124164\n",
      "batch 762: loss 0.074410\n",
      "batch 763: loss 0.061791\n",
      "batch 764: loss 0.022732\n",
      "batch 765: loss 0.159395\n",
      "batch 766: loss 0.040100\n",
      "batch 767: loss 0.125949\n",
      "batch 768: loss 0.009113\n",
      "batch 769: loss 0.038269\n",
      "batch 770: loss 0.069951\n",
      "batch 771: loss 0.014473\n",
      "batch 772: loss 0.015423\n",
      "batch 773: loss 0.043414\n",
      "batch 774: loss 0.079178\n",
      "batch 775: loss 0.076244\n",
      "batch 776: loss 0.044636\n",
      "batch 777: loss 0.093305\n",
      "batch 778: loss 0.009617\n",
      "batch 779: loss 0.047413\n",
      "batch 780: loss 0.016246\n",
      "batch 781: loss 0.081771\n",
      "batch 782: loss 0.130684\n",
      "batch 783: loss 0.033943\n",
      "batch 784: loss 0.007403\n",
      "batch 785: loss 0.020978\n",
      "batch 786: loss 0.000792\n",
      "batch 787: loss 0.033231\n",
      "batch 788: loss 0.116876\n",
      "batch 789: loss 0.120042\n",
      "batch 790: loss 0.028460\n",
      "batch 791: loss 0.037640\n",
      "batch 792: loss 0.007101\n",
      "batch 793: loss 0.052747\n",
      "batch 794: loss 0.015592\n",
      "batch 795: loss 0.114953\n",
      "batch 796: loss 0.108949\n",
      "batch 797: loss 0.046096\n",
      "batch 798: loss 0.069665\n",
      "batch 799: loss 0.070750\n",
      "batch 800: loss 0.020130\n",
      "batch 801: loss 0.011620\n",
      "batch 802: loss 0.038647\n",
      "batch 803: loss 0.264688\n",
      "batch 804: loss 0.017506\n",
      "batch 805: loss 0.079353\n",
      "batch 806: loss 0.006104\n",
      "batch 807: loss 0.012849\n",
      "batch 808: loss 0.021387\n",
      "batch 809: loss 0.044394\n",
      "batch 810: loss 0.010368\n",
      "batch 811: loss 0.001383\n",
      "batch 812: loss 0.006884\n",
      "batch 813: loss 0.028245\n",
      "batch 814: loss 0.054325\n",
      "batch 815: loss 0.015404\n",
      "batch 816: loss 0.009101\n",
      "batch 817: loss 0.025625\n",
      "batch 818: loss 0.087006\n",
      "batch 819: loss 0.053677\n",
      "batch 820: loss 0.094187\n",
      "batch 821: loss 0.030532\n",
      "batch 822: loss 0.004617\n",
      "batch 823: loss 0.002397\n",
      "batch 824: loss 0.028544\n",
      "batch 825: loss 0.042869\n",
      "batch 826: loss 0.048625\n",
      "batch 827: loss 0.052022\n",
      "batch 828: loss 0.011573\n",
      "batch 829: loss 0.019069\n",
      "batch 830: loss 0.065933\n",
      "batch 831: loss 0.085148\n",
      "batch 832: loss 0.003340\n",
      "batch 833: loss 0.005207\n",
      "batch 834: loss 0.005578\n",
      "batch 835: loss 0.001655\n",
      "batch 836: loss 0.013332\n",
      "batch 837: loss 0.027091\n",
      "batch 838: loss 0.039050\n",
      "batch 839: loss 0.031661\n",
      "batch 840: loss 0.006555\n",
      "batch 841: loss 0.035904\n",
      "batch 842: loss 0.018401\n",
      "batch 843: loss 0.058308\n",
      "batch 844: loss 0.026047\n",
      "batch 845: loss 0.002049\n",
      "batch 846: loss 0.001965\n",
      "batch 847: loss 0.018703\n",
      "batch 848: loss 0.086045\n",
      "batch 849: loss 0.008218\n",
      "batch 850: loss 0.032102\n",
      "batch 851: loss 0.054582\n",
      "batch 852: loss 0.028556\n",
      "batch 853: loss 0.012679\n",
      "batch 854: loss 0.006455\n",
      "batch 855: loss 0.066331\n",
      "batch 856: loss 0.002707\n",
      "batch 857: loss 0.038558\n",
      "batch 858: loss 0.015128\n",
      "batch 859: loss 0.125733\n",
      "batch 860: loss 0.049529\n",
      "batch 861: loss 0.114375\n",
      "batch 862: loss 0.047898\n",
      "batch 863: loss 0.011861\n",
      "batch 864: loss 0.126915\n",
      "batch 865: loss 0.004929\n",
      "batch 866: loss 0.036705\n",
      "batch 867: loss 0.038997\n",
      "batch 868: loss 0.057491\n",
      "batch 869: loss 0.045773\n",
      "batch 870: loss 0.047400\n",
      "batch 871: loss 0.028799\n",
      "batch 872: loss 0.185663\n",
      "batch 873: loss 0.014309\n",
      "batch 874: loss 0.005588\n",
      "batch 875: loss 0.015602\n",
      "batch 876: loss 0.033185\n",
      "batch 877: loss 0.116782\n",
      "batch 878: loss 0.009779\n",
      "batch 879: loss 0.071780\n",
      "batch 880: loss 0.053982\n",
      "batch 881: loss 0.095779\n",
      "batch 882: loss 0.080527\n",
      "batch 883: loss 0.023376\n",
      "batch 884: loss 0.094568\n",
      "batch 885: loss 0.067048\n",
      "batch 886: loss 0.119013\n",
      "batch 887: loss 0.005820\n",
      "batch 888: loss 0.017500\n",
      "batch 889: loss 0.007778\n",
      "batch 890: loss 0.109682\n",
      "batch 891: loss 0.028717\n",
      "batch 892: loss 0.132550\n",
      "batch 893: loss 0.026012\n",
      "batch 894: loss 0.003976\n",
      "batch 895: loss 0.051829\n",
      "batch 896: loss 0.054335\n",
      "batch 897: loss 0.001489\n",
      "batch 898: loss 0.003793\n",
      "batch 899: loss 0.003576\n",
      "batch 900: loss 0.173975\n",
      "batch 901: loss 0.011503\n",
      "batch 902: loss 0.148617\n",
      "batch 903: loss 0.025165\n",
      "batch 904: loss 0.055440\n",
      "batch 905: loss 0.041067\n",
      "batch 906: loss 0.018329\n",
      "batch 907: loss 0.042375\n",
      "batch 908: loss 0.136308\n",
      "batch 909: loss 0.086685\n",
      "batch 910: loss 0.011774\n",
      "batch 911: loss 0.054171\n",
      "batch 912: loss 0.195640\n",
      "batch 913: loss 0.012580\n",
      "batch 914: loss 0.011201\n",
      "batch 915: loss 0.079762\n",
      "batch 916: loss 0.045352\n",
      "batch 917: loss 0.017947\n",
      "batch 918: loss 0.004322\n",
      "batch 919: loss 0.005188\n",
      "batch 920: loss 0.089214\n",
      "batch 921: loss 0.016661\n",
      "batch 922: loss 0.047627\n",
      "batch 923: loss 0.022804\n",
      "batch 924: loss 0.326104\n",
      "batch 925: loss 0.086969\n",
      "batch 926: loss 0.003750\n",
      "batch 927: loss 0.004901\n",
      "batch 928: loss 0.074588\n",
      "batch 929: loss 0.016466\n",
      "batch 930: loss 0.060770\n",
      "batch 931: loss 0.035181\n",
      "batch 932: loss 0.039134\n",
      "batch 933: loss 0.131169\n",
      "batch 934: loss 0.078535\n",
      "batch 935: loss 0.187186\n",
      "batch 936: loss 0.120062\n",
      "batch 937: loss 0.064747\n",
      "batch 938: loss 0.005711\n",
      "batch 939: loss 0.044646\n",
      "batch 940: loss 0.035618\n",
      "batch 941: loss 0.050646\n",
      "batch 942: loss 0.039602\n",
      "batch 943: loss 0.067723\n",
      "batch 944: loss 0.039610\n",
      "batch 945: loss 0.134522\n",
      "batch 946: loss 0.074400\n",
      "batch 947: loss 0.006757\n",
      "batch 948: loss 0.111779\n",
      "batch 949: loss 0.022828\n",
      "batch 950: loss 0.120257\n",
      "batch 951: loss 0.058636\n",
      "batch 952: loss 0.036230\n",
      "batch 953: loss 0.021942\n",
      "batch 954: loss 0.108414\n",
      "batch 955: loss 0.271336\n",
      "batch 956: loss 0.061086\n",
      "batch 957: loss 0.069999\n",
      "batch 958: loss 0.032485\n",
      "batch 959: loss 0.007949\n",
      "batch 960: loss 0.009470\n",
      "batch 961: loss 0.017321\n",
      "batch 962: loss 0.007871\n",
      "batch 963: loss 0.062437\n",
      "batch 964: loss 0.037171\n",
      "batch 965: loss 0.013991\n",
      "batch 966: loss 0.005000\n",
      "batch 967: loss 0.004245\n",
      "batch 968: loss 0.007004\n",
      "batch 969: loss 0.005297\n",
      "batch 970: loss 0.068685\n",
      "batch 971: loss 0.045696\n",
      "batch 972: loss 0.028165\n",
      "batch 973: loss 0.170973\n",
      "batch 974: loss 0.011357\n",
      "batch 975: loss 0.028755\n",
      "batch 976: loss 0.015899\n",
      "batch 977: loss 0.014569\n",
      "batch 978: loss 0.020104\n",
      "batch 979: loss 0.029980\n",
      "batch 980: loss 0.004768\n",
      "batch 981: loss 0.131510\n",
      "batch 982: loss 0.035060\n",
      "batch 983: loss 0.009051\n",
      "batch 984: loss 0.016085\n",
      "batch 985: loss 0.002345\n",
      "batch 986: loss 0.022069\n",
      "batch 987: loss 0.005628\n",
      "batch 988: loss 0.008803\n",
      "batch 989: loss 0.021194\n",
      "batch 990: loss 0.022419\n",
      "batch 991: loss 0.007662\n",
      "batch 992: loss 0.023717\n",
      "batch 993: loss 0.096939\n",
      "batch 994: loss 0.007478\n",
      "batch 995: loss 0.020270\n",
      "batch 996: loss 0.087842\n",
      "batch 997: loss 0.022850\n",
      "batch 998: loss 0.000485\n",
      "batch 999: loss 0.144396\n",
      "batch 1000: loss 0.018117\n",
      "batch 1001: loss 0.008147\n",
      "batch 1002: loss 0.007017\n",
      "batch 1003: loss 0.009292\n",
      "batch 1004: loss 0.008032\n",
      "batch 1005: loss 0.002791\n",
      "batch 1006: loss 0.002811\n",
      "batch 1007: loss 0.001026\n",
      "batch 1008: loss 0.044383\n",
      "batch 1009: loss 0.012246\n",
      "batch 1010: loss 0.009116\n",
      "batch 1011: loss 0.001941\n",
      "batch 1012: loss 0.017978\n",
      "batch 1013: loss 0.005211\n",
      "batch 1014: loss 0.005942\n",
      "batch 1015: loss 0.017937\n",
      "batch 1016: loss 0.023045\n",
      "batch 1017: loss 0.003973\n",
      "batch 1018: loss 0.003536\n",
      "batch 1019: loss 0.009955\n",
      "batch 1020: loss 0.015078\n",
      "batch 1021: loss 0.025291\n",
      "batch 1022: loss 0.047830\n",
      "batch 1023: loss 0.087243\n",
      "batch 1024: loss 0.039010\n",
      "batch 1025: loss 0.013492\n",
      "batch 1026: loss 0.056650\n",
      "batch 1027: loss 0.070135\n",
      "batch 1028: loss 0.013068\n",
      "batch 1029: loss 0.041174\n",
      "batch 1030: loss 0.107323\n",
      "batch 1031: loss 0.005934\n",
      "batch 1032: loss 0.042066\n",
      "batch 1033: loss 0.095378\n",
      "batch 1034: loss 0.020063\n",
      "batch 1035: loss 0.086756\n",
      "batch 1036: loss 0.018115\n",
      "batch 1037: loss 0.035109\n",
      "batch 1038: loss 0.002343\n",
      "batch 1039: loss 0.079440\n",
      "batch 1040: loss 0.054466\n",
      "batch 1041: loss 0.003645\n",
      "batch 1042: loss 0.158389\n",
      "batch 1043: loss 0.004840\n",
      "batch 1044: loss 0.008613\n",
      "batch 1045: loss 0.010013\n",
      "batch 1046: loss 0.001233\n",
      "batch 1047: loss 0.003994\n",
      "batch 1048: loss 0.060132\n",
      "batch 1049: loss 0.011173\n",
      "batch 1050: loss 0.155683\n",
      "batch 1051: loss 0.134267\n",
      "batch 1052: loss 0.015871\n",
      "batch 1053: loss 0.079709\n",
      "batch 1054: loss 0.005645\n",
      "batch 1055: loss 0.002703\n",
      "batch 1056: loss 0.184794\n",
      "batch 1057: loss 0.009760\n",
      "batch 1058: loss 0.133122\n",
      "batch 1059: loss 0.040898\n",
      "batch 1060: loss 0.002569\n",
      "batch 1061: loss 0.087205\n",
      "batch 1062: loss 0.245064\n",
      "batch 1063: loss 0.052008\n",
      "batch 1064: loss 0.037604\n",
      "batch 1065: loss 0.032224\n",
      "batch 1066: loss 0.007439\n",
      "batch 1067: loss 0.010947\n",
      "batch 1068: loss 0.005723\n",
      "batch 1069: loss 0.111300\n",
      "batch 1070: loss 0.062251\n",
      "batch 1071: loss 0.019746\n",
      "batch 1072: loss 0.001321\n",
      "batch 1073: loss 0.105155\n",
      "batch 1074: loss 0.069788\n",
      "batch 1075: loss 0.013556\n",
      "batch 1076: loss 0.006694\n",
      "batch 1077: loss 0.068366\n",
      "batch 1078: loss 0.018761\n",
      "batch 1079: loss 0.039087\n",
      "batch 1080: loss 0.108463\n",
      "batch 1081: loss 0.023596\n",
      "batch 1082: loss 0.082475\n",
      "batch 1083: loss 0.099233\n",
      "batch 1084: loss 0.003116\n",
      "batch 1085: loss 0.014737\n",
      "batch 1086: loss 0.036352\n",
      "batch 1087: loss 0.004648\n",
      "batch 1088: loss 0.002702\n",
      "batch 1089: loss 0.087125\n",
      "batch 1090: loss 0.097084\n",
      "batch 1091: loss 0.035076\n",
      "batch 1092: loss 0.068883\n",
      "batch 1093: loss 0.011048\n",
      "batch 1094: loss 0.112217\n",
      "batch 1095: loss 0.003320\n",
      "batch 1096: loss 0.006920\n",
      "batch 1097: loss 0.053116\n",
      "batch 1098: loss 0.027184\n",
      "batch 1099: loss 0.032163\n",
      "batch 1100: loss 0.092524\n",
      "batch 1101: loss 0.011924\n",
      "batch 1102: loss 0.006416\n",
      "batch 1103: loss 0.021395\n",
      "batch 1104: loss 0.065489\n",
      "batch 1105: loss 0.049633\n",
      "batch 1106: loss 0.071532\n",
      "batch 1107: loss 0.010524\n",
      "batch 1108: loss 0.036250\n",
      "batch 1109: loss 0.006829\n",
      "batch 1110: loss 0.014020\n",
      "batch 1111: loss 0.016625\n",
      "batch 1112: loss 0.054180\n",
      "batch 1113: loss 0.019871\n",
      "batch 1114: loss 0.049961\n",
      "batch 1115: loss 0.005426\n",
      "batch 1116: loss 0.007518\n",
      "batch 1117: loss 0.002279\n",
      "batch 1118: loss 0.003256\n",
      "batch 1119: loss 0.004085\n",
      "batch 1120: loss 0.017687\n",
      "batch 1121: loss 0.069582\n",
      "batch 1122: loss 0.001525\n",
      "batch 1123: loss 0.002125\n",
      "batch 1124: loss 0.060848\n",
      "batch 1125: loss 0.013760\n",
      "batch 1126: loss 0.040927\n",
      "batch 1127: loss 0.023624\n",
      "batch 1128: loss 0.147336\n",
      "batch 1129: loss 0.019452\n",
      "batch 1130: loss 0.004154\n",
      "batch 1131: loss 0.056114\n",
      "batch 1132: loss 0.003760\n",
      "batch 1133: loss 0.013755\n",
      "batch 1134: loss 0.004207\n",
      "batch 1135: loss 0.065042\n",
      "batch 1136: loss 0.003943\n",
      "batch 1137: loss 0.011054\n",
      "batch 1138: loss 0.066367\n",
      "batch 1139: loss 0.002146\n",
      "batch 1140: loss 0.009739\n",
      "batch 1141: loss 0.014070\n",
      "batch 1142: loss 0.087441\n",
      "batch 1143: loss 0.038678\n",
      "batch 1144: loss 0.023325\n",
      "batch 1145: loss 0.005457\n",
      "batch 1146: loss 0.002161\n",
      "batch 1147: loss 0.014338\n",
      "batch 1148: loss 0.010436\n",
      "batch 1149: loss 0.007679\n",
      "batch 1150: loss 0.026506\n",
      "batch 1151: loss 0.092211\n",
      "batch 1152: loss 0.002336\n",
      "batch 1153: loss 0.054114\n",
      "batch 1154: loss 0.001217\n",
      "batch 1155: loss 0.020054\n",
      "batch 1156: loss 0.041331\n",
      "batch 1157: loss 0.109315\n",
      "batch 1158: loss 0.095154\n",
      "batch 1159: loss 0.001764\n",
      "batch 1160: loss 0.009513\n",
      "batch 1161: loss 0.014776\n",
      "batch 1162: loss 0.000590\n",
      "batch 1163: loss 0.061160\n",
      "batch 1164: loss 0.001515\n",
      "batch 1165: loss 0.038851\n",
      "batch 1166: loss 0.004002\n",
      "batch 1167: loss 0.018032\n",
      "batch 1168: loss 0.001295\n",
      "batch 1169: loss 0.054317\n",
      "batch 1170: loss 0.040651\n",
      "batch 1171: loss 0.081936\n",
      "batch 1172: loss 0.179468\n",
      "batch 1173: loss 0.008424\n",
      "batch 1174: loss 0.010245\n",
      "batch 1175: loss 0.002756\n",
      "batch 1176: loss 0.049309\n",
      "batch 1177: loss 0.007627\n",
      "batch 1178: loss 0.056014\n",
      "batch 1179: loss 0.005846\n",
      "batch 1180: loss 0.017578\n",
      "batch 1181: loss 0.002905\n",
      "batch 1182: loss 0.031046\n",
      "batch 1183: loss 0.018206\n",
      "batch 1184: loss 0.005142\n",
      "batch 1185: loss 0.011343\n",
      "batch 1186: loss 0.004700\n",
      "batch 1187: loss 0.019810\n",
      "batch 1188: loss 0.009498\n",
      "batch 1189: loss 0.003330\n",
      "batch 1190: loss 0.006381\n",
      "batch 1191: loss 0.137765\n",
      "batch 1192: loss 0.009567\n",
      "batch 1193: loss 0.013736\n",
      "batch 1194: loss 0.011330\n",
      "batch 1195: loss 0.006920\n",
      "batch 1196: loss 0.004529\n",
      "batch 1197: loss 0.006223\n",
      "batch 1198: loss 0.008263\n",
      "batch 1199: loss 0.323755\n",
      "batch 1200: loss 0.001825\n",
      "batch 1201: loss 0.035958\n",
      "batch 1202: loss 0.125573\n",
      "batch 1203: loss 0.112546\n",
      "batch 1204: loss 0.018880\n",
      "batch 1205: loss 0.028843\n",
      "batch 1206: loss 0.026865\n",
      "batch 1207: loss 0.024884\n",
      "batch 1208: loss 0.025411\n",
      "batch 1209: loss 0.033775\n",
      "batch 1210: loss 0.034788\n",
      "batch 1211: loss 0.004118\n",
      "batch 1212: loss 0.021072\n",
      "batch 1213: loss 0.002934\n",
      "batch 1214: loss 0.002047\n",
      "batch 1215: loss 0.055637\n",
      "batch 1216: loss 0.005113\n",
      "batch 1217: loss 0.005307\n",
      "batch 1218: loss 0.019425\n",
      "batch 1219: loss 0.032942\n",
      "batch 1220: loss 0.049066\n",
      "batch 1221: loss 0.007102\n",
      "batch 1222: loss 0.055983\n",
      "batch 1223: loss 0.009881\n",
      "batch 1224: loss 0.006711\n",
      "batch 1225: loss 0.000780\n",
      "batch 1226: loss 0.044396\n",
      "batch 1227: loss 0.094597\n",
      "batch 1228: loss 0.045660\n",
      "batch 1229: loss 0.049607\n",
      "batch 1230: loss 0.014499\n",
      "batch 1231: loss 0.073725\n",
      "batch 1232: loss 0.026393\n",
      "batch 1233: loss 0.063103\n",
      "batch 1234: loss 0.010235\n",
      "batch 1235: loss 0.095260\n",
      "batch 1236: loss 0.009332\n",
      "batch 1237: loss 0.082658\n",
      "batch 1238: loss 0.020661\n",
      "batch 1239: loss 0.029942\n",
      "batch 1240: loss 0.145812\n",
      "batch 1241: loss 0.098164\n",
      "batch 1242: loss 0.000494\n",
      "batch 1243: loss 0.036885\n",
      "batch 1244: loss 0.094380\n",
      "batch 1245: loss 0.009492\n",
      "batch 1246: loss 0.130026\n",
      "batch 1247: loss 0.066527\n",
      "batch 1248: loss 0.007848\n",
      "batch 1249: loss 0.098020\n",
      "batch 1250: loss 0.072289\n",
      "batch 1251: loss 0.074247\n",
      "batch 1252: loss 0.026470\n",
      "batch 1253: loss 0.082022\n",
      "batch 1254: loss 0.003882\n",
      "batch 1255: loss 0.044763\n",
      "batch 1256: loss 0.083139\n",
      "batch 1257: loss 0.013653\n",
      "batch 1258: loss 0.002491\n",
      "batch 1259: loss 0.012527\n",
      "batch 1260: loss 0.006413\n",
      "batch 1261: loss 0.008334\n",
      "batch 1262: loss 0.007063\n",
      "batch 1263: loss 0.042509\n",
      "batch 1264: loss 0.019116\n",
      "batch 1265: loss 0.016618\n",
      "batch 1266: loss 0.003368\n",
      "batch 1267: loss 0.008902\n",
      "batch 1268: loss 0.018166\n",
      "batch 1269: loss 0.000649\n",
      "batch 1270: loss 0.014532\n",
      "batch 1271: loss 0.063439\n",
      "batch 1272: loss 0.006562\n",
      "batch 1273: loss 0.004751\n",
      "batch 1274: loss 0.022610\n",
      "batch 1275: loss 0.030460\n",
      "batch 1276: loss 0.005354\n",
      "batch 1277: loss 0.011989\n",
      "batch 1278: loss 0.116637\n",
      "batch 1279: loss 0.010513\n",
      "batch 1280: loss 0.000366\n",
      "batch 1281: loss 0.002647\n",
      "batch 1282: loss 0.051982\n",
      "batch 1283: loss 0.060530\n",
      "batch 1284: loss 0.024410\n",
      "batch 1285: loss 0.021186\n",
      "batch 1286: loss 0.012854\n",
      "batch 1287: loss 0.057380\n",
      "batch 1288: loss 0.004239\n",
      "batch 1289: loss 0.043984\n",
      "batch 1290: loss 0.022720\n",
      "batch 1291: loss 0.021399\n",
      "batch 1292: loss 0.241431\n",
      "batch 1293: loss 0.007861\n",
      "batch 1294: loss 0.023399\n",
      "batch 1295: loss 0.002308\n",
      "batch 1296: loss 0.004961\n",
      "batch 1297: loss 0.091500\n",
      "batch 1298: loss 0.068601\n",
      "batch 1299: loss 0.002818\n",
      "batch 1300: loss 0.156779\n",
      "batch 1301: loss 0.103389\n",
      "batch 1302: loss 0.005961\n",
      "batch 1303: loss 0.069092\n",
      "batch 1304: loss 0.033853\n",
      "batch 1305: loss 0.024988\n",
      "batch 1306: loss 0.003918\n",
      "batch 1307: loss 0.007699\n",
      "batch 1308: loss 0.008896\n",
      "batch 1309: loss 0.001503\n",
      "batch 1310: loss 0.012167\n",
      "batch 1311: loss 0.131072\n",
      "batch 1312: loss 0.056199\n",
      "batch 1313: loss 0.010735\n",
      "batch 1314: loss 0.008836\n",
      "batch 1315: loss 0.010500\n",
      "batch 1316: loss 0.004558\n",
      "batch 1317: loss 0.034123\n",
      "batch 1318: loss 0.033024\n",
      "batch 1319: loss 0.000572\n",
      "batch 1320: loss 0.079647\n",
      "batch 1321: loss 0.019455\n",
      "batch 1322: loss 0.016696\n",
      "batch 1323: loss 0.023843\n",
      "batch 1324: loss 0.002165\n",
      "batch 1325: loss 0.005915\n",
      "batch 1326: loss 0.039193\n",
      "batch 1327: loss 0.007211\n",
      "batch 1328: loss 0.025518\n",
      "batch 1329: loss 0.004533\n",
      "batch 1330: loss 0.185816\n",
      "batch 1331: loss 0.051136\n",
      "batch 1332: loss 0.001957\n",
      "batch 1333: loss 0.006941\n",
      "batch 1334: loss 0.020708\n",
      "batch 1335: loss 0.004765\n",
      "batch 1336: loss 0.009231\n",
      "batch 1337: loss 0.048892\n",
      "batch 1338: loss 0.073798\n",
      "batch 1339: loss 0.004833\n",
      "batch 1340: loss 0.011089\n",
      "batch 1341: loss 0.022969\n",
      "batch 1342: loss 0.018494\n",
      "batch 1343: loss 0.014172\n",
      "batch 1344: loss 0.006112\n",
      "batch 1345: loss 0.003725\n",
      "batch 1346: loss 0.004131\n",
      "batch 1347: loss 0.007782\n",
      "batch 1348: loss 0.008082\n",
      "batch 1349: loss 0.013547\n",
      "batch 1350: loss 0.158415\n",
      "batch 1351: loss 0.014223\n",
      "batch 1352: loss 0.014425\n",
      "batch 1353: loss 0.019136\n",
      "batch 1354: loss 0.023496\n",
      "batch 1355: loss 0.014639\n",
      "batch 1356: loss 0.190137\n",
      "batch 1357: loss 0.020513\n",
      "batch 1358: loss 0.001856\n",
      "batch 1359: loss 0.004629\n",
      "batch 1360: loss 0.085753\n",
      "batch 1361: loss 0.029474\n",
      "batch 1362: loss 0.032168\n",
      "batch 1363: loss 0.031215\n",
      "batch 1364: loss 0.013436\n",
      "batch 1365: loss 0.068284\n",
      "batch 1366: loss 0.007898\n",
      "batch 1367: loss 0.057811\n",
      "batch 1368: loss 0.000629\n",
      "batch 1369: loss 0.026086\n",
      "batch 1370: loss 0.044386\n",
      "batch 1371: loss 0.042403\n",
      "batch 1372: loss 0.023718\n",
      "batch 1373: loss 0.012996\n",
      "batch 1374: loss 0.030310\n",
      "batch 1375: loss 0.008933\n",
      "batch 1376: loss 0.252707\n",
      "batch 1377: loss 0.024164\n",
      "batch 1378: loss 0.029132\n",
      "batch 1379: loss 0.002574\n",
      "batch 1380: loss 0.028372\n",
      "batch 1381: loss 0.076026\n",
      "batch 1382: loss 0.103841\n",
      "batch 1383: loss 0.049081\n",
      "batch 1384: loss 0.200585\n",
      "batch 1385: loss 0.017381\n",
      "batch 1386: loss 0.026755\n",
      "batch 1387: loss 0.090586\n",
      "batch 1388: loss 0.012301\n",
      "batch 1389: loss 0.067753\n",
      "batch 1390: loss 0.006581\n",
      "batch 1391: loss 0.307747\n",
      "batch 1392: loss 0.005436\n",
      "batch 1393: loss 0.065057\n",
      "batch 1394: loss 0.025363\n",
      "batch 1395: loss 0.003186\n",
      "batch 1396: loss 0.045766\n",
      "batch 1397: loss 0.088620\n",
      "batch 1398: loss 0.045360\n",
      "batch 1399: loss 0.033598\n",
      "batch 1400: loss 0.013981\n",
      "batch 1401: loss 0.079516\n",
      "batch 1402: loss 0.003401\n",
      "batch 1403: loss 0.003621\n",
      "batch 1404: loss 0.054620\n",
      "batch 1405: loss 0.016121\n",
      "batch 1406: loss 0.027980\n",
      "batch 1407: loss 0.046355\n",
      "batch 1408: loss 0.019166\n",
      "batch 1409: loss 0.036408\n",
      "batch 1410: loss 0.191379\n",
      "batch 1411: loss 0.066925\n",
      "batch 1412: loss 0.128079\n",
      "batch 1413: loss 0.008382\n",
      "batch 1414: loss 0.003582\n",
      "batch 1415: loss 0.004675\n",
      "batch 1416: loss 0.096276\n",
      "batch 1417: loss 0.061334\n",
      "batch 1418: loss 0.057008\n",
      "batch 1419: loss 0.016146\n",
      "batch 1420: loss 0.015973\n",
      "batch 1421: loss 0.017527\n",
      "batch 1422: loss 0.141759\n",
      "batch 1423: loss 0.110147\n",
      "batch 1424: loss 0.004987\n",
      "batch 1425: loss 0.066363\n",
      "batch 1426: loss 0.010874\n",
      "batch 1427: loss 0.061620\n",
      "batch 1428: loss 0.006514\n",
      "batch 1429: loss 0.017123\n",
      "batch 1430: loss 0.066040\n",
      "batch 1431: loss 0.013725\n",
      "batch 1432: loss 0.001363\n",
      "batch 1433: loss 0.044643\n",
      "batch 1434: loss 0.005109\n",
      "batch 1435: loss 0.019000\n",
      "batch 1436: loss 0.019176\n",
      "batch 1437: loss 0.019313\n",
      "batch 1438: loss 0.002955\n",
      "batch 1439: loss 0.010487\n",
      "batch 1440: loss 0.010318\n",
      "batch 1441: loss 0.002878\n",
      "batch 1442: loss 0.066059\n",
      "batch 1443: loss 0.099449\n",
      "batch 1444: loss 0.006004\n",
      "batch 1445: loss 0.122083\n",
      "batch 1446: loss 0.023656\n",
      "batch 1447: loss 0.005036\n",
      "batch 1448: loss 0.001690\n",
      "batch 1449: loss 0.122120\n",
      "batch 1450: loss 0.003118\n",
      "batch 1451: loss 0.034779\n",
      "batch 1452: loss 0.058240\n",
      "batch 1453: loss 0.066100\n",
      "batch 1454: loss 0.025677\n",
      "batch 1455: loss 0.015837\n",
      "batch 1456: loss 0.069308\n",
      "batch 1457: loss 0.077399\n",
      "batch 1458: loss 0.000815\n",
      "batch 1459: loss 0.002450\n",
      "batch 1460: loss 0.002597\n",
      "batch 1461: loss 0.014609\n",
      "batch 1462: loss 0.011622\n",
      "batch 1463: loss 0.003611\n",
      "batch 1464: loss 0.002561\n",
      "batch 1465: loss 0.009265\n",
      "batch 1466: loss 0.015320\n",
      "batch 1467: loss 0.014982\n",
      "batch 1468: loss 0.029797\n",
      "batch 1469: loss 0.004462\n",
      "batch 1470: loss 0.000737\n",
      "batch 1471: loss 0.002372\n",
      "batch 1472: loss 0.026076\n",
      "batch 1473: loss 0.006658\n",
      "batch 1474: loss 0.018901\n",
      "batch 1475: loss 0.095785\n",
      "batch 1476: loss 0.198944\n",
      "batch 1477: loss 0.017963\n",
      "batch 1478: loss 0.158225\n",
      "batch 1479: loss 0.082021\n",
      "batch 1480: loss 0.040056\n",
      "batch 1481: loss 0.014464\n",
      "batch 1482: loss 0.019138\n",
      "batch 1483: loss 0.008673\n",
      "batch 1484: loss 0.023319\n",
      "batch 1485: loss 0.001804\n",
      "batch 1486: loss 0.004032\n",
      "batch 1487: loss 0.008294\n",
      "batch 1488: loss 0.004513\n",
      "batch 1489: loss 0.013445\n",
      "batch 1490: loss 0.004124\n",
      "batch 1491: loss 0.006830\n",
      "batch 1492: loss 0.073433\n",
      "batch 1493: loss 0.006496\n",
      "batch 1494: loss 0.044140\n",
      "batch 1495: loss 0.043687\n",
      "batch 1496: loss 0.021136\n",
      "batch 1497: loss 0.041049\n",
      "batch 1498: loss 0.013144\n",
      "batch 1499: loss 0.035125\n",
      "batch 1500: loss 0.005111\n",
      "batch 1501: loss 0.019015\n",
      "batch 1502: loss 0.002106\n",
      "batch 1503: loss 0.043810\n",
      "batch 1504: loss 0.013380\n",
      "batch 1505: loss 0.045025\n",
      "batch 1506: loss 0.001725\n",
      "batch 1507: loss 0.004961\n",
      "batch 1508: loss 0.016980\n",
      "batch 1509: loss 0.074911\n",
      "batch 1510: loss 0.029634\n",
      "batch 1511: loss 0.008929\n",
      "batch 1512: loss 0.008986\n",
      "batch 1513: loss 0.016160\n",
      "batch 1514: loss 0.000655\n",
      "batch 1515: loss 0.009292\n",
      "batch 1516: loss 0.019962\n",
      "batch 1517: loss 0.066183\n",
      "batch 1518: loss 0.010365\n",
      "batch 1519: loss 0.007245\n",
      "batch 1520: loss 0.001071\n",
      "batch 1521: loss 0.002373\n",
      "batch 1522: loss 0.013430\n",
      "batch 1523: loss 0.024495\n",
      "batch 1524: loss 0.001098\n",
      "batch 1525: loss 0.055907\n",
      "batch 1526: loss 0.000207\n",
      "batch 1527: loss 0.105798\n",
      "batch 1528: loss 0.045153\n",
      "batch 1529: loss 0.014008\n",
      "batch 1530: loss 0.000448\n",
      "batch 1531: loss 0.060678\n",
      "batch 1532: loss 0.093833\n",
      "batch 1533: loss 0.000959\n",
      "batch 1534: loss 0.008535\n",
      "batch 1535: loss 0.117764\n",
      "batch 1536: loss 0.013844\n",
      "batch 1537: loss 0.036094\n",
      "batch 1538: loss 0.001541\n",
      "batch 1539: loss 0.005413\n",
      "batch 1540: loss 0.106961\n",
      "batch 1541: loss 0.026699\n",
      "batch 1542: loss 0.047380\n",
      "batch 1543: loss 0.059966\n",
      "batch 1544: loss 0.015006\n",
      "batch 1545: loss 0.011422\n",
      "batch 1546: loss 0.006442\n",
      "batch 1547: loss 0.002206\n",
      "batch 1548: loss 0.009403\n",
      "batch 1549: loss 0.021972\n",
      "batch 1550: loss 0.098667\n",
      "batch 1551: loss 0.000244\n",
      "batch 1552: loss 0.001749\n",
      "batch 1553: loss 0.008699\n",
      "batch 1554: loss 0.001918\n",
      "batch 1555: loss 0.000586\n",
      "batch 1556: loss 0.264086\n",
      "batch 1557: loss 0.000859\n",
      "batch 1558: loss 0.070375\n",
      "batch 1559: loss 0.156867\n",
      "batch 1560: loss 0.040762\n",
      "batch 1561: loss 0.001568\n",
      "batch 1562: loss 0.017972\n",
      "batch 1563: loss 0.023720\n",
      "batch 1564: loss 0.001596\n",
      "batch 1565: loss 0.084871\n",
      "batch 1566: loss 0.074690\n",
      "batch 1567: loss 0.156797\n",
      "batch 1568: loss 0.040726\n",
      "batch 1569: loss 0.020143\n",
      "batch 1570: loss 0.069566\n",
      "batch 1571: loss 0.013383\n",
      "batch 1572: loss 0.053637\n",
      "batch 1573: loss 0.101268\n",
      "batch 1574: loss 0.011816\n",
      "batch 1575: loss 0.016534\n",
      "batch 1576: loss 0.026982\n",
      "batch 1577: loss 0.007167\n",
      "batch 1578: loss 0.003262\n",
      "batch 1579: loss 0.016025\n",
      "batch 1580: loss 0.093617\n",
      "batch 1581: loss 0.065266\n",
      "batch 1582: loss 0.012022\n",
      "batch 1583: loss 0.034581\n",
      "batch 1584: loss 0.003915\n",
      "batch 1585: loss 0.003127\n",
      "batch 1586: loss 0.114408\n",
      "batch 1587: loss 0.026325\n",
      "batch 1588: loss 0.019332\n",
      "batch 1589: loss 0.021439\n",
      "batch 1590: loss 0.006752\n",
      "batch 1591: loss 0.027557\n",
      "batch 1592: loss 0.100092\n",
      "batch 1593: loss 0.058226\n",
      "batch 1594: loss 0.017551\n",
      "batch 1595: loss 0.000895\n",
      "batch 1596: loss 0.000595\n",
      "batch 1597: loss 0.036095\n",
      "batch 1598: loss 0.000812\n",
      "batch 1599: loss 0.152678\n",
      "batch 1600: loss 0.000445\n",
      "batch 1601: loss 0.017018\n",
      "batch 1602: loss 0.022979\n",
      "batch 1603: loss 0.014050\n",
      "batch 1604: loss 0.005477\n",
      "batch 1605: loss 0.003480\n",
      "batch 1606: loss 0.005122\n",
      "batch 1607: loss 0.118163\n",
      "batch 1608: loss 0.082090\n",
      "batch 1609: loss 0.003869\n",
      "batch 1610: loss 0.001813\n",
      "batch 1611: loss 0.021501\n",
      "batch 1612: loss 0.053385\n",
      "batch 1613: loss 0.002254\n",
      "batch 1614: loss 0.006526\n",
      "batch 1615: loss 0.011673\n",
      "batch 1616: loss 0.011716\n",
      "batch 1617: loss 0.000716\n",
      "batch 1618: loss 0.017566\n",
      "batch 1619: loss 0.017216\n",
      "batch 1620: loss 0.002701\n",
      "batch 1621: loss 0.007406\n",
      "batch 1622: loss 0.057369\n",
      "batch 1623: loss 0.091700\n",
      "batch 1624: loss 0.002506\n",
      "batch 1625: loss 0.041148\n",
      "batch 1626: loss 0.000287\n",
      "batch 1627: loss 0.017482\n",
      "batch 1628: loss 0.230540\n",
      "batch 1629: loss 0.094097\n",
      "batch 1630: loss 0.037224\n",
      "batch 1631: loss 0.003071\n",
      "batch 1632: loss 0.047086\n",
      "batch 1633: loss 0.002467\n",
      "batch 1634: loss 0.088180\n",
      "batch 1635: loss 0.012603\n",
      "batch 1636: loss 0.010585\n",
      "batch 1637: loss 0.147113\n",
      "batch 1638: loss 0.002610\n",
      "batch 1639: loss 0.005316\n",
      "batch 1640: loss 0.037928\n",
      "batch 1641: loss 0.013239\n",
      "batch 1642: loss 0.096496\n",
      "batch 1643: loss 0.080757\n",
      "batch 1644: loss 0.057672\n",
      "batch 1645: loss 0.005710\n",
      "batch 1646: loss 0.013931\n",
      "batch 1647: loss 0.007805\n",
      "batch 1648: loss 0.027026\n",
      "batch 1649: loss 0.018148\n",
      "batch 1650: loss 0.003019\n",
      "batch 1651: loss 0.008887\n",
      "batch 1652: loss 0.073918\n",
      "batch 1653: loss 0.002856\n",
      "batch 1654: loss 0.002623\n",
      "batch 1655: loss 0.036437\n",
      "batch 1656: loss 0.009251\n",
      "batch 1657: loss 0.006652\n",
      "batch 1658: loss 0.002527\n",
      "batch 1659: loss 0.005490\n",
      "batch 1660: loss 0.003234\n",
      "batch 1661: loss 0.005372\n",
      "batch 1662: loss 0.002904\n",
      "batch 1663: loss 0.135909\n",
      "batch 1664: loss 0.016886\n",
      "batch 1665: loss 0.039504\n",
      "batch 1666: loss 0.011754\n",
      "batch 1667: loss 0.003762\n",
      "batch 1668: loss 0.005451\n",
      "batch 1669: loss 0.035203\n",
      "batch 1670: loss 0.007987\n",
      "batch 1671: loss 0.001345\n",
      "batch 1672: loss 0.034502\n",
      "batch 1673: loss 0.144432\n",
      "batch 1674: loss 0.009780\n",
      "batch 1675: loss 0.002332\n",
      "batch 1676: loss 0.076039\n",
      "batch 1677: loss 0.007135\n",
      "batch 1678: loss 0.001677\n",
      "batch 1679: loss 0.047141\n",
      "batch 1680: loss 0.006981\n",
      "batch 1681: loss 0.063298\n",
      "batch 1682: loss 0.000292\n",
      "batch 1683: loss 0.105243\n",
      "batch 1684: loss 0.072475\n",
      "batch 1685: loss 0.001563\n",
      "batch 1686: loss 0.004493\n",
      "batch 1687: loss 0.008876\n",
      "batch 1688: loss 0.008863\n",
      "batch 1689: loss 0.156259\n",
      "batch 1690: loss 0.003090\n",
      "batch 1691: loss 0.045849\n",
      "batch 1692: loss 0.006096\n",
      "batch 1693: loss 0.009265\n",
      "batch 1694: loss 0.003170\n",
      "batch 1695: loss 0.175546\n",
      "batch 1696: loss 0.004317\n",
      "batch 1697: loss 0.007420\n",
      "batch 1698: loss 0.002559\n",
      "batch 1699: loss 0.030851\n",
      "batch 1700: loss 0.040058\n",
      "batch 1701: loss 0.009530\n",
      "batch 1702: loss 0.007925\n",
      "batch 1703: loss 0.005861\n",
      "batch 1704: loss 0.010397\n",
      "batch 1705: loss 0.062963\n",
      "batch 1706: loss 0.008821\n",
      "batch 1707: loss 0.051920\n",
      "batch 1708: loss 0.067119\n",
      "batch 1709: loss 0.022288\n",
      "batch 1710: loss 0.001833\n",
      "batch 1711: loss 0.008305\n",
      "batch 1712: loss 0.025230\n",
      "batch 1713: loss 0.006937\n",
      "batch 1714: loss 0.026279\n",
      "batch 1715: loss 0.023897\n",
      "batch 1716: loss 0.031812\n",
      "batch 1717: loss 0.079399\n",
      "batch 1718: loss 0.007595\n",
      "batch 1719: loss 0.023251\n",
      "batch 1720: loss 0.023699\n",
      "batch 1721: loss 0.030245\n",
      "batch 1722: loss 0.081563\n",
      "batch 1723: loss 0.002657\n",
      "batch 1724: loss 0.106071\n",
      "batch 1725: loss 0.022365\n",
      "batch 1726: loss 0.029898\n",
      "batch 1727: loss 0.017177\n",
      "batch 1728: loss 0.002728\n",
      "batch 1729: loss 0.158503\n",
      "batch 1730: loss 0.017201\n",
      "batch 1731: loss 0.152644\n",
      "batch 1732: loss 0.029162\n",
      "batch 1733: loss 0.011844\n",
      "batch 1734: loss 0.003322\n",
      "batch 1735: loss 0.035398\n",
      "batch 1736: loss 0.002371\n",
      "batch 1737: loss 0.012837\n",
      "batch 1738: loss 0.001205\n",
      "batch 1739: loss 0.025397\n",
      "batch 1740: loss 0.003202\n",
      "batch 1741: loss 0.001391\n",
      "batch 1742: loss 0.002384\n",
      "batch 1743: loss 0.014056\n",
      "batch 1744: loss 0.004400\n",
      "batch 1745: loss 0.000483\n",
      "batch 1746: loss 0.019954\n",
      "batch 1747: loss 0.030926\n",
      "batch 1748: loss 0.027579\n",
      "batch 1749: loss 0.086859\n",
      "batch 1750: loss 0.009345\n",
      "batch 1751: loss 0.003155\n",
      "batch 1752: loss 0.021892\n",
      "batch 1753: loss 0.004211\n",
      "batch 1754: loss 0.037126\n",
      "batch 1755: loss 0.030968\n",
      "batch 1756: loss 0.001732\n",
      "batch 1757: loss 0.013485\n",
      "batch 1758: loss 0.167733\n",
      "batch 1759: loss 0.069690\n",
      "batch 1760: loss 0.003760\n",
      "batch 1761: loss 0.007444\n",
      "batch 1762: loss 0.091894\n",
      "batch 1763: loss 0.007229\n",
      "batch 1764: loss 0.087294\n",
      "batch 1765: loss 0.008578\n",
      "batch 1766: loss 0.046980\n",
      "batch 1767: loss 0.010538\n",
      "batch 1768: loss 0.101010\n",
      "batch 1769: loss 0.029337\n",
      "batch 1770: loss 0.007416\n",
      "batch 1771: loss 0.055780\n",
      "batch 1772: loss 0.001126\n",
      "batch 1773: loss 0.153378\n",
      "batch 1774: loss 0.069978\n",
      "batch 1775: loss 0.009663\n",
      "batch 1776: loss 0.026701\n",
      "batch 1777: loss 0.062674\n",
      "batch 1778: loss 0.112594\n",
      "batch 1779: loss 0.004408\n",
      "batch 1780: loss 0.020172\n",
      "batch 1781: loss 0.115618\n",
      "batch 1782: loss 0.043272\n",
      "batch 1783: loss 0.055004\n",
      "batch 1784: loss 0.060301\n",
      "batch 1785: loss 0.086090\n",
      "batch 1786: loss 0.002807\n",
      "batch 1787: loss 0.043187\n",
      "batch 1788: loss 0.010425\n",
      "batch 1789: loss 0.001171\n",
      "batch 1790: loss 0.038374\n",
      "batch 1791: loss 0.006283\n",
      "batch 1792: loss 0.017991\n",
      "batch 1793: loss 0.031239\n",
      "batch 1794: loss 0.021679\n",
      "batch 1795: loss 0.004021\n",
      "batch 1796: loss 0.034377\n",
      "batch 1797: loss 0.025915\n",
      "batch 1798: loss 0.018819\n",
      "batch 1799: loss 0.001896\n",
      "batch 1800: loss 0.112892\n",
      "batch 1801: loss 0.115720\n",
      "batch 1802: loss 0.134664\n",
      "batch 1803: loss 0.016519\n",
      "batch 1804: loss 0.029523\n",
      "batch 1805: loss 0.018542\n",
      "batch 1806: loss 0.028650\n",
      "batch 1807: loss 0.016446\n",
      "batch 1808: loss 0.003461\n",
      "batch 1809: loss 0.072960\n",
      "batch 1810: loss 0.037743\n",
      "batch 1811: loss 0.007883\n",
      "batch 1812: loss 0.007104\n",
      "batch 1813: loss 0.010118\n",
      "batch 1814: loss 0.002980\n",
      "batch 1815: loss 0.063170\n",
      "batch 1816: loss 0.002418\n",
      "batch 1817: loss 0.018146\n",
      "batch 1818: loss 0.010984\n",
      "batch 1819: loss 0.017375\n",
      "batch 1820: loss 0.006970\n",
      "batch 1821: loss 0.008318\n",
      "batch 1822: loss 0.003656\n",
      "batch 1823: loss 0.011639\n",
      "batch 1824: loss 0.118160\n",
      "batch 1825: loss 0.002512\n",
      "batch 1826: loss 0.003699\n",
      "batch 1827: loss 0.046036\n",
      "batch 1828: loss 0.005174\n",
      "batch 1829: loss 0.002614\n",
      "batch 1830: loss 0.005942\n",
      "batch 1831: loss 0.042385\n",
      "batch 1832: loss 0.093191\n",
      "batch 1833: loss 0.008127\n",
      "batch 1834: loss 0.016170\n",
      "batch 1835: loss 0.004099\n",
      "batch 1836: loss 0.049173\n",
      "batch 1837: loss 0.020006\n",
      "batch 1838: loss 0.047215\n",
      "batch 1839: loss 0.089660\n",
      "batch 1840: loss 0.007170\n",
      "batch 1841: loss 0.038466\n",
      "batch 1842: loss 0.033037\n",
      "batch 1843: loss 0.008006\n",
      "batch 1844: loss 0.004595\n",
      "batch 1845: loss 0.001064\n",
      "batch 1846: loss 0.123019\n",
      "batch 1847: loss 0.040728\n",
      "batch 1848: loss 0.003300\n",
      "batch 1849: loss 0.004209\n",
      "batch 1850: loss 0.036304\n",
      "batch 1851: loss 0.049156\n",
      "batch 1852: loss 0.004699\n",
      "batch 1853: loss 0.008274\n",
      "batch 1854: loss 0.008767\n",
      "batch 1855: loss 0.062458\n",
      "batch 1856: loss 0.023205\n",
      "batch 1857: loss 0.005217\n",
      "batch 1858: loss 0.013353\n",
      "batch 1859: loss 0.008146\n",
      "batch 1860: loss 0.003147\n",
      "batch 1861: loss 0.019537\n",
      "batch 1862: loss 0.005152\n",
      "batch 1863: loss 0.003023\n",
      "batch 1864: loss 0.006017\n",
      "batch 1865: loss 0.003629\n",
      "batch 1866: loss 0.002173\n",
      "batch 1867: loss 0.000808\n",
      "batch 1868: loss 0.042156\n",
      "batch 1869: loss 0.050314\n",
      "batch 1870: loss 0.010184\n",
      "batch 1871: loss 0.035052\n",
      "batch 1872: loss 0.001953\n",
      "batch 1873: loss 0.118313\n",
      "batch 1874: loss 0.025635\n",
      "batch 1875: loss 0.020296\n",
      "batch 1876: loss 0.092358\n",
      "batch 1877: loss 0.018441\n",
      "batch 1878: loss 0.016899\n",
      "batch 1879: loss 0.029111\n",
      "batch 1880: loss 0.010063\n",
      "batch 1881: loss 0.023489\n",
      "batch 1882: loss 0.009098\n",
      "batch 1883: loss 0.003732\n",
      "batch 1884: loss 0.000755\n",
      "batch 1885: loss 0.021024\n",
      "batch 1886: loss 0.007107\n",
      "batch 1887: loss 0.014084\n",
      "batch 1888: loss 0.019472\n",
      "batch 1889: loss 0.018319\n",
      "batch 1890: loss 0.003291\n",
      "batch 1891: loss 0.070714\n",
      "batch 1892: loss 0.005736\n",
      "batch 1893: loss 0.010112\n",
      "batch 1894: loss 0.004186\n",
      "batch 1895: loss 0.091381\n",
      "batch 1896: loss 0.002580\n",
      "batch 1897: loss 0.000869\n",
      "batch 1898: loss 0.258800\n",
      "batch 1899: loss 0.029769\n",
      "batch 1900: loss 0.029704\n",
      "batch 1901: loss 0.216725\n",
      "batch 1902: loss 0.041448\n",
      "batch 1903: loss 0.004839\n",
      "batch 1904: loss 0.001108\n",
      "batch 1905: loss 0.006677\n",
      "batch 1906: loss 0.166116\n",
      "batch 1907: loss 0.052394\n",
      "batch 1908: loss 0.106520\n",
      "batch 1909: loss 0.001206\n",
      "batch 1910: loss 0.008981\n",
      "batch 1911: loss 0.046447\n",
      "batch 1912: loss 0.010832\n",
      "batch 1913: loss 0.036334\n",
      "batch 1914: loss 0.007518\n",
      "batch 1915: loss 0.002424\n",
      "batch 1916: loss 0.055290\n",
      "batch 1917: loss 0.034311\n",
      "batch 1918: loss 0.044364\n",
      "batch 1919: loss 0.005885\n",
      "batch 1920: loss 0.028305\n",
      "batch 1921: loss 0.010187\n",
      "batch 1922: loss 0.033297\n",
      "batch 1923: loss 0.008769\n",
      "batch 1924: loss 0.008437\n",
      "batch 1925: loss 0.014920\n",
      "batch 1926: loss 0.065071\n",
      "batch 1927: loss 0.012730\n",
      "batch 1928: loss 0.005398\n",
      "batch 1929: loss 0.015227\n",
      "batch 1930: loss 0.004443\n",
      "batch 1931: loss 0.003565\n",
      "batch 1932: loss 0.022324\n",
      "batch 1933: loss 0.062255\n",
      "batch 1934: loss 0.001385\n",
      "batch 1935: loss 0.024351\n",
      "batch 1936: loss 0.003004\n",
      "batch 1937: loss 0.001611\n",
      "batch 1938: loss 0.001378\n",
      "batch 1939: loss 0.002432\n",
      "batch 1940: loss 0.000452\n",
      "batch 1941: loss 0.036488\n",
      "batch 1942: loss 0.020927\n",
      "batch 1943: loss 0.019301\n",
      "batch 1944: loss 0.034235\n",
      "batch 1945: loss 0.059899\n",
      "batch 1946: loss 0.001516\n",
      "batch 1947: loss 0.011387\n",
      "batch 1948: loss 0.007758\n",
      "batch 1949: loss 0.003537\n",
      "batch 1950: loss 0.038120\n",
      "batch 1951: loss 0.000334\n",
      "batch 1952: loss 0.015205\n",
      "batch 1953: loss 0.039224\n",
      "batch 1954: loss 0.002219\n",
      "batch 1955: loss 0.003565\n",
      "batch 1956: loss 0.043987\n",
      "batch 1957: loss 0.017264\n",
      "batch 1958: loss 0.011433\n",
      "batch 1959: loss 0.013536\n",
      "batch 1960: loss 0.134516\n",
      "batch 1961: loss 0.012500\n",
      "batch 1962: loss 0.000155\n",
      "batch 1963: loss 0.019895\n",
      "batch 1964: loss 0.010609\n",
      "batch 1965: loss 0.055613\n",
      "batch 1966: loss 0.061528\n",
      "batch 1967: loss 0.005668\n",
      "batch 1968: loss 0.001869\n",
      "batch 1969: loss 0.012127\n",
      "batch 1970: loss 0.029668\n",
      "batch 1971: loss 0.016742\n",
      "batch 1972: loss 0.002699\n",
      "batch 1973: loss 0.002344\n",
      "batch 1974: loss 0.061149\n",
      "batch 1975: loss 0.163713\n",
      "batch 1976: loss 0.000226\n",
      "batch 1977: loss 0.000805\n",
      "batch 1978: loss 0.010199\n",
      "batch 1979: loss 0.010712\n",
      "batch 1980: loss 0.006756\n",
      "batch 1981: loss 0.010863\n",
      "batch 1982: loss 0.002004\n",
      "batch 1983: loss 0.054687\n",
      "batch 1984: loss 0.052868\n",
      "batch 1985: loss 0.022086\n",
      "batch 1986: loss 0.001535\n",
      "batch 1987: loss 0.005493\n",
      "batch 1988: loss 0.124141\n",
      "batch 1989: loss 0.008500\n",
      "batch 1990: loss 0.012561\n",
      "batch 1991: loss 0.003274\n",
      "batch 1992: loss 0.001559\n",
      "batch 1993: loss 0.193007\n",
      "batch 1994: loss 0.010840\n",
      "batch 1995: loss 0.008973\n",
      "batch 1996: loss 0.009657\n",
      "batch 1997: loss 0.097234\n",
      "batch 1998: loss 0.001559\n",
      "batch 1999: loss 0.074554\n",
      "batch 2000: loss 0.003326\n",
      "batch 2001: loss 0.031987\n",
      "batch 2002: loss 0.047052\n",
      "batch 2003: loss 0.001653\n",
      "batch 2004: loss 0.001430\n",
      "batch 2005: loss 0.008114\n",
      "batch 2006: loss 0.037351\n",
      "batch 2007: loss 0.262745\n",
      "batch 2008: loss 0.064141\n",
      "batch 2009: loss 0.025299\n",
      "batch 2010: loss 0.003148\n",
      "batch 2011: loss 0.081064\n",
      "batch 2012: loss 0.172741\n",
      "batch 2013: loss 0.001516\n",
      "batch 2014: loss 0.019859\n",
      "batch 2015: loss 0.037176\n",
      "batch 2016: loss 0.065689\n",
      "batch 2017: loss 0.008871\n",
      "batch 2018: loss 0.110270\n",
      "batch 2019: loss 0.059853\n",
      "batch 2020: loss 0.024576\n",
      "batch 2021: loss 0.130214\n",
      "batch 2022: loss 0.004140\n",
      "batch 2023: loss 0.004687\n",
      "batch 2024: loss 0.009390\n",
      "batch 2025: loss 0.019741\n",
      "batch 2026: loss 0.033187\n",
      "batch 2027: loss 0.061531\n",
      "batch 2028: loss 0.006801\n",
      "batch 2029: loss 0.024496\n",
      "batch 2030: loss 0.129771\n",
      "batch 2031: loss 0.004742\n",
      "batch 2032: loss 0.050158\n",
      "batch 2033: loss 0.003480\n",
      "batch 2034: loss 0.025353\n",
      "batch 2035: loss 0.002940\n",
      "batch 2036: loss 0.012615\n",
      "batch 2037: loss 0.017333\n",
      "batch 2038: loss 0.022204\n",
      "batch 2039: loss 0.004785\n",
      "batch 2040: loss 0.017930\n",
      "batch 2041: loss 0.043236\n",
      "batch 2042: loss 0.004214\n",
      "batch 2043: loss 0.183189\n",
      "batch 2044: loss 0.002646\n",
      "batch 2045: loss 0.002419\n",
      "batch 2046: loss 0.030432\n",
      "batch 2047: loss 0.105028\n",
      "batch 2048: loss 0.005355\n",
      "batch 2049: loss 0.004983\n",
      "batch 2050: loss 0.054579\n",
      "batch 2051: loss 0.002209\n",
      "batch 2052: loss 0.008556\n",
      "batch 2053: loss 0.002902\n",
      "batch 2054: loss 0.006847\n",
      "batch 2055: loss 0.002577\n",
      "batch 2056: loss 0.010449\n",
      "batch 2057: loss 0.010167\n",
      "batch 2058: loss 0.130588\n",
      "batch 2059: loss 0.088592\n",
      "batch 2060: loss 0.001925\n",
      "batch 2061: loss 0.002075\n",
      "batch 2062: loss 0.030614\n",
      "batch 2063: loss 0.018169\n",
      "batch 2064: loss 0.021100\n",
      "batch 2065: loss 0.099061\n",
      "batch 2066: loss 0.060320\n",
      "batch 2067: loss 0.003181\n",
      "batch 2068: loss 0.001461\n",
      "batch 2069: loss 0.080619\n",
      "batch 2070: loss 0.009547\n",
      "batch 2071: loss 0.090566\n",
      "batch 2072: loss 0.018733\n",
      "batch 2073: loss 0.005959\n",
      "batch 2074: loss 0.017340\n",
      "batch 2075: loss 0.007420\n",
      "batch 2076: loss 0.057857\n",
      "batch 2077: loss 0.025111\n",
      "batch 2078: loss 0.006692\n",
      "batch 2079: loss 0.008425\n",
      "batch 2080: loss 0.003086\n",
      "batch 2081: loss 0.023615\n",
      "batch 2082: loss 0.001726\n",
      "batch 2083: loss 0.013410\n",
      "batch 2084: loss 0.045243\n",
      "batch 2085: loss 0.043592\n",
      "batch 2086: loss 0.011785\n",
      "batch 2087: loss 0.001726\n",
      "batch 2088: loss 0.001895\n",
      "batch 2089: loss 0.008704\n",
      "batch 2090: loss 0.029496\n",
      "batch 2091: loss 0.022611\n",
      "batch 2092: loss 0.006094\n",
      "batch 2093: loss 0.013174\n",
      "batch 2094: loss 0.089676\n",
      "batch 2095: loss 0.027801\n",
      "batch 2096: loss 0.054296\n",
      "batch 2097: loss 0.026546\n",
      "batch 2098: loss 0.106492\n",
      "batch 2099: loss 0.001875\n",
      "batch 2100: loss 0.006865\n",
      "batch 2101: loss 0.001844\n",
      "batch 2102: loss 0.006331\n",
      "batch 2103: loss 0.043829\n",
      "batch 2104: loss 0.012696\n",
      "batch 2105: loss 0.094153\n",
      "batch 2106: loss 0.008952\n",
      "batch 2107: loss 0.007075\n",
      "batch 2108: loss 0.003606\n",
      "batch 2109: loss 0.004393\n",
      "batch 2110: loss 0.051072\n",
      "batch 2111: loss 0.160126\n",
      "batch 2112: loss 0.049506\n",
      "batch 2113: loss 0.000769\n",
      "batch 2114: loss 0.002047\n",
      "batch 2115: loss 0.001161\n",
      "batch 2116: loss 0.031436\n",
      "batch 2117: loss 0.094087\n",
      "batch 2118: loss 0.002176\n",
      "batch 2119: loss 0.018406\n",
      "batch 2120: loss 0.004662\n",
      "batch 2121: loss 0.016923\n",
      "batch 2122: loss 0.007214\n",
      "batch 2123: loss 0.032580\n",
      "batch 2124: loss 0.020696\n",
      "batch 2125: loss 0.008107\n",
      "batch 2126: loss 0.000517\n",
      "batch 2127: loss 0.052813\n",
      "batch 2128: loss 0.019686\n",
      "batch 2129: loss 0.016050\n",
      "batch 2130: loss 0.009275\n",
      "batch 2131: loss 0.004051\n",
      "batch 2132: loss 0.043301\n",
      "batch 2133: loss 0.022574\n",
      "batch 2134: loss 0.016890\n",
      "batch 2135: loss 0.018729\n",
      "batch 2136: loss 0.008312\n",
      "batch 2137: loss 0.098005\n",
      "batch 2138: loss 0.003758\n",
      "batch 2139: loss 0.000271\n",
      "batch 2140: loss 0.008159\n",
      "batch 2141: loss 0.007387\n",
      "batch 2142: loss 0.072513\n",
      "batch 2143: loss 0.021754\n",
      "batch 2144: loss 0.006602\n",
      "batch 2145: loss 0.032185\n",
      "batch 2146: loss 0.000782\n",
      "batch 2147: loss 0.001204\n",
      "batch 2148: loss 0.007634\n",
      "batch 2149: loss 0.000794\n",
      "batch 2150: loss 0.001072\n",
      "batch 2151: loss 0.003670\n",
      "batch 2152: loss 0.025080\n",
      "batch 2153: loss 0.007256\n",
      "batch 2154: loss 0.023210\n",
      "batch 2155: loss 0.039009\n",
      "batch 2156: loss 0.000697\n",
      "batch 2157: loss 0.000492\n",
      "batch 2158: loss 0.001844\n",
      "batch 2159: loss 0.000748\n",
      "batch 2160: loss 0.001504\n",
      "batch 2161: loss 0.004324\n",
      "batch 2162: loss 0.025674\n",
      "batch 2163: loss 0.003472\n",
      "batch 2164: loss 0.037132\n",
      "batch 2165: loss 0.002830\n",
      "batch 2166: loss 0.023627\n",
      "batch 2167: loss 0.008802\n",
      "batch 2168: loss 0.033718\n",
      "batch 2169: loss 0.030081\n",
      "batch 2170: loss 0.031483\n",
      "batch 2171: loss 0.130245\n",
      "batch 2172: loss 0.012075\n",
      "batch 2173: loss 0.040647\n",
      "batch 2174: loss 0.007324\n",
      "batch 2175: loss 0.076689\n",
      "batch 2176: loss 0.060285\n",
      "batch 2177: loss 0.119676\n",
      "batch 2178: loss 0.001794\n",
      "batch 2179: loss 0.026694\n",
      "batch 2180: loss 0.003880\n",
      "batch 2181: loss 0.059300\n",
      "batch 2182: loss 0.016888\n",
      "batch 2183: loss 0.010436\n",
      "batch 2184: loss 0.007479\n",
      "batch 2185: loss 0.015455\n",
      "batch 2186: loss 0.037579\n",
      "batch 2187: loss 0.003126\n",
      "batch 2188: loss 0.005373\n",
      "batch 2189: loss 0.038405\n",
      "batch 2190: loss 0.008067\n",
      "batch 2191: loss 0.001777\n",
      "batch 2192: loss 0.003105\n",
      "batch 2193: loss 0.008388\n",
      "batch 2194: loss 0.017183\n",
      "batch 2195: loss 0.006280\n",
      "batch 2196: loss 0.007903\n",
      "batch 2197: loss 0.005805\n",
      "batch 2198: loss 0.045220\n",
      "batch 2199: loss 0.001020\n",
      "batch 2200: loss 0.000210\n",
      "batch 2201: loss 0.025545\n",
      "batch 2202: loss 0.019244\n",
      "batch 2203: loss 0.002850\n",
      "batch 2204: loss 0.000212\n",
      "batch 2205: loss 0.002432\n",
      "batch 2206: loss 0.001436\n",
      "batch 2207: loss 0.000234\n",
      "batch 2208: loss 0.003732\n",
      "batch 2209: loss 0.001524\n",
      "batch 2210: loss 0.095816\n",
      "batch 2211: loss 0.073383\n",
      "batch 2212: loss 0.002500\n",
      "batch 2213: loss 0.002981\n",
      "batch 2214: loss 0.016751\n",
      "batch 2215: loss 0.001835\n",
      "batch 2216: loss 0.032137\n",
      "batch 2217: loss 0.150477\n",
      "batch 2218: loss 0.002309\n",
      "batch 2219: loss 0.003067\n",
      "batch 2220: loss 0.001655\n",
      "batch 2221: loss 0.015552\n",
      "batch 2222: loss 0.032512\n",
      "batch 2223: loss 0.295173\n",
      "batch 2224: loss 0.122190\n",
      "batch 2225: loss 0.023408\n",
      "batch 2226: loss 0.001201\n",
      "batch 2227: loss 0.010600\n",
      "batch 2228: loss 0.001574\n",
      "batch 2229: loss 0.000600\n",
      "batch 2230: loss 0.040665\n",
      "batch 2231: loss 0.003968\n",
      "batch 2232: loss 0.013911\n",
      "batch 2233: loss 0.000391\n",
      "batch 2234: loss 0.035519\n",
      "batch 2235: loss 0.013166\n",
      "batch 2236: loss 0.012036\n",
      "batch 2237: loss 0.009461\n",
      "batch 2238: loss 0.043425\n",
      "batch 2239: loss 0.057211\n",
      "batch 2240: loss 0.004635\n",
      "batch 2241: loss 0.009155\n",
      "batch 2242: loss 0.075382\n",
      "batch 2243: loss 0.015478\n",
      "batch 2244: loss 0.091424\n",
      "batch 2245: loss 0.032826\n",
      "batch 2246: loss 0.019401\n",
      "batch 2247: loss 0.030931\n",
      "batch 2248: loss 0.003526\n",
      "batch 2249: loss 0.002301\n",
      "batch 2250: loss 0.007686\n",
      "batch 2251: loss 0.002156\n",
      "batch 2252: loss 0.021946\n",
      "batch 2253: loss 0.011914\n",
      "batch 2254: loss 0.035222\n",
      "batch 2255: loss 0.003026\n",
      "batch 2256: loss 0.001047\n",
      "batch 2257: loss 0.006756\n",
      "batch 2258: loss 0.155009\n",
      "batch 2259: loss 0.011883\n",
      "batch 2260: loss 0.004496\n",
      "batch 2261: loss 0.013014\n",
      "batch 2262: loss 0.012357\n",
      "batch 2263: loss 0.007007\n",
      "batch 2264: loss 0.010668\n",
      "batch 2265: loss 0.013656\n",
      "batch 2266: loss 0.091869\n",
      "batch 2267: loss 0.034259\n",
      "batch 2268: loss 0.021689\n",
      "batch 2269: loss 0.001143\n",
      "batch 2270: loss 0.003255\n",
      "batch 2271: loss 0.003392\n",
      "batch 2272: loss 0.000567\n",
      "batch 2273: loss 0.003981\n",
      "batch 2274: loss 0.004841\n",
      "batch 2275: loss 0.038412\n",
      "batch 2276: loss 0.013806\n",
      "batch 2277: loss 0.000417\n",
      "batch 2278: loss 0.003962\n",
      "batch 2279: loss 0.003500\n",
      "batch 2280: loss 0.000132\n",
      "batch 2281: loss 0.009859\n",
      "batch 2282: loss 0.010719\n",
      "batch 2283: loss 0.004844\n",
      "batch 2284: loss 0.132351\n",
      "batch 2285: loss 0.018048\n",
      "batch 2286: loss 0.005034\n",
      "batch 2287: loss 0.069801\n",
      "batch 2288: loss 0.001658\n",
      "batch 2289: loss 0.001301\n",
      "batch 2290: loss 0.003241\n",
      "batch 2291: loss 0.000251\n",
      "batch 2292: loss 0.006462\n",
      "batch 2293: loss 0.005863\n",
      "batch 2294: loss 0.104304\n",
      "batch 2295: loss 0.001386\n",
      "batch 2296: loss 0.016200\n",
      "batch 2297: loss 0.109210\n",
      "batch 2298: loss 0.032238\n",
      "batch 2299: loss 0.026456\n",
      "batch 2300: loss 0.039110\n",
      "batch 2301: loss 0.123923\n",
      "batch 2302: loss 0.005714\n",
      "batch 2303: loss 0.002246\n",
      "batch 2304: loss 0.009415\n",
      "batch 2305: loss 0.022946\n",
      "batch 2306: loss 0.015185\n",
      "batch 2307: loss 0.004675\n",
      "batch 2308: loss 0.004166\n",
      "batch 2309: loss 0.001859\n",
      "batch 2310: loss 0.008277\n",
      "batch 2311: loss 0.018295\n",
      "batch 2312: loss 0.019737\n",
      "batch 2313: loss 0.012690\n",
      "batch 2314: loss 0.085678\n",
      "batch 2315: loss 0.013824\n",
      "batch 2316: loss 0.009896\n",
      "batch 2317: loss 0.051379\n",
      "batch 2318: loss 0.008945\n",
      "batch 2319: loss 0.009878\n",
      "batch 2320: loss 0.002132\n",
      "batch 2321: loss 0.055218\n",
      "batch 2322: loss 0.006263\n",
      "batch 2323: loss 0.044699\n",
      "batch 2324: loss 0.180552\n",
      "batch 2325: loss 0.000519\n",
      "batch 2326: loss 0.013396\n",
      "batch 2327: loss 0.013191\n",
      "batch 2328: loss 0.003002\n",
      "batch 2329: loss 0.003189\n",
      "batch 2330: loss 0.004081\n",
      "batch 2331: loss 0.181885\n",
      "batch 2332: loss 0.003919\n",
      "batch 2333: loss 0.055698\n",
      "batch 2334: loss 0.005345\n",
      "batch 2335: loss 0.002479\n",
      "batch 2336: loss 0.004854\n",
      "batch 2337: loss 0.022788\n",
      "batch 2338: loss 0.056735\n",
      "batch 2339: loss 0.011941\n",
      "batch 2340: loss 0.000956\n",
      "batch 2341: loss 0.013009\n",
      "batch 2342: loss 0.159251\n",
      "batch 2343: loss 0.041613\n",
      "batch 2344: loss 0.078106\n",
      "batch 2345: loss 0.012078\n",
      "batch 2346: loss 0.007153\n",
      "batch 2347: loss 0.056001\n",
      "batch 2348: loss 0.044334\n",
      "batch 2349: loss 0.005042\n",
      "batch 2350: loss 0.006548\n",
      "batch 2351: loss 0.007170\n",
      "batch 2352: loss 0.078114\n",
      "batch 2353: loss 0.001716\n",
      "batch 2354: loss 0.044289\n",
      "batch 2355: loss 0.039767\n",
      "batch 2356: loss 0.002155\n",
      "batch 2357: loss 0.239992\n",
      "batch 2358: loss 0.002766\n",
      "batch 2359: loss 0.003749\n",
      "batch 2360: loss 0.002844\n",
      "batch 2361: loss 0.044876\n",
      "batch 2362: loss 0.002419\n",
      "batch 2363: loss 0.137323\n",
      "batch 2364: loss 0.025468\n",
      "batch 2365: loss 0.236600\n",
      "batch 2366: loss 0.039455\n",
      "batch 2367: loss 0.043031\n",
      "batch 2368: loss 0.014905\n",
      "batch 2369: loss 0.020644\n",
      "batch 2370: loss 0.004749\n",
      "batch 2371: loss 0.012898\n",
      "batch 2372: loss 0.012674\n",
      "batch 2373: loss 0.004965\n",
      "batch 2374: loss 0.080104\n",
      "batch 2375: loss 0.004173\n",
      "batch 2376: loss 0.052677\n",
      "batch 2377: loss 0.004488\n",
      "batch 2378: loss 0.024228\n",
      "batch 2379: loss 0.005807\n",
      "batch 2380: loss 0.173497\n",
      "batch 2381: loss 0.154986\n",
      "batch 2382: loss 0.127454\n",
      "batch 2383: loss 0.046155\n",
      "batch 2384: loss 0.030856\n",
      "batch 2385: loss 0.011157\n",
      "batch 2386: loss 0.003183\n",
      "batch 2387: loss 0.009300\n",
      "batch 2388: loss 0.012952\n",
      "batch 2389: loss 0.045662\n",
      "batch 2390: loss 0.076824\n",
      "batch 2391: loss 0.002969\n",
      "batch 2392: loss 0.040786\n",
      "batch 2393: loss 0.015974\n",
      "batch 2394: loss 0.004056\n",
      "batch 2395: loss 0.001265\n",
      "batch 2396: loss 0.174640\n",
      "batch 2397: loss 0.016852\n",
      "batch 2398: loss 0.033017\n",
      "batch 2399: loss 0.009237\n",
      "batch 2400: loss 0.038804\n",
      "batch 2401: loss 0.064396\n",
      "batch 2402: loss 0.011788\n",
      "batch 2403: loss 0.002534\n",
      "batch 2404: loss 0.009393\n",
      "batch 2405: loss 0.037959\n",
      "batch 2406: loss 0.002926\n",
      "batch 2407: loss 0.001861\n",
      "batch 2408: loss 0.087453\n",
      "batch 2409: loss 0.018054\n",
      "batch 2410: loss 0.021640\n",
      "batch 2411: loss 0.001893\n",
      "batch 2412: loss 0.032260\n",
      "batch 2413: loss 0.030567\n",
      "batch 2414: loss 0.002713\n",
      "batch 2415: loss 0.013843\n",
      "batch 2416: loss 0.135325\n",
      "batch 2417: loss 0.011995\n",
      "batch 2418: loss 0.030500\n",
      "batch 2419: loss 0.000888\n",
      "batch 2420: loss 0.045786\n",
      "batch 2421: loss 0.001899\n",
      "batch 2422: loss 0.001522\n",
      "batch 2423: loss 0.014695\n",
      "batch 2424: loss 0.008627\n",
      "batch 2425: loss 0.003244\n",
      "batch 2426: loss 0.033291\n",
      "batch 2427: loss 0.003551\n",
      "batch 2428: loss 0.001054\n",
      "batch 2429: loss 0.026277\n",
      "batch 2430: loss 0.009435\n",
      "batch 2431: loss 0.008278\n",
      "batch 2432: loss 0.007567\n",
      "batch 2433: loss 0.002815\n",
      "batch 2434: loss 0.001373\n",
      "batch 2435: loss 0.002878\n",
      "batch 2436: loss 0.050167\n",
      "batch 2437: loss 0.004745\n",
      "batch 2438: loss 0.002560\n",
      "batch 2439: loss 0.001271\n",
      "batch 2440: loss 0.006962\n",
      "batch 2441: loss 0.037044\n",
      "batch 2442: loss 0.000366\n",
      "batch 2443: loss 0.004689\n",
      "batch 2444: loss 0.000487\n",
      "batch 2445: loss 0.005380\n",
      "batch 2446: loss 0.009596\n",
      "batch 2447: loss 0.056245\n",
      "batch 2448: loss 0.004989\n",
      "batch 2449: loss 0.008030\n",
      "batch 2450: loss 0.016321\n",
      "batch 2451: loss 0.036578\n",
      "batch 2452: loss 0.002449\n",
      "batch 2453: loss 0.013083\n",
      "batch 2454: loss 0.002023\n",
      "batch 2455: loss 0.005206\n",
      "batch 2456: loss 0.011723\n",
      "batch 2457: loss 0.000272\n",
      "batch 2458: loss 0.002350\n",
      "batch 2459: loss 0.002036\n",
      "batch 2460: loss 0.000371\n",
      "batch 2461: loss 0.000242\n",
      "batch 2462: loss 0.021422\n",
      "batch 2463: loss 0.009459\n",
      "batch 2464: loss 0.010198\n",
      "batch 2465: loss 0.136731\n",
      "batch 2466: loss 0.003450\n",
      "batch 2467: loss 0.001729\n",
      "batch 2468: loss 0.056294\n",
      "batch 2469: loss 0.146651\n",
      "batch 2470: loss 0.095591\n",
      "batch 2471: loss 0.074576\n",
      "batch 2472: loss 0.026386\n",
      "batch 2473: loss 0.015435\n",
      "batch 2474: loss 0.029538\n",
      "batch 2475: loss 0.137734\n",
      "batch 2476: loss 0.000702\n",
      "batch 2477: loss 0.016503\n",
      "batch 2478: loss 0.009410\n",
      "batch 2479: loss 0.104811\n",
      "batch 2480: loss 0.034721\n",
      "batch 2481: loss 0.010932\n",
      "batch 2482: loss 0.101106\n",
      "batch 2483: loss 0.036497\n",
      "batch 2484: loss 0.003264\n",
      "batch 2485: loss 0.009772\n",
      "batch 2486: loss 0.020981\n",
      "batch 2487: loss 0.004742\n",
      "batch 2488: loss 0.043816\n",
      "batch 2489: loss 0.052019\n",
      "batch 2490: loss 0.002971\n",
      "batch 2491: loss 0.005107\n",
      "batch 2492: loss 0.012960\n",
      "batch 2493: loss 0.027389\n",
      "batch 2494: loss 0.004690\n",
      "batch 2495: loss 0.000370\n",
      "batch 2496: loss 0.048450\n",
      "batch 2497: loss 0.060931\n",
      "batch 2498: loss 0.010067\n",
      "batch 2499: loss 0.000910\n",
      "batch 2500: loss 0.098853\n",
      "batch 2501: loss 0.002580\n",
      "batch 2502: loss 0.004656\n",
      "batch 2503: loss 0.002037\n",
      "batch 2504: loss 0.003834\n",
      "batch 2505: loss 0.063028\n",
      "batch 2506: loss 0.035889\n",
      "batch 2507: loss 0.005428\n",
      "batch 2508: loss 0.041448\n",
      "batch 2509: loss 0.029137\n",
      "batch 2510: loss 0.001697\n",
      "batch 2511: loss 0.008625\n",
      "batch 2512: loss 0.005668\n",
      "batch 2513: loss 0.063131\n",
      "batch 2514: loss 0.010710\n",
      "batch 2515: loss 0.008078\n",
      "batch 2516: loss 0.001639\n",
      "batch 2517: loss 0.085143\n",
      "batch 2518: loss 0.000503\n",
      "batch 2519: loss 0.004961\n",
      "batch 2520: loss 0.015361\n",
      "batch 2521: loss 0.002855\n",
      "batch 2522: loss 0.008679\n",
      "batch 2523: loss 0.001073\n",
      "batch 2524: loss 0.089389\n",
      "batch 2525: loss 0.007217\n",
      "batch 2526: loss 0.004333\n",
      "batch 2527: loss 0.007617\n",
      "batch 2528: loss 0.027970\n",
      "batch 2529: loss 0.101351\n",
      "batch 2530: loss 0.000581\n",
      "batch 2531: loss 0.001276\n",
      "batch 2532: loss 0.002332\n",
      "batch 2533: loss 0.032783\n",
      "batch 2534: loss 0.248505\n",
      "batch 2535: loss 0.003998\n",
      "batch 2536: loss 0.005237\n",
      "batch 2537: loss 0.000767\n",
      "batch 2538: loss 0.020554\n",
      "batch 2539: loss 0.002913\n",
      "batch 2540: loss 0.004503\n",
      "batch 2541: loss 0.025258\n",
      "batch 2542: loss 0.006291\n",
      "batch 2543: loss 0.000730\n",
      "batch 2544: loss 0.059066\n",
      "batch 2545: loss 0.003992\n",
      "batch 2546: loss 0.007820\n",
      "batch 2547: loss 0.025561\n",
      "batch 2548: loss 0.006924\n",
      "batch 2549: loss 0.028470\n",
      "batch 2550: loss 0.014477\n",
      "batch 2551: loss 0.018661\n",
      "batch 2552: loss 0.010720\n",
      "batch 2553: loss 0.033216\n",
      "batch 2554: loss 0.001280\n",
      "batch 2555: loss 0.008090\n",
      "batch 2556: loss 0.123364\n",
      "batch 2557: loss 0.034373\n",
      "batch 2558: loss 0.008316\n",
      "batch 2559: loss 0.000548\n",
      "batch 2560: loss 0.007738\n",
      "batch 2561: loss 0.014440\n",
      "batch 2562: loss 0.005069\n",
      "batch 2563: loss 0.005021\n",
      "batch 2564: loss 0.002893\n",
      "batch 2565: loss 0.013339\n",
      "batch 2566: loss 0.015187\n",
      "batch 2567: loss 0.000569\n",
      "batch 2568: loss 0.001633\n",
      "batch 2569: loss 0.005053\n",
      "batch 2570: loss 0.008228\n",
      "batch 2571: loss 0.015370\n",
      "batch 2572: loss 0.002569\n",
      "batch 2573: loss 0.001043\n",
      "batch 2574: loss 0.005216\n",
      "batch 2575: loss 0.041083\n",
      "batch 2576: loss 0.014916\n",
      "batch 2577: loss 0.011723\n",
      "batch 2578: loss 0.008818\n",
      "batch 2579: loss 0.006040\n",
      "batch 2580: loss 0.000175\n",
      "batch 2581: loss 0.000870\n",
      "batch 2582: loss 0.014341\n",
      "batch 2583: loss 0.000999\n",
      "batch 2584: loss 0.003530\n",
      "batch 2585: loss 0.000412\n",
      "batch 2586: loss 0.007017\n",
      "batch 2587: loss 0.020546\n",
      "batch 2588: loss 0.044176\n",
      "batch 2589: loss 0.007492\n",
      "batch 2590: loss 0.032034\n",
      "batch 2591: loss 0.011775\n",
      "batch 2592: loss 0.015169\n",
      "batch 2593: loss 0.000753\n",
      "batch 2594: loss 0.047792\n",
      "batch 2595: loss 0.000389\n",
      "batch 2596: loss 0.001089\n",
      "batch 2597: loss 0.111684\n",
      "batch 2598: loss 0.004573\n",
      "batch 2599: loss 0.007926\n",
      "batch 2600: loss 0.011632\n",
      "batch 2601: loss 0.004167\n",
      "batch 2602: loss 0.013837\n",
      "batch 2603: loss 0.005820\n",
      "batch 2604: loss 0.050746\n",
      "batch 2605: loss 0.012775\n",
      "batch 2606: loss 0.000993\n",
      "batch 2607: loss 0.004133\n",
      "batch 2608: loss 0.011712\n",
      "batch 2609: loss 0.002334\n",
      "batch 2610: loss 0.005924\n",
      "batch 2611: loss 0.082636\n",
      "batch 2612: loss 0.048935\n",
      "batch 2613: loss 0.151244\n",
      "batch 2614: loss 0.001291\n",
      "batch 2615: loss 0.004512\n",
      "batch 2616: loss 0.004386\n",
      "batch 2617: loss 0.027741\n",
      "batch 2618: loss 0.014634\n",
      "batch 2619: loss 0.066999\n",
      "batch 2620: loss 0.007909\n",
      "batch 2621: loss 0.007946\n",
      "batch 2622: loss 0.005972\n",
      "batch 2623: loss 0.022035\n",
      "batch 2624: loss 0.014043\n",
      "batch 2625: loss 0.011281\n",
      "batch 2626: loss 0.000853\n",
      "batch 2627: loss 0.000541\n",
      "batch 2628: loss 0.007147\n",
      "batch 2629: loss 0.008965\n",
      "batch 2630: loss 0.007601\n",
      "batch 2631: loss 0.002201\n",
      "batch 2632: loss 0.050751\n",
      "batch 2633: loss 0.000499\n",
      "batch 2634: loss 0.007413\n",
      "batch 2635: loss 0.002932\n",
      "batch 2636: loss 0.004300\n",
      "batch 2637: loss 0.008230\n",
      "batch 2638: loss 0.028919\n",
      "batch 2639: loss 0.035531\n",
      "batch 2640: loss 0.035637\n",
      "batch 2641: loss 0.002142\n",
      "batch 2642: loss 0.003193\n",
      "batch 2643: loss 0.028712\n",
      "batch 2644: loss 0.006505\n",
      "batch 2645: loss 0.005733\n",
      "batch 2646: loss 0.017301\n",
      "batch 2647: loss 0.010039\n",
      "batch 2648: loss 0.099565\n",
      "batch 2649: loss 0.015280\n",
      "batch 2650: loss 0.002427\n",
      "batch 2651: loss 0.015531\n",
      "batch 2652: loss 0.033734\n",
      "batch 2653: loss 0.014608\n",
      "batch 2654: loss 0.001031\n",
      "batch 2655: loss 0.001953\n",
      "batch 2656: loss 0.021699\n",
      "batch 2657: loss 0.171521\n",
      "batch 2658: loss 0.001252\n",
      "batch 2659: loss 0.034135\n",
      "batch 2660: loss 0.022408\n",
      "batch 2661: loss 0.013672\n",
      "batch 2662: loss 0.000401\n",
      "batch 2663: loss 0.060240\n",
      "batch 2664: loss 0.011541\n",
      "batch 2665: loss 0.038832\n",
      "batch 2666: loss 0.020329\n",
      "batch 2667: loss 0.001452\n",
      "batch 2668: loss 0.025604\n",
      "batch 2669: loss 0.061247\n",
      "batch 2670: loss 0.009100\n",
      "batch 2671: loss 0.001462\n",
      "batch 2672: loss 0.105960\n",
      "batch 2673: loss 0.000455\n",
      "batch 2674: loss 0.074537\n",
      "batch 2675: loss 0.004354\n",
      "batch 2676: loss 0.008944\n",
      "batch 2677: loss 0.029006\n",
      "batch 2678: loss 0.007360\n",
      "batch 2679: loss 0.002982\n",
      "batch 2680: loss 0.000422\n",
      "batch 2681: loss 0.023745\n",
      "batch 2682: loss 0.000348\n",
      "batch 2683: loss 0.080253\n",
      "batch 2684: loss 0.003372\n",
      "batch 2685: loss 0.001257\n",
      "batch 2686: loss 0.002369\n",
      "batch 2687: loss 0.027010\n",
      "batch 2688: loss 0.016724\n",
      "batch 2689: loss 0.007164\n",
      "batch 2690: loss 0.004395\n",
      "batch 2691: loss 0.000489\n",
      "batch 2692: loss 0.006569\n",
      "batch 2693: loss 0.098465\n",
      "batch 2694: loss 0.050980\n",
      "batch 2695: loss 0.006896\n",
      "batch 2696: loss 0.007717\n",
      "batch 2697: loss 0.011610\n",
      "batch 2698: loss 0.040257\n",
      "batch 2699: loss 0.061199\n",
      "batch 2700: loss 0.159941\n",
      "batch 2701: loss 0.002716\n",
      "batch 2702: loss 0.022680\n",
      "batch 2703: loss 0.014418\n",
      "batch 2704: loss 0.021043\n",
      "batch 2705: loss 0.028722\n",
      "batch 2706: loss 0.016632\n",
      "batch 2707: loss 0.000567\n",
      "batch 2708: loss 0.017465\n",
      "batch 2709: loss 0.024339\n",
      "batch 2710: loss 0.051616\n",
      "batch 2711: loss 0.005557\n",
      "batch 2712: loss 0.018127\n",
      "batch 2713: loss 0.026929\n",
      "batch 2714: loss 0.009391\n",
      "batch 2715: loss 0.057871\n",
      "batch 2716: loss 0.003897\n",
      "batch 2717: loss 0.000339\n",
      "batch 2718: loss 0.020361\n",
      "batch 2719: loss 0.005008\n",
      "batch 2720: loss 0.001568\n",
      "batch 2721: loss 0.026574\n",
      "batch 2722: loss 0.001772\n",
      "batch 2723: loss 0.002116\n",
      "batch 2724: loss 0.002389\n",
      "batch 2725: loss 0.001206\n",
      "batch 2726: loss 0.018614\n",
      "batch 2727: loss 0.001551\n",
      "batch 2728: loss 0.003018\n",
      "batch 2729: loss 0.005753\n",
      "batch 2730: loss 0.000725\n",
      "batch 2731: loss 0.004910\n",
      "batch 2732: loss 0.001026\n",
      "batch 2733: loss 0.006158\n",
      "batch 2734: loss 0.005761\n",
      "batch 2735: loss 0.003723\n",
      "batch 2736: loss 0.004957\n",
      "batch 2737: loss 0.001168\n",
      "batch 2738: loss 0.015019\n",
      "batch 2739: loss 0.000458\n",
      "batch 2740: loss 0.030573\n",
      "batch 2741: loss 0.006972\n",
      "batch 2742: loss 0.003888\n",
      "batch 2743: loss 0.002218\n",
      "batch 2744: loss 0.085384\n",
      "batch 2745: loss 0.083659\n",
      "batch 2746: loss 0.001434\n",
      "batch 2747: loss 0.000274\n",
      "batch 2748: loss 0.063077\n",
      "batch 2749: loss 0.000504\n",
      "batch 2750: loss 0.000934\n",
      "batch 2751: loss 0.003743\n",
      "batch 2752: loss 0.004003\n",
      "batch 2753: loss 0.007585\n",
      "batch 2754: loss 0.007660\n",
      "batch 2755: loss 0.007953\n",
      "batch 2756: loss 0.026509\n",
      "batch 2757: loss 0.041369\n",
      "batch 2758: loss 0.000372\n",
      "batch 2759: loss 0.039059\n",
      "batch 2760: loss 0.028172\n",
      "batch 2761: loss 0.003158\n",
      "batch 2762: loss 0.020716\n",
      "batch 2763: loss 0.048158\n",
      "batch 2764: loss 0.000879\n",
      "batch 2765: loss 0.000064\n",
      "batch 2766: loss 0.010427\n",
      "batch 2767: loss 0.078395\n",
      "batch 2768: loss 0.008976\n",
      "batch 2769: loss 0.068098\n",
      "batch 2770: loss 0.007747\n",
      "batch 2771: loss 0.008665\n",
      "batch 2772: loss 0.023391\n",
      "batch 2773: loss 0.177655\n",
      "batch 2774: loss 0.002269\n",
      "batch 2775: loss 0.000533\n",
      "batch 2776: loss 0.030483\n",
      "batch 2777: loss 0.025918\n",
      "batch 2778: loss 0.009990\n",
      "batch 2779: loss 0.003408\n",
      "batch 2780: loss 0.063577\n",
      "batch 2781: loss 0.006122\n",
      "batch 2782: loss 0.056309\n",
      "batch 2783: loss 0.039283\n",
      "batch 2784: loss 0.002753\n",
      "batch 2785: loss 0.033112\n",
      "batch 2786: loss 0.081520\n",
      "batch 2787: loss 0.005796\n",
      "batch 2788: loss 0.032186\n",
      "batch 2789: loss 0.018427\n",
      "batch 2790: loss 0.049201\n",
      "batch 2791: loss 0.013383\n",
      "batch 2792: loss 0.047152\n",
      "batch 2793: loss 0.003920\n",
      "batch 2794: loss 0.092758\n",
      "batch 2795: loss 0.010233\n",
      "batch 2796: loss 0.146464\n",
      "batch 2797: loss 0.011585\n",
      "batch 2798: loss 0.016353\n",
      "batch 2799: loss 0.002407\n",
      "batch 2800: loss 0.150514\n",
      "batch 2801: loss 0.005029\n",
      "batch 2802: loss 0.010112\n",
      "batch 2803: loss 0.001069\n",
      "batch 2804: loss 0.013453\n",
      "batch 2805: loss 0.002620\n",
      "batch 2806: loss 0.016849\n",
      "batch 2807: loss 0.008000\n",
      "batch 2808: loss 0.010486\n",
      "batch 2809: loss 0.042932\n",
      "batch 2810: loss 0.003630\n",
      "batch 2811: loss 0.000951\n",
      "batch 2812: loss 0.013279\n",
      "batch 2813: loss 0.031342\n",
      "batch 2814: loss 0.051219\n",
      "batch 2815: loss 0.002065\n",
      "batch 2816: loss 0.000142\n",
      "batch 2817: loss 0.060964\n",
      "batch 2818: loss 0.028341\n",
      "batch 2819: loss 0.003078\n",
      "batch 2820: loss 0.033393\n",
      "batch 2821: loss 0.001705\n",
      "batch 2822: loss 0.001111\n",
      "batch 2823: loss 0.118463\n",
      "batch 2824: loss 0.000116\n",
      "batch 2825: loss 0.040455\n",
      "batch 2826: loss 0.001420\n",
      "batch 2827: loss 0.001771\n",
      "batch 2828: loss 0.013229\n",
      "batch 2829: loss 0.023995\n",
      "batch 2830: loss 0.001997\n",
      "batch 2831: loss 0.003004\n",
      "batch 2832: loss 0.002619\n",
      "batch 2833: loss 0.006259\n",
      "batch 2834: loss 0.009025\n",
      "batch 2835: loss 0.012851\n",
      "batch 2836: loss 0.001976\n",
      "batch 2837: loss 0.007159\n",
      "batch 2838: loss 0.003378\n",
      "batch 2839: loss 0.002050\n",
      "batch 2840: loss 0.182308\n",
      "batch 2841: loss 0.004261\n",
      "batch 2842: loss 0.004312\n",
      "batch 2843: loss 0.108608\n",
      "batch 2844: loss 0.017844\n",
      "batch 2845: loss 0.027102\n",
      "batch 2846: loss 0.000841\n",
      "batch 2847: loss 0.002471\n",
      "batch 2848: loss 0.015767\n",
      "batch 2849: loss 0.003330\n",
      "batch 2850: loss 0.041852\n",
      "batch 2851: loss 0.004355\n",
      "batch 2852: loss 0.114120\n",
      "batch 2853: loss 0.002355\n",
      "batch 2854: loss 0.001402\n",
      "batch 2855: loss 0.036172\n",
      "batch 2856: loss 0.009166\n",
      "batch 2857: loss 0.014340\n",
      "batch 2858: loss 0.000997\n",
      "batch 2859: loss 0.003776\n",
      "batch 2860: loss 0.005259\n",
      "batch 2861: loss 0.001654\n",
      "batch 2862: loss 0.004531\n",
      "batch 2863: loss 0.026063\n",
      "batch 2864: loss 0.023222\n",
      "batch 2865: loss 0.001863\n",
      "batch 2866: loss 0.014005\n",
      "batch 2867: loss 0.009019\n",
      "batch 2868: loss 0.087493\n",
      "batch 2869: loss 0.001724\n",
      "batch 2870: loss 0.001671\n",
      "batch 2871: loss 0.157093\n",
      "batch 2872: loss 0.000394\n",
      "batch 2873: loss 0.012641\n",
      "batch 2874: loss 0.002899\n",
      "batch 2875: loss 0.005723\n",
      "batch 2876: loss 0.011364\n",
      "batch 2877: loss 0.012349\n",
      "batch 2878: loss 0.007124\n",
      "batch 2879: loss 0.000354\n",
      "batch 2880: loss 0.035204\n",
      "batch 2881: loss 0.000398\n",
      "batch 2882: loss 0.008407\n",
      "batch 2883: loss 0.049567\n",
      "batch 2884: loss 0.002055\n",
      "batch 2885: loss 0.007712\n",
      "batch 2886: loss 0.000844\n",
      "batch 2887: loss 0.067921\n",
      "batch 2888: loss 0.011831\n",
      "batch 2889: loss 0.005769\n",
      "batch 2890: loss 0.090621\n",
      "batch 2891: loss 0.002676\n",
      "batch 2892: loss 0.005906\n",
      "batch 2893: loss 0.062496\n",
      "batch 2894: loss 0.002172\n",
      "batch 2895: loss 0.032685\n",
      "batch 2896: loss 0.001471\n",
      "batch 2897: loss 0.004956\n",
      "batch 2898: loss 0.001899\n",
      "batch 2899: loss 0.026733\n",
      "batch 2900: loss 0.004340\n",
      "batch 2901: loss 0.001734\n",
      "batch 2902: loss 0.054156\n",
      "batch 2903: loss 0.001698\n",
      "batch 2904: loss 0.002548\n",
      "batch 2905: loss 0.018200\n",
      "batch 2906: loss 0.007950\n",
      "batch 2907: loss 0.161253\n",
      "batch 2908: loss 0.027001\n",
      "batch 2909: loss 0.008960\n",
      "batch 2910: loss 0.006623\n",
      "batch 2911: loss 0.011151\n",
      "batch 2912: loss 0.024526\n",
      "batch 2913: loss 0.007786\n",
      "batch 2914: loss 0.024981\n",
      "batch 2915: loss 0.003256\n",
      "batch 2916: loss 0.036182\n",
      "batch 2917: loss 0.128655\n",
      "batch 2918: loss 0.003415\n",
      "batch 2919: loss 0.069630\n",
      "batch 2920: loss 0.006802\n",
      "batch 2921: loss 0.009369\n",
      "batch 2922: loss 0.002276\n",
      "batch 2923: loss 0.012954\n",
      "batch 2924: loss 0.058423\n",
      "batch 2925: loss 0.047151\n",
      "batch 2926: loss 0.007064\n",
      "batch 2927: loss 0.126044\n",
      "batch 2928: loss 0.008758\n",
      "batch 2929: loss 0.012182\n",
      "batch 2930: loss 0.000846\n",
      "batch 2931: loss 0.002336\n",
      "batch 2932: loss 0.012586\n",
      "batch 2933: loss 0.003089\n",
      "batch 2934: loss 0.020394\n",
      "batch 2935: loss 0.022852\n",
      "batch 2936: loss 0.002338\n",
      "batch 2937: loss 0.140289\n",
      "batch 2938: loss 0.000177\n",
      "batch 2939: loss 0.001395\n",
      "batch 2940: loss 0.001104\n",
      "batch 2941: loss 0.001797\n",
      "batch 2942: loss 0.001043\n",
      "batch 2943: loss 0.005433\n",
      "batch 2944: loss 0.013223\n",
      "batch 2945: loss 0.001042\n",
      "batch 2946: loss 0.002543\n",
      "batch 2947: loss 0.007839\n",
      "batch 2948: loss 0.015410\n",
      "batch 2949: loss 0.001188\n",
      "batch 2950: loss 0.001976\n",
      "batch 2951: loss 0.002154\n",
      "batch 2952: loss 0.018978\n",
      "batch 2953: loss 0.001381\n",
      "batch 2954: loss 0.001247\n",
      "batch 2955: loss 0.001235\n",
      "batch 2956: loss 0.002086\n",
      "batch 2957: loss 0.000229\n",
      "batch 2958: loss 0.011732\n",
      "batch 2959: loss 0.202137\n",
      "batch 2960: loss 0.000747\n",
      "batch 2961: loss 0.001644\n",
      "batch 2962: loss 0.001529\n",
      "batch 2963: loss 0.004631\n",
      "batch 2964: loss 0.007350\n",
      "batch 2965: loss 0.003586\n",
      "batch 2966: loss 0.006693\n",
      "batch 2967: loss 0.011186\n",
      "batch 2968: loss 0.059043\n",
      "batch 2969: loss 0.024430\n",
      "batch 2970: loss 0.000629\n",
      "batch 2971: loss 0.046444\n",
      "batch 2972: loss 0.010188\n",
      "batch 2973: loss 0.052421\n",
      "batch 2974: loss 0.001015\n",
      "batch 2975: loss 0.051493\n",
      "batch 2976: loss 0.024855\n",
      "batch 2977: loss 0.006901\n",
      "batch 2978: loss 0.218974\n",
      "batch 2979: loss 0.001243\n",
      "batch 2980: loss 0.033435\n",
      "batch 2981: loss 0.005010\n",
      "batch 2982: loss 0.003800\n",
      "batch 2983: loss 0.078793\n",
      "batch 2984: loss 0.018088\n",
      "batch 2985: loss 0.021062\n",
      "batch 2986: loss 0.042405\n",
      "batch 2987: loss 0.001675\n",
      "batch 2988: loss 0.004619\n",
      "batch 2989: loss 0.136603\n",
      "batch 2990: loss 0.006676\n",
      "batch 2991: loss 0.037250\n",
      "batch 2992: loss 0.130237\n",
      "batch 2993: loss 0.002286\n",
      "batch 2994: loss 0.001567\n",
      "batch 2995: loss 0.003137\n",
      "batch 2996: loss 0.001643\n",
      "batch 2997: loss 0.071742\n",
      "batch 2998: loss 0.001863\n",
      "batch 2999: loss 0.047850\n",
      "batch 3000: loss 0.002364\n",
      "batch 3001: loss 0.125844\n",
      "batch 3002: loss 0.006512\n",
      "batch 3003: loss 0.004681\n",
      "batch 3004: loss 0.004345\n",
      "batch 3005: loss 0.013429\n",
      "batch 3006: loss 0.002436\n",
      "batch 3007: loss 0.016555\n",
      "batch 3008: loss 0.007217\n",
      "batch 3009: loss 0.002647\n",
      "batch 3010: loss 0.134033\n",
      "batch 3011: loss 0.011894\n",
      "batch 3012: loss 0.017155\n",
      "batch 3013: loss 0.012920\n",
      "batch 3014: loss 0.008311\n",
      "batch 3015: loss 0.324384\n",
      "batch 3016: loss 0.006620\n",
      "batch 3017: loss 0.012495\n",
      "batch 3018: loss 0.001887\n",
      "batch 3019: loss 0.003466\n",
      "batch 3020: loss 0.002931\n",
      "batch 3021: loss 0.003536\n",
      "batch 3022: loss 0.000657\n",
      "batch 3023: loss 0.000628\n",
      "batch 3024: loss 0.000751\n",
      "batch 3025: loss 0.020881\n",
      "batch 3026: loss 0.017432\n",
      "batch 3027: loss 0.002480\n",
      "batch 3028: loss 0.006851\n",
      "batch 3029: loss 0.028991\n",
      "batch 3030: loss 0.007958\n",
      "batch 3031: loss 0.008754\n",
      "batch 3032: loss 0.033275\n",
      "batch 3033: loss 0.000505\n",
      "batch 3034: loss 0.020812\n",
      "batch 3035: loss 0.001399\n",
      "batch 3036: loss 0.001334\n",
      "batch 3037: loss 0.014352\n",
      "batch 3038: loss 0.032923\n",
      "batch 3039: loss 0.014719\n",
      "batch 3040: loss 0.000084\n",
      "batch 3041: loss 0.001518\n",
      "batch 3042: loss 0.003267\n",
      "batch 3043: loss 0.000547\n",
      "batch 3044: loss 0.044957\n",
      "batch 3045: loss 0.003992\n",
      "batch 3046: loss 0.010191\n",
      "batch 3047: loss 0.001045\n",
      "batch 3048: loss 0.000224\n",
      "batch 3049: loss 0.001627\n",
      "batch 3050: loss 0.002661\n",
      "batch 3051: loss 0.000172\n",
      "batch 3052: loss 0.020125\n",
      "batch 3053: loss 0.007964\n",
      "batch 3054: loss 0.003010\n",
      "batch 3055: loss 0.000621\n",
      "batch 3056: loss 0.001072\n",
      "batch 3057: loss 0.004530\n",
      "batch 3058: loss 0.000387\n",
      "batch 3059: loss 0.000694\n",
      "batch 3060: loss 0.014812\n",
      "batch 3061: loss 0.006422\n",
      "batch 3062: loss 0.000219\n",
      "batch 3063: loss 0.001193\n",
      "batch 3064: loss 0.001660\n",
      "batch 3065: loss 0.001590\n",
      "batch 3066: loss 0.000219\n",
      "batch 3067: loss 0.002914\n",
      "batch 3068: loss 0.012808\n",
      "batch 3069: loss 0.136471\n",
      "batch 3070: loss 0.009586\n",
      "batch 3071: loss 0.001191\n",
      "batch 3072: loss 0.002065\n",
      "batch 3073: loss 0.053746\n",
      "batch 3074: loss 0.006787\n",
      "batch 3075: loss 0.000647\n",
      "batch 3076: loss 0.000444\n",
      "batch 3077: loss 0.001308\n",
      "batch 3078: loss 0.031470\n",
      "batch 3079: loss 0.051577\n",
      "batch 3080: loss 0.002019\n",
      "batch 3081: loss 0.004226\n",
      "batch 3082: loss 0.054225\n",
      "batch 3083: loss 0.004626\n",
      "batch 3084: loss 0.045015\n",
      "batch 3085: loss 0.014906\n",
      "batch 3086: loss 0.002006\n",
      "batch 3087: loss 0.000506\n",
      "batch 3088: loss 0.000510\n",
      "batch 3089: loss 0.000111\n",
      "batch 3090: loss 0.029308\n",
      "batch 3091: loss 0.000996\n",
      "batch 3092: loss 0.003240\n",
      "batch 3093: loss 0.009700\n",
      "batch 3094: loss 0.005934\n",
      "batch 3095: loss 0.043295\n",
      "batch 3096: loss 0.016645\n",
      "batch 3097: loss 0.000662\n",
      "batch 3098: loss 0.021110\n",
      "batch 3099: loss 0.000598\n",
      "batch 3100: loss 0.001265\n",
      "batch 3101: loss 0.015475\n",
      "batch 3102: loss 0.015425\n",
      "batch 3103: loss 0.007724\n",
      "batch 3104: loss 0.057029\n",
      "batch 3105: loss 0.014388\n",
      "batch 3106: loss 0.001778\n",
      "batch 3107: loss 0.000098\n",
      "batch 3108: loss 0.000758\n",
      "batch 3109: loss 0.001115\n",
      "batch 3110: loss 0.003270\n",
      "batch 3111: loss 0.003479\n",
      "batch 3112: loss 0.005263\n",
      "batch 3113: loss 0.000660\n",
      "batch 3114: loss 0.000349\n",
      "batch 3115: loss 0.000399\n",
      "batch 3116: loss 0.023159\n",
      "batch 3117: loss 0.021454\n",
      "batch 3118: loss 0.005247\n",
      "batch 3119: loss 0.051893\n",
      "batch 3120: loss 0.002947\n",
      "batch 3121: loss 0.007943\n",
      "batch 3122: loss 0.029171\n",
      "batch 3123: loss 0.006203\n",
      "batch 3124: loss 0.002562\n",
      "batch 3125: loss 0.006316\n",
      "batch 3126: loss 0.000763\n",
      "batch 3127: loss 0.001369\n",
      "batch 3128: loss 0.000680\n",
      "batch 3129: loss 0.000760\n",
      "batch 3130: loss 0.036055\n",
      "batch 3131: loss 0.010422\n",
      "batch 3132: loss 0.000330\n",
      "batch 3133: loss 0.000316\n",
      "batch 3134: loss 0.025759\n",
      "batch 3135: loss 0.031003\n",
      "batch 3136: loss 0.004105\n",
      "batch 3137: loss 0.000979\n",
      "batch 3138: loss 0.014179\n",
      "batch 3139: loss 0.081356\n",
      "batch 3140: loss 0.008321\n",
      "batch 3141: loss 0.005741\n",
      "batch 3142: loss 0.025423\n",
      "batch 3143: loss 0.086484\n",
      "batch 3144: loss 0.011302\n",
      "batch 3145: loss 0.007799\n",
      "batch 3146: loss 0.084373\n",
      "batch 3147: loss 0.034764\n",
      "batch 3148: loss 0.003800\n",
      "batch 3149: loss 0.002244\n",
      "batch 3150: loss 0.036331\n",
      "batch 3151: loss 0.000148\n",
      "batch 3152: loss 0.038079\n",
      "batch 3153: loss 0.024978\n",
      "batch 3154: loss 0.016121\n",
      "batch 3155: loss 0.033872\n",
      "batch 3156: loss 0.010813\n",
      "batch 3157: loss 0.135204\n",
      "batch 3158: loss 0.051537\n",
      "batch 3159: loss 0.008631\n",
      "batch 3160: loss 0.008602\n",
      "batch 3161: loss 0.002507\n",
      "batch 3162: loss 0.000680\n",
      "batch 3163: loss 0.007984\n",
      "batch 3164: loss 0.000234\n",
      "batch 3165: loss 0.072647\n",
      "batch 3166: loss 0.037398\n",
      "batch 3167: loss 0.013439\n",
      "batch 3168: loss 0.001584\n",
      "batch 3169: loss 0.002846\n",
      "batch 3170: loss 0.000861\n",
      "batch 3171: loss 0.044061\n",
      "batch 3172: loss 0.015383\n",
      "batch 3173: loss 0.000993\n",
      "batch 3174: loss 0.002238\n",
      "batch 3175: loss 0.021199\n",
      "batch 3176: loss 0.004477\n",
      "batch 3177: loss 0.041663\n",
      "batch 3178: loss 0.021191\n",
      "batch 3179: loss 0.003342\n",
      "batch 3180: loss 0.015424\n",
      "batch 3181: loss 0.035614\n",
      "batch 3182: loss 0.002469\n",
      "batch 3183: loss 0.001865\n",
      "batch 3184: loss 0.000874\n",
      "batch 3185: loss 0.019136\n",
      "batch 3186: loss 0.088087\n",
      "batch 3187: loss 0.001001\n",
      "batch 3188: loss 0.000448\n",
      "batch 3189: loss 0.056264\n",
      "batch 3190: loss 0.017696\n",
      "batch 3191: loss 0.002605\n",
      "batch 3192: loss 0.015642\n",
      "batch 3193: loss 0.061626\n",
      "batch 3194: loss 0.004760\n",
      "batch 3195: loss 0.006697\n",
      "batch 3196: loss 0.067098\n",
      "batch 3197: loss 0.006696\n",
      "batch 3198: loss 0.006571\n",
      "batch 3199: loss 0.000159\n",
      "batch 3200: loss 0.000215\n",
      "batch 3201: loss 0.001407\n",
      "batch 3202: loss 0.015845\n",
      "batch 3203: loss 0.018873\n",
      "batch 3204: loss 0.002390\n",
      "batch 3205: loss 0.020767\n",
      "batch 3206: loss 0.092162\n",
      "batch 3207: loss 0.002681\n",
      "batch 3208: loss 0.002151\n",
      "batch 3209: loss 0.003976\n",
      "batch 3210: loss 0.009971\n",
      "batch 3211: loss 0.001817\n",
      "batch 3212: loss 0.001018\n",
      "batch 3213: loss 0.003389\n",
      "batch 3214: loss 0.000878\n",
      "batch 3215: loss 0.021040\n",
      "batch 3216: loss 0.004607\n",
      "batch 3217: loss 0.019738\n",
      "batch 3218: loss 0.001293\n",
      "batch 3219: loss 0.002900\n",
      "batch 3220: loss 0.000471\n",
      "batch 3221: loss 0.000200\n",
      "batch 3222: loss 0.000326\n",
      "batch 3223: loss 0.002041\n",
      "batch 3224: loss 0.000225\n",
      "batch 3225: loss 0.029463\n",
      "batch 3226: loss 0.166637\n",
      "batch 3227: loss 0.001389\n",
      "batch 3228: loss 0.013686\n",
      "batch 3229: loss 0.000943\n",
      "batch 3230: loss 0.021994\n",
      "batch 3231: loss 0.000830\n",
      "batch 3232: loss 0.001707\n",
      "batch 3233: loss 0.001226\n",
      "batch 3234: loss 0.001871\n",
      "batch 3235: loss 0.001222\n",
      "batch 3236: loss 0.000318\n",
      "batch 3237: loss 0.002699\n",
      "batch 3238: loss 0.000699\n",
      "batch 3239: loss 0.003981\n",
      "batch 3240: loss 0.000872\n",
      "batch 3241: loss 0.005512\n",
      "batch 3242: loss 0.005404\n",
      "batch 3243: loss 0.000758\n",
      "batch 3244: loss 0.023625\n",
      "batch 3245: loss 0.001030\n",
      "batch 3246: loss 0.120536\n",
      "batch 3247: loss 0.005185\n",
      "batch 3248: loss 0.024358\n",
      "batch 3249: loss 0.041498\n",
      "batch 3250: loss 0.002176\n",
      "batch 3251: loss 0.001189\n",
      "batch 3252: loss 0.002681\n",
      "batch 3253: loss 0.009064\n",
      "batch 3254: loss 0.000990\n",
      "batch 3255: loss 0.000266\n",
      "batch 3256: loss 0.174531\n",
      "batch 3257: loss 0.071346\n",
      "batch 3258: loss 0.050396\n",
      "batch 3259: loss 0.011498\n",
      "batch 3260: loss 0.000013\n",
      "batch 3261: loss 0.003193\n",
      "batch 3262: loss 0.018264\n",
      "batch 3263: loss 0.000250\n",
      "batch 3264: loss 0.010640\n",
      "batch 3265: loss 0.000756\n",
      "batch 3266: loss 0.000407\n",
      "batch 3267: loss 0.003613\n",
      "batch 3268: loss 0.004865\n",
      "batch 3269: loss 0.019623\n",
      "batch 3270: loss 0.011257\n",
      "batch 3271: loss 0.011005\n",
      "batch 3272: loss 0.002106\n",
      "batch 3273: loss 0.000815\n",
      "batch 3274: loss 0.018229\n",
      "batch 3275: loss 0.001451\n",
      "batch 3276: loss 0.198577\n",
      "batch 3277: loss 0.124441\n",
      "batch 3278: loss 0.063170\n",
      "batch 3279: loss 0.000638\n",
      "batch 3280: loss 0.000109\n",
      "batch 3281: loss 0.002699\n",
      "batch 3282: loss 0.003090\n",
      "batch 3283: loss 0.002373\n",
      "batch 3284: loss 0.004621\n",
      "batch 3285: loss 0.042931\n",
      "batch 3286: loss 0.003684\n",
      "batch 3287: loss 0.000828\n",
      "batch 3288: loss 0.113528\n",
      "batch 3289: loss 0.002266\n",
      "batch 3290: loss 0.007093\n",
      "batch 3291: loss 0.017693\n",
      "batch 3292: loss 0.027989\n",
      "batch 3293: loss 0.040750\n",
      "batch 3294: loss 0.002046\n",
      "batch 3295: loss 0.022122\n",
      "batch 3296: loss 0.001579\n",
      "batch 3297: loss 0.031018\n",
      "batch 3298: loss 0.043141\n",
      "batch 3299: loss 0.053647\n",
      "batch 3300: loss 0.011474\n",
      "batch 3301: loss 0.002235\n",
      "batch 3302: loss 0.003368\n",
      "batch 3303: loss 0.036163\n",
      "batch 3304: loss 0.171326\n",
      "batch 3305: loss 0.005114\n",
      "batch 3306: loss 0.185990\n",
      "batch 3307: loss 0.012979\n",
      "batch 3308: loss 0.071819\n",
      "batch 3309: loss 0.019911\n",
      "batch 3310: loss 0.004151\n",
      "batch 3311: loss 0.003086\n",
      "batch 3312: loss 0.041276\n",
      "batch 3313: loss 0.008186\n",
      "batch 3314: loss 0.050495\n",
      "batch 3315: loss 0.012543\n",
      "batch 3316: loss 0.001170\n",
      "batch 3317: loss 0.103596\n",
      "batch 3318: loss 0.038088\n",
      "batch 3319: loss 0.037550\n",
      "batch 3320: loss 0.148984\n",
      "batch 3321: loss 0.076939\n",
      "batch 3322: loss 0.013259\n",
      "batch 3323: loss 0.001575\n",
      "batch 3324: loss 0.005748\n",
      "batch 3325: loss 0.026283\n",
      "batch 3326: loss 0.008526\n",
      "batch 3327: loss 0.005449\n",
      "batch 3328: loss 0.070208\n",
      "batch 3329: loss 0.003032\n",
      "batch 3330: loss 0.007450\n",
      "batch 3331: loss 0.002601\n",
      "batch 3332: loss 0.002616\n",
      "batch 3333: loss 0.051315\n",
      "batch 3334: loss 0.006752\n",
      "batch 3335: loss 0.108593\n",
      "batch 3336: loss 0.049043\n",
      "batch 3337: loss 0.004532\n",
      "batch 3338: loss 0.002454\n",
      "batch 3339: loss 0.029504\n",
      "batch 3340: loss 0.004045\n",
      "batch 3341: loss 0.004672\n",
      "batch 3342: loss 0.013178\n",
      "batch 3343: loss 0.005654\n",
      "batch 3344: loss 0.004428\n",
      "batch 3345: loss 0.004267\n",
      "batch 3346: loss 0.043023\n",
      "batch 3347: loss 0.000748\n",
      "batch 3348: loss 0.001236\n",
      "batch 3349: loss 0.020688\n",
      "batch 3350: loss 0.008761\n",
      "batch 3351: loss 0.066304\n",
      "batch 3352: loss 0.005248\n",
      "batch 3353: loss 0.077564\n",
      "batch 3354: loss 0.001279\n",
      "batch 3355: loss 0.000219\n",
      "batch 3356: loss 0.002318\n",
      "batch 3357: loss 0.034602\n",
      "batch 3358: loss 0.034633\n",
      "batch 3359: loss 0.001153\n",
      "batch 3360: loss 0.064152\n",
      "batch 3361: loss 0.019818\n",
      "batch 3362: loss 0.001918\n",
      "batch 3363: loss 0.007215\n",
      "batch 3364: loss 0.007266\n",
      "batch 3365: loss 0.009105\n",
      "batch 3366: loss 0.002755\n",
      "batch 3367: loss 0.012347\n",
      "batch 3368: loss 0.002532\n",
      "batch 3369: loss 0.016596\n",
      "batch 3370: loss 0.000272\n",
      "batch 3371: loss 0.003006\n",
      "batch 3372: loss 0.009491\n",
      "batch 3373: loss 0.002572\n",
      "batch 3374: loss 0.066944\n",
      "batch 3375: loss 0.002039\n",
      "batch 3376: loss 0.010097\n",
      "batch 3377: loss 0.114698\n",
      "batch 3378: loss 0.015175\n",
      "batch 3379: loss 0.001610\n",
      "batch 3380: loss 0.001689\n",
      "batch 3381: loss 0.108194\n",
      "batch 3382: loss 0.107827\n",
      "batch 3383: loss 0.009132\n",
      "batch 3384: loss 0.049723\n",
      "batch 3385: loss 0.082825\n",
      "batch 3386: loss 0.003226\n",
      "batch 3387: loss 0.002660\n",
      "batch 3388: loss 0.008180\n",
      "batch 3389: loss 0.106291\n",
      "batch 3390: loss 0.050987\n",
      "batch 3391: loss 0.004594\n",
      "batch 3392: loss 0.022159\n",
      "batch 3393: loss 0.119932\n",
      "batch 3394: loss 0.005460\n",
      "batch 3395: loss 0.052358\n",
      "batch 3396: loss 0.017296\n",
      "batch 3397: loss 0.001031\n",
      "batch 3398: loss 0.000327\n",
      "batch 3399: loss 0.008061\n",
      "batch 3400: loss 0.075817\n",
      "batch 3401: loss 0.008248\n",
      "batch 3402: loss 0.000716\n",
      "batch 3403: loss 0.021099\n",
      "batch 3404: loss 0.047137\n",
      "batch 3405: loss 0.013209\n",
      "batch 3406: loss 0.008974\n",
      "batch 3407: loss 0.000843\n",
      "batch 3408: loss 0.002032\n",
      "batch 3409: loss 0.003575\n",
      "batch 3410: loss 0.001887\n",
      "batch 3411: loss 0.007275\n",
      "batch 3412: loss 0.002309\n",
      "batch 3413: loss 0.010313\n",
      "batch 3414: loss 0.000162\n",
      "batch 3415: loss 0.000344\n",
      "batch 3416: loss 0.072925\n",
      "batch 3417: loss 0.022636\n",
      "batch 3418: loss 0.002312\n",
      "batch 3419: loss 0.000909\n",
      "batch 3420: loss 0.000201\n",
      "batch 3421: loss 0.010208\n",
      "batch 3422: loss 0.002182\n",
      "batch 3423: loss 0.001076\n",
      "batch 3424: loss 0.051806\n",
      "batch 3425: loss 0.003469\n",
      "batch 3426: loss 0.002439\n",
      "batch 3427: loss 0.000672\n",
      "batch 3428: loss 0.008497\n",
      "batch 3429: loss 0.020872\n",
      "batch 3430: loss 0.000437\n",
      "batch 3431: loss 0.017426\n",
      "batch 3432: loss 0.018992\n",
      "batch 3433: loss 0.006834\n",
      "batch 3434: loss 0.001475\n",
      "batch 3435: loss 0.002905\n",
      "batch 3436: loss 0.030436\n",
      "batch 3437: loss 0.004853\n",
      "batch 3438: loss 0.000710\n",
      "batch 3439: loss 0.022503\n",
      "batch 3440: loss 0.032071\n",
      "batch 3441: loss 0.000833\n",
      "batch 3442: loss 0.002359\n",
      "batch 3443: loss 0.193128\n",
      "batch 3444: loss 0.020912\n",
      "batch 3445: loss 0.161192\n",
      "batch 3446: loss 0.005454\n",
      "batch 3447: loss 0.053496\n",
      "batch 3448: loss 0.028185\n",
      "batch 3449: loss 0.008202\n",
      "batch 3450: loss 0.007309\n",
      "batch 3451: loss 0.003281\n",
      "batch 3452: loss 0.011603\n",
      "batch 3453: loss 0.085936\n",
      "batch 3454: loss 0.000318\n",
      "batch 3455: loss 0.011428\n",
      "batch 3456: loss 0.011054\n",
      "batch 3457: loss 0.027481\n",
      "batch 3458: loss 0.000968\n",
      "batch 3459: loss 0.065590\n",
      "batch 3460: loss 0.002648\n",
      "batch 3461: loss 0.008646\n",
      "batch 3462: loss 0.022352\n",
      "batch 3463: loss 0.000710\n",
      "batch 3464: loss 0.068035\n",
      "batch 3465: loss 0.109889\n",
      "batch 3466: loss 0.011423\n",
      "batch 3467: loss 0.003784\n",
      "batch 3468: loss 0.036272\n",
      "batch 3469: loss 0.002385\n",
      "batch 3470: loss 0.095537\n",
      "batch 3471: loss 0.018865\n",
      "batch 3472: loss 0.001252\n",
      "batch 3473: loss 0.000641\n",
      "batch 3474: loss 0.097296\n",
      "batch 3475: loss 0.038415\n",
      "batch 3476: loss 0.018016\n",
      "batch 3477: loss 0.023216\n",
      "batch 3478: loss 0.002817\n",
      "batch 3479: loss 0.096299\n",
      "batch 3480: loss 0.003389\n",
      "batch 3481: loss 0.030962\n",
      "batch 3482: loss 0.034243\n",
      "batch 3483: loss 0.002884\n",
      "batch 3484: loss 0.004955\n",
      "batch 3485: loss 0.005948\n",
      "batch 3486: loss 0.001550\n",
      "batch 3487: loss 0.003819\n",
      "batch 3488: loss 0.006574\n",
      "batch 3489: loss 0.005855\n",
      "batch 3490: loss 0.027129\n",
      "batch 3491: loss 0.044093\n",
      "batch 3492: loss 0.002063\n",
      "batch 3493: loss 0.005933\n",
      "batch 3494: loss 0.004849\n",
      "batch 3495: loss 0.004250\n",
      "batch 3496: loss 0.021374\n",
      "batch 3497: loss 0.084302\n",
      "batch 3498: loss 0.006584\n",
      "batch 3499: loss 0.002811\n",
      "batch 3500: loss 0.001031\n",
      "batch 3501: loss 0.000312\n",
      "batch 3502: loss 0.000703\n",
      "batch 3503: loss 0.000344\n",
      "batch 3504: loss 0.002519\n",
      "batch 3505: loss 0.057150\n",
      "batch 3506: loss 0.006097\n",
      "batch 3507: loss 0.006259\n",
      "batch 3508: loss 0.152119\n",
      "batch 3509: loss 0.001766\n",
      "batch 3510: loss 0.038181\n",
      "batch 3511: loss 0.001033\n",
      "batch 3512: loss 0.003097\n",
      "batch 3513: loss 0.001254\n",
      "batch 3514: loss 0.000755\n",
      "batch 3515: loss 0.000491\n",
      "batch 3516: loss 0.001731\n",
      "batch 3517: loss 0.000656\n",
      "batch 3518: loss 0.001848\n",
      "batch 3519: loss 0.002364\n",
      "batch 3520: loss 0.029007\n",
      "batch 3521: loss 0.003864\n",
      "batch 3522: loss 0.001151\n",
      "batch 3523: loss 0.000227\n",
      "batch 3524: loss 0.006274\n",
      "batch 3525: loss 0.016641\n",
      "batch 3526: loss 0.004276\n",
      "batch 3527: loss 0.001978\n",
      "batch 3528: loss 0.015157\n",
      "batch 3529: loss 0.062913\n",
      "batch 3530: loss 0.000245\n",
      "batch 3531: loss 0.009948\n",
      "batch 3532: loss 0.000941\n",
      "batch 3533: loss 0.001155\n",
      "batch 3534: loss 0.028890\n",
      "batch 3535: loss 0.001839\n",
      "batch 3536: loss 0.001408\n",
      "batch 3537: loss 0.041322\n",
      "batch 3538: loss 0.002488\n",
      "batch 3539: loss 0.005813\n",
      "batch 3540: loss 0.001594\n",
      "batch 3541: loss 0.001606\n",
      "batch 3542: loss 0.003437\n",
      "batch 3543: loss 0.009093\n",
      "batch 3544: loss 0.003123\n",
      "batch 3545: loss 0.002766\n",
      "batch 3546: loss 0.005522\n",
      "batch 3547: loss 0.096024\n",
      "batch 3548: loss 0.033489\n",
      "batch 3549: loss 0.001958\n",
      "batch 3550: loss 0.018640\n",
      "batch 3551: loss 0.021130\n",
      "batch 3552: loss 0.010000\n",
      "batch 3553: loss 0.003017\n",
      "batch 3554: loss 0.005183\n",
      "batch 3555: loss 0.040799\n",
      "batch 3556: loss 0.002044\n",
      "batch 3557: loss 0.007587\n",
      "batch 3558: loss 0.011750\n",
      "batch 3559: loss 0.001542\n",
      "batch 3560: loss 0.004079\n",
      "batch 3561: loss 0.000927\n",
      "batch 3562: loss 0.001383\n",
      "batch 3563: loss 0.001529\n",
      "batch 3564: loss 0.001209\n",
      "batch 3565: loss 0.002758\n",
      "batch 3566: loss 0.007017\n",
      "batch 3567: loss 0.001438\n",
      "batch 3568: loss 0.066442\n",
      "batch 3569: loss 0.000979\n",
      "batch 3570: loss 0.000586\n",
      "batch 3571: loss 0.000389\n",
      "batch 3572: loss 0.048933\n",
      "batch 3573: loss 0.031843\n",
      "batch 3574: loss 0.001900\n",
      "batch 3575: loss 0.028160\n",
      "batch 3576: loss 0.004034\n",
      "batch 3577: loss 0.004498\n",
      "batch 3578: loss 0.000505\n",
      "batch 3579: loss 0.000501\n",
      "batch 3580: loss 0.081088\n",
      "batch 3581: loss 0.001801\n",
      "batch 3582: loss 0.005122\n",
      "batch 3583: loss 0.120075\n",
      "batch 3584: loss 0.068532\n",
      "batch 3585: loss 0.000200\n",
      "batch 3586: loss 0.004615\n",
      "batch 3587: loss 0.000804\n",
      "batch 3588: loss 0.015360\n",
      "batch 3589: loss 0.000485\n",
      "batch 3590: loss 0.063810\n",
      "batch 3591: loss 0.000611\n",
      "batch 3592: loss 0.004782\n",
      "batch 3593: loss 0.028264\n",
      "batch 3594: loss 0.034385\n",
      "batch 3595: loss 0.001587\n",
      "batch 3596: loss 0.007717\n",
      "batch 3597: loss 0.004482\n",
      "batch 3598: loss 0.008083\n",
      "batch 3599: loss 0.008142\n",
      "batch 3600: loss 0.002242\n",
      "batch 3601: loss 0.007422\n",
      "batch 3602: loss 0.023124\n",
      "batch 3603: loss 0.006971\n",
      "batch 3604: loss 0.018832\n",
      "batch 3605: loss 0.066376\n",
      "batch 3606: loss 0.034803\n",
      "batch 3607: loss 0.018070\n",
      "batch 3608: loss 0.000389\n",
      "batch 3609: loss 0.002136\n",
      "batch 3610: loss 0.009127\n",
      "batch 3611: loss 0.000477\n",
      "batch 3612: loss 0.000383\n",
      "batch 3613: loss 0.002839\n",
      "batch 3614: loss 0.000359\n",
      "batch 3615: loss 0.002385\n",
      "batch 3616: loss 0.000788\n",
      "batch 3617: loss 0.008642\n",
      "batch 3618: loss 0.007309\n",
      "batch 3619: loss 0.002716\n",
      "batch 3620: loss 0.001900\n",
      "batch 3621: loss 0.000983\n",
      "batch 3622: loss 0.003721\n",
      "batch 3623: loss 0.008576\n",
      "batch 3624: loss 0.000138\n",
      "batch 3625: loss 0.037386\n",
      "batch 3626: loss 0.000535\n",
      "batch 3627: loss 0.009488\n",
      "batch 3628: loss 0.000739\n",
      "batch 3629: loss 0.010184\n",
      "batch 3630: loss 0.000460\n",
      "batch 3631: loss 0.000274\n",
      "batch 3632: loss 0.042783\n",
      "batch 3633: loss 0.001953\n",
      "batch 3634: loss 0.010975\n",
      "batch 3635: loss 0.043793\n",
      "batch 3636: loss 0.009657\n",
      "batch 3637: loss 0.001409\n",
      "batch 3638: loss 0.014079\n",
      "batch 3639: loss 0.018172\n",
      "batch 3640: loss 0.002074\n",
      "batch 3641: loss 0.002278\n",
      "batch 3642: loss 0.003528\n",
      "batch 3643: loss 0.003257\n",
      "batch 3644: loss 0.077463\n",
      "batch 3645: loss 0.007111\n",
      "batch 3646: loss 0.001476\n",
      "batch 3647: loss 0.075957\n",
      "batch 3648: loss 0.001000\n",
      "batch 3649: loss 0.000410\n",
      "batch 3650: loss 0.005206\n",
      "batch 3651: loss 0.010802\n",
      "batch 3652: loss 0.003417\n",
      "batch 3653: loss 0.003540\n",
      "batch 3654: loss 0.037197\n",
      "batch 3655: loss 0.077925\n",
      "batch 3656: loss 0.000050\n",
      "batch 3657: loss 0.000021\n",
      "batch 3658: loss 0.002175\n",
      "batch 3659: loss 0.007437\n",
      "batch 3660: loss 0.022743\n",
      "batch 3661: loss 0.000735\n",
      "batch 3662: loss 0.018862\n",
      "batch 3663: loss 0.000307\n",
      "batch 3664: loss 0.027340\n",
      "batch 3665: loss 0.003082\n",
      "batch 3666: loss 0.000130\n",
      "batch 3667: loss 0.004193\n",
      "batch 3668: loss 0.004284\n",
      "batch 3669: loss 0.024346\n",
      "batch 3670: loss 0.002398\n",
      "batch 3671: loss 0.000107\n",
      "batch 3672: loss 0.005362\n",
      "batch 3673: loss 0.000493\n",
      "batch 3674: loss 0.019521\n",
      "batch 3675: loss 0.000660\n",
      "batch 3676: loss 0.000713\n",
      "batch 3677: loss 0.003865\n",
      "batch 3678: loss 0.000803\n",
      "batch 3679: loss 0.001844\n",
      "batch 3680: loss 0.001597\n",
      "batch 3681: loss 0.000651\n",
      "batch 3682: loss 0.035281\n",
      "batch 3683: loss 0.001316\n",
      "batch 3684: loss 0.000212\n",
      "batch 3685: loss 0.000112\n",
      "batch 3686: loss 0.051210\n",
      "batch 3687: loss 0.000357\n",
      "batch 3688: loss 0.004533\n",
      "batch 3689: loss 0.046207\n",
      "batch 3690: loss 0.011518\n",
      "batch 3691: loss 0.017769\n",
      "batch 3692: loss 0.003999\n",
      "batch 3693: loss 0.067008\n",
      "batch 3694: loss 0.016019\n",
      "batch 3695: loss 0.091022\n",
      "batch 3696: loss 0.001497\n",
      "batch 3697: loss 0.000242\n",
      "batch 3698: loss 0.005114\n",
      "batch 3699: loss 0.000699\n",
      "batch 3700: loss 0.000134\n",
      "batch 3701: loss 0.005303\n",
      "batch 3702: loss 0.007045\n",
      "batch 3703: loss 0.002960\n",
      "batch 3704: loss 0.107542\n",
      "batch 3705: loss 0.003489\n",
      "batch 3706: loss 0.036373\n",
      "batch 3707: loss 0.000238\n",
      "batch 3708: loss 0.082262\n",
      "batch 3709: loss 0.025223\n",
      "batch 3710: loss 0.005385\n",
      "batch 3711: loss 0.000102\n",
      "batch 3712: loss 0.031167\n",
      "batch 3713: loss 0.007747\n",
      "batch 3714: loss 0.005695\n",
      "batch 3715: loss 0.000409\n",
      "batch 3716: loss 0.068441\n",
      "batch 3717: loss 0.120572\n",
      "batch 3718: loss 0.009009\n",
      "batch 3719: loss 0.002650\n",
      "batch 3720: loss 0.114413\n",
      "batch 3721: loss 0.035120\n",
      "batch 3722: loss 0.041237\n",
      "batch 3723: loss 0.002737\n",
      "batch 3724: loss 0.011050\n",
      "batch 3725: loss 0.000692\n",
      "batch 3726: loss 0.000208\n",
      "batch 3727: loss 0.002364\n",
      "batch 3728: loss 0.000093\n",
      "batch 3729: loss 0.003437\n",
      "batch 3730: loss 0.027829\n",
      "batch 3731: loss 0.004406\n",
      "batch 3732: loss 0.025418\n",
      "batch 3733: loss 0.005373\n",
      "batch 3734: loss 0.035997\n",
      "batch 3735: loss 0.020252\n",
      "batch 3736: loss 0.000778\n",
      "batch 3737: loss 0.000139\n",
      "batch 3738: loss 0.002331\n",
      "batch 3739: loss 0.002385\n",
      "batch 3740: loss 0.012661\n",
      "batch 3741: loss 0.006846\n",
      "batch 3742: loss 0.066914\n",
      "batch 3743: loss 0.000559\n",
      "batch 3744: loss 0.022400\n",
      "batch 3745: loss 0.005612\n",
      "batch 3746: loss 0.000617\n",
      "batch 3747: loss 0.001121\n",
      "batch 3748: loss 0.001028\n",
      "batch 3749: loss 0.030552\n",
      "batch 3750: loss 0.016756\n",
      "batch 3751: loss 0.009819\n",
      "batch 3752: loss 0.089613\n",
      "batch 3753: loss 0.122314\n",
      "batch 3754: loss 0.034556\n",
      "batch 3755: loss 0.003195\n",
      "batch 3756: loss 0.002009\n",
      "batch 3757: loss 0.075736\n",
      "batch 3758: loss 0.000452\n",
      "batch 3759: loss 0.032336\n",
      "batch 3760: loss 0.039554\n",
      "batch 3761: loss 0.004057\n",
      "batch 3762: loss 0.049808\n",
      "batch 3763: loss 0.001491\n",
      "batch 3764: loss 0.000168\n",
      "batch 3765: loss 0.005664\n",
      "batch 3766: loss 0.051522\n",
      "batch 3767: loss 0.000286\n",
      "batch 3768: loss 0.029724\n",
      "batch 3769: loss 0.050670\n",
      "batch 3770: loss 0.046011\n",
      "batch 3771: loss 0.128926\n",
      "batch 3772: loss 0.035926\n",
      "batch 3773: loss 0.002396\n",
      "batch 3774: loss 0.020546\n",
      "batch 3775: loss 0.029165\n",
      "batch 3776: loss 0.038181\n",
      "batch 3777: loss 0.034387\n",
      "batch 3778: loss 0.007877\n",
      "batch 3779: loss 0.021258\n",
      "batch 3780: loss 0.003162\n",
      "batch 3781: loss 0.026789\n",
      "batch 3782: loss 0.006932\n",
      "batch 3783: loss 0.001827\n",
      "batch 3784: loss 0.065656\n",
      "batch 3785: loss 0.033169\n",
      "batch 3786: loss 0.008113\n",
      "batch 3787: loss 0.003234\n",
      "batch 3788: loss 0.001690\n",
      "batch 3789: loss 0.016601\n",
      "batch 3790: loss 0.006172\n",
      "batch 3791: loss 0.015223\n",
      "batch 3792: loss 0.003498\n",
      "batch 3793: loss 0.032291\n",
      "batch 3794: loss 0.000539\n",
      "batch 3795: loss 0.015121\n",
      "batch 3796: loss 0.016849\n",
      "batch 3797: loss 0.040882\n",
      "batch 3798: loss 0.000396\n",
      "batch 3799: loss 0.000600\n",
      "batch 3800: loss 0.020685\n",
      "batch 3801: loss 0.007073\n",
      "batch 3802: loss 0.000508\n",
      "batch 3803: loss 0.028640\n",
      "batch 3804: loss 0.004671\n",
      "batch 3805: loss 0.012893\n",
      "batch 3806: loss 0.000492\n",
      "batch 3807: loss 0.007993\n",
      "batch 3808: loss 0.002152\n",
      "batch 3809: loss 0.000689\n",
      "batch 3810: loss 0.000919\n",
      "batch 3811: loss 0.000481\n",
      "batch 3812: loss 0.006846\n",
      "batch 3813: loss 0.001456\n",
      "batch 3814: loss 0.013273\n",
      "batch 3815: loss 0.000438\n",
      "batch 3816: loss 0.017493\n",
      "batch 3817: loss 0.000046\n",
      "batch 3818: loss 0.024760\n",
      "batch 3819: loss 0.000881\n",
      "batch 3820: loss 0.000149\n",
      "batch 3821: loss 0.000775\n",
      "batch 3822: loss 0.004538\n",
      "batch 3823: loss 0.002468\n",
      "batch 3824: loss 0.000663\n",
      "batch 3825: loss 0.000066\n",
      "batch 3826: loss 0.000872\n",
      "batch 3827: loss 0.005371\n",
      "batch 3828: loss 0.010891\n",
      "batch 3829: loss 0.000665\n",
      "batch 3830: loss 0.011643\n",
      "batch 3831: loss 0.001191\n",
      "batch 3832: loss 0.002035\n",
      "batch 3833: loss 0.001989\n",
      "batch 3834: loss 0.005641\n",
      "batch 3835: loss 0.016076\n",
      "batch 3836: loss 0.004086\n",
      "batch 3837: loss 0.002362\n",
      "batch 3838: loss 0.003046\n",
      "batch 3839: loss 0.000622\n",
      "batch 3840: loss 0.003166\n",
      "batch 3841: loss 0.006113\n",
      "batch 3842: loss 0.000163\n",
      "batch 3843: loss 0.004312\n",
      "batch 3844: loss 0.001799\n",
      "batch 3845: loss 0.030277\n",
      "batch 3846: loss 0.011790\n",
      "batch 3847: loss 0.020990\n",
      "batch 3848: loss 0.018157\n",
      "batch 3849: loss 0.000119\n",
      "batch 3850: loss 0.000065\n",
      "batch 3851: loss 0.008813\n",
      "batch 3852: loss 0.001936\n",
      "batch 3853: loss 0.006964\n",
      "batch 3854: loss 0.002063\n",
      "batch 3855: loss 0.009640\n",
      "batch 3856: loss 0.000202\n",
      "batch 3857: loss 0.046280\n",
      "batch 3858: loss 0.002220\n",
      "batch 3859: loss 0.000255\n",
      "batch 3860: loss 0.040434\n",
      "batch 3861: loss 0.000274\n",
      "batch 3862: loss 0.038028\n",
      "batch 3863: loss 0.000519\n",
      "batch 3864: loss 0.000021\n",
      "batch 3865: loss 0.002075\n",
      "batch 3866: loss 0.172739\n",
      "batch 3867: loss 0.004250\n",
      "batch 3868: loss 0.000014\n",
      "batch 3869: loss 0.001587\n",
      "batch 3870: loss 0.129614\n",
      "batch 3871: loss 0.037243\n",
      "batch 3872: loss 0.008233\n",
      "batch 3873: loss 0.003710\n",
      "batch 3874: loss 0.046159\n",
      "batch 3875: loss 0.000100\n",
      "batch 3876: loss 0.004003\n",
      "batch 3877: loss 0.154465\n",
      "batch 3878: loss 0.043454\n",
      "batch 3879: loss 0.007890\n",
      "batch 3880: loss 0.014041\n",
      "batch 3881: loss 0.034639\n",
      "batch 3882: loss 0.017153\n",
      "batch 3883: loss 0.000249\n",
      "batch 3884: loss 0.000240\n",
      "batch 3885: loss 0.020315\n",
      "batch 3886: loss 0.000348\n",
      "batch 3887: loss 0.001999\n",
      "batch 3888: loss 0.007767\n",
      "batch 3889: loss 0.000260\n",
      "batch 3890: loss 0.005995\n",
      "batch 3891: loss 0.000840\n",
      "batch 3892: loss 0.170110\n",
      "batch 3893: loss 0.000498\n",
      "batch 3894: loss 0.000188\n",
      "batch 3895: loss 0.020859\n",
      "batch 3896: loss 0.037523\n",
      "batch 3897: loss 0.040166\n",
      "batch 3898: loss 0.004432\n",
      "batch 3899: loss 0.002523\n",
      "batch 3900: loss 0.050823\n",
      "batch 3901: loss 0.002583\n",
      "batch 3902: loss 0.000651\n",
      "batch 3903: loss 0.003281\n",
      "batch 3904: loss 0.010035\n",
      "batch 3905: loss 0.006998\n",
      "batch 3906: loss 0.002139\n",
      "batch 3907: loss 0.015367\n",
      "batch 3908: loss 0.001864\n",
      "batch 3909: loss 0.053538\n",
      "batch 3910: loss 0.019777\n",
      "batch 3911: loss 0.022772\n",
      "batch 3912: loss 0.126054\n",
      "batch 3913: loss 0.003998\n",
      "batch 3914: loss 0.029566\n",
      "batch 3915: loss 0.007177\n",
      "batch 3916: loss 0.004834\n",
      "batch 3917: loss 0.004252\n",
      "batch 3918: loss 0.057352\n",
      "batch 3919: loss 0.024734\n",
      "batch 3920: loss 0.001243\n",
      "batch 3921: loss 0.009702\n",
      "batch 3922: loss 0.000355\n",
      "batch 3923: loss 0.005280\n",
      "batch 3924: loss 0.003716\n",
      "batch 3925: loss 0.002599\n",
      "batch 3926: loss 0.011021\n",
      "batch 3927: loss 0.001770\n",
      "batch 3928: loss 0.016834\n",
      "batch 3929: loss 0.016385\n",
      "batch 3930: loss 0.002065\n",
      "batch 3931: loss 0.002431\n",
      "batch 3932: loss 0.002169\n",
      "batch 3933: loss 0.074959\n",
      "batch 3934: loss 0.005830\n",
      "batch 3935: loss 0.002430\n",
      "batch 3936: loss 0.000822\n",
      "batch 3937: loss 0.072248\n",
      "batch 3938: loss 0.004772\n",
      "batch 3939: loss 0.000269\n",
      "batch 3940: loss 0.014621\n",
      "batch 3941: loss 0.072972\n",
      "batch 3942: loss 0.000310\n",
      "batch 3943: loss 0.000185\n",
      "batch 3944: loss 0.000064\n",
      "batch 3945: loss 0.000829\n",
      "batch 3946: loss 0.008155\n",
      "batch 3947: loss 0.000973\n",
      "batch 3948: loss 0.000193\n",
      "batch 3949: loss 0.005833\n",
      "batch 3950: loss 0.019805\n",
      "batch 3951: loss 0.000652\n",
      "batch 3952: loss 0.001564\n",
      "batch 3953: loss 0.062773\n",
      "batch 3954: loss 0.010878\n",
      "batch 3955: loss 0.058729\n",
      "batch 3956: loss 0.038209\n",
      "batch 3957: loss 0.000510\n",
      "batch 3958: loss 0.005281\n",
      "batch 3959: loss 0.001762\n",
      "batch 3960: loss 0.041429\n",
      "batch 3961: loss 0.000629\n",
      "batch 3962: loss 0.007642\n",
      "batch 3963: loss 0.044738\n",
      "batch 3964: loss 0.038715\n",
      "batch 3965: loss 0.015923\n",
      "batch 3966: loss 0.000327\n",
      "batch 3967: loss 0.000011\n",
      "batch 3968: loss 0.071154\n",
      "batch 3969: loss 0.000212\n",
      "batch 3970: loss 0.000560\n",
      "batch 3971: loss 0.000296\n",
      "batch 3972: loss 0.000311\n",
      "batch 3973: loss 0.017204\n",
      "batch 3974: loss 0.030618\n",
      "batch 3975: loss 0.051879\n",
      "batch 3976: loss 0.000177\n",
      "batch 3977: loss 0.059327\n",
      "batch 3978: loss 0.028460\n",
      "batch 3979: loss 0.000639\n",
      "batch 3980: loss 0.000281\n",
      "batch 3981: loss 0.008667\n",
      "batch 3982: loss 0.000214\n",
      "batch 3983: loss 0.003355\n",
      "batch 3984: loss 0.063930\n",
      "batch 3985: loss 0.000289\n",
      "batch 3986: loss 0.000095\n",
      "batch 3987: loss 0.000659\n",
      "batch 3988: loss 0.000031\n",
      "batch 3989: loss 0.000716\n",
      "batch 3990: loss 0.001658\n",
      "batch 3991: loss 0.002882\n",
      "batch 3992: loss 0.000261\n",
      "batch 3993: loss 0.004404\n",
      "batch 3994: loss 0.000074\n",
      "batch 3995: loss 0.064429\n",
      "batch 3996: loss 0.001099\n",
      "batch 3997: loss 0.000352\n",
      "batch 3998: loss 0.002215\n",
      "batch 3999: loss 0.000194\n",
      "batch 4000: loss 0.000422\n",
      "batch 4001: loss 0.022825\n",
      "batch 4002: loss 0.003879\n",
      "batch 4003: loss 0.003697\n",
      "batch 4004: loss 0.008812\n",
      "batch 4005: loss 0.017520\n",
      "batch 4006: loss 0.076175\n",
      "batch 4007: loss 0.028857\n",
      "batch 4008: loss 0.000835\n",
      "batch 4009: loss 0.078636\n",
      "batch 4010: loss 0.006572\n",
      "batch 4011: loss 0.003568\n",
      "batch 4012: loss 0.001578\n",
      "batch 4013: loss 0.000011\n",
      "batch 4014: loss 0.011786\n",
      "batch 4015: loss 0.000293\n",
      "batch 4016: loss 0.000287\n",
      "batch 4017: loss 0.000323\n",
      "batch 4018: loss 0.011384\n",
      "batch 4019: loss 0.000492\n",
      "batch 4020: loss 0.056629\n",
      "batch 4021: loss 0.071955\n",
      "batch 4022: loss 0.003739\n",
      "batch 4023: loss 0.063162\n",
      "batch 4024: loss 0.007381\n",
      "batch 4025: loss 0.009148\n",
      "batch 4026: loss 0.021058\n",
      "batch 4027: loss 0.001971\n",
      "batch 4028: loss 0.060946\n",
      "batch 4029: loss 0.004756\n",
      "batch 4030: loss 0.019901\n",
      "batch 4031: loss 0.003144\n",
      "batch 4032: loss 0.000489\n",
      "batch 4033: loss 0.002993\n",
      "batch 4034: loss 0.000577\n",
      "batch 4035: loss 0.010773\n",
      "batch 4036: loss 0.000967\n",
      "batch 4037: loss 0.000276\n",
      "batch 4038: loss 0.030094\n",
      "batch 4039: loss 0.002376\n",
      "batch 4040: loss 0.000250\n",
      "batch 4041: loss 0.004424\n",
      "batch 4042: loss 0.043313\n",
      "batch 4043: loss 0.178310\n",
      "batch 4044: loss 0.009585\n",
      "batch 4045: loss 0.001403\n",
      "batch 4046: loss 0.005459\n",
      "batch 4047: loss 0.039492\n",
      "batch 4048: loss 0.038739\n",
      "batch 4049: loss 0.005026\n",
      "batch 4050: loss 0.036031\n",
      "batch 4051: loss 0.008090\n",
      "batch 4052: loss 0.001091\n",
      "batch 4053: loss 0.000510\n",
      "batch 4054: loss 0.000268\n",
      "batch 4055: loss 0.002259\n",
      "batch 4056: loss 0.035111\n",
      "batch 4057: loss 0.000205\n",
      "batch 4058: loss 0.012721\n",
      "batch 4059: loss 0.013776\n",
      "batch 4060: loss 0.000125\n",
      "batch 4061: loss 0.055530\n",
      "batch 4062: loss 0.006556\n",
      "batch 4063: loss 0.000313\n",
      "batch 4064: loss 0.002532\n",
      "batch 4065: loss 0.057512\n",
      "batch 4066: loss 0.003129\n",
      "batch 4067: loss 0.002136\n",
      "batch 4068: loss 0.004898\n",
      "batch 4069: loss 0.000105\n",
      "batch 4070: loss 0.000238\n",
      "batch 4071: loss 0.000112\n",
      "batch 4072: loss 0.000726\n",
      "batch 4073: loss 0.000790\n",
      "batch 4074: loss 0.000086\n",
      "batch 4075: loss 0.000276\n",
      "batch 4076: loss 0.005685\n",
      "batch 4077: loss 0.002258\n",
      "batch 4078: loss 0.000071\n",
      "batch 4079: loss 0.000077\n",
      "batch 4080: loss 0.026008\n",
      "batch 4081: loss 0.054441\n",
      "batch 4082: loss 0.002565\n",
      "batch 4083: loss 0.000738\n",
      "batch 4084: loss 0.000731\n",
      "batch 4085: loss 0.001229\n",
      "batch 4086: loss 0.001886\n",
      "batch 4087: loss 0.024451\n",
      "batch 4088: loss 0.002059\n",
      "batch 4089: loss 0.000084\n",
      "batch 4090: loss 0.000237\n",
      "batch 4091: loss 0.001910\n",
      "batch 4092: loss 0.001699\n",
      "batch 4093: loss 0.001423\n",
      "batch 4094: loss 0.036555\n",
      "batch 4095: loss 0.000753\n",
      "batch 4096: loss 0.002009\n",
      "batch 4097: loss 0.019255\n",
      "batch 4098: loss 0.143879\n",
      "batch 4099: loss 0.002590\n",
      "batch 4100: loss 0.009741\n",
      "batch 4101: loss 0.062404\n",
      "batch 4102: loss 0.041336\n",
      "batch 4103: loss 0.010150\n",
      "batch 4104: loss 0.000548\n",
      "batch 4105: loss 0.002248\n",
      "batch 4106: loss 0.000258\n",
      "batch 4107: loss 0.000486\n",
      "batch 4108: loss 0.031607\n",
      "batch 4109: loss 0.011274\n",
      "batch 4110: loss 0.002561\n",
      "batch 4111: loss 0.002609\n",
      "batch 4112: loss 0.009020\n",
      "batch 4113: loss 0.022458\n",
      "batch 4114: loss 0.000128\n",
      "batch 4115: loss 0.000270\n",
      "batch 4116: loss 0.001276\n",
      "batch 4117: loss 0.000396\n",
      "batch 4118: loss 0.000187\n",
      "batch 4119: loss 0.002542\n",
      "batch 4120: loss 0.006953\n",
      "batch 4121: loss 0.000625\n",
      "batch 4122: loss 0.000458\n",
      "batch 4123: loss 0.000917\n",
      "batch 4124: loss 0.000097\n",
      "batch 4125: loss 0.000623\n",
      "batch 4126: loss 0.000470\n",
      "batch 4127: loss 0.000549\n",
      "batch 4128: loss 0.002482\n",
      "batch 4129: loss 0.000647\n",
      "batch 4130: loss 0.000850\n",
      "batch 4131: loss 0.004902\n",
      "batch 4132: loss 0.001576\n",
      "batch 4133: loss 0.000030\n",
      "batch 4134: loss 0.037382\n",
      "batch 4135: loss 0.161682\n",
      "batch 4136: loss 0.000099\n",
      "batch 4137: loss 0.002353\n",
      "batch 4138: loss 0.029905\n",
      "batch 4139: loss 0.035572\n",
      "batch 4140: loss 0.001422\n",
      "batch 4141: loss 0.001053\n",
      "batch 4142: loss 0.001298\n",
      "batch 4143: loss 0.002451\n",
      "batch 4144: loss 0.000168\n",
      "batch 4145: loss 0.002442\n",
      "batch 4146: loss 0.000105\n",
      "batch 4147: loss 0.000901\n",
      "batch 4148: loss 0.000222\n",
      "batch 4149: loss 0.001355\n",
      "batch 4150: loss 0.128911\n",
      "batch 4151: loss 0.000264\n",
      "batch 4152: loss 0.000176\n",
      "batch 4153: loss 0.000295\n",
      "batch 4154: loss 0.000538\n",
      "batch 4155: loss 0.004582\n",
      "batch 4156: loss 0.000420\n",
      "batch 4157: loss 0.006278\n",
      "batch 4158: loss 0.004388\n",
      "batch 4159: loss 0.164045\n",
      "batch 4160: loss 0.000193\n",
      "batch 4161: loss 0.000701\n",
      "batch 4162: loss 0.049651\n",
      "batch 4163: loss 0.003671\n",
      "batch 4164: loss 0.000991\n",
      "batch 4165: loss 0.014301\n",
      "batch 4166: loss 0.000936\n",
      "batch 4167: loss 0.066904\n",
      "batch 4168: loss 0.014284\n",
      "batch 4169: loss 0.003529\n",
      "batch 4170: loss 0.022069\n",
      "batch 4171: loss 0.007051\n",
      "batch 4172: loss 0.002628\n",
      "batch 4173: loss 0.000823\n",
      "batch 4174: loss 0.000271\n",
      "batch 4175: loss 0.021726\n",
      "batch 4176: loss 0.000496\n",
      "batch 4177: loss 0.005068\n",
      "batch 4178: loss 0.064392\n",
      "batch 4179: loss 0.000513\n",
      "batch 4180: loss 0.024165\n",
      "batch 4181: loss 0.002690\n",
      "batch 4182: loss 0.016703\n",
      "batch 4183: loss 0.009161\n",
      "batch 4184: loss 0.017555\n",
      "batch 4185: loss 0.000652\n",
      "batch 4186: loss 0.009106\n",
      "batch 4187: loss 0.000313\n",
      "batch 4188: loss 0.013190\n",
      "batch 4189: loss 0.005425\n",
      "batch 4190: loss 0.000882\n",
      "batch 4191: loss 0.004393\n",
      "batch 4192: loss 0.005917\n",
      "batch 4193: loss 0.032143\n",
      "batch 4194: loss 0.091282\n",
      "batch 4195: loss 0.054162\n",
      "batch 4196: loss 0.009419\n",
      "batch 4197: loss 0.021997\n",
      "batch 4198: loss 0.000274\n",
      "batch 4199: loss 0.005393\n",
      "batch 4200: loss 0.005816\n",
      "batch 4201: loss 0.024497\n",
      "batch 4202: loss 0.007496\n",
      "batch 4203: loss 0.013576\n",
      "batch 4204: loss 0.127330\n",
      "batch 4205: loss 0.000627\n",
      "batch 4206: loss 0.000304\n",
      "batch 4207: loss 0.000592\n",
      "batch 4208: loss 0.001710\n",
      "batch 4209: loss 0.000098\n",
      "batch 4210: loss 0.027037\n",
      "batch 4211: loss 0.000699\n",
      "batch 4212: loss 0.014739\n",
      "batch 4213: loss 0.006438\n",
      "batch 4214: loss 0.001724\n",
      "batch 4215: loss 0.004005\n",
      "batch 4216: loss 0.012731\n",
      "batch 4217: loss 0.001403\n",
      "batch 4218: loss 0.001496\n",
      "batch 4219: loss 0.000428\n",
      "batch 4220: loss 0.023159\n",
      "batch 4221: loss 0.000457\n",
      "batch 4222: loss 0.007422\n",
      "batch 4223: loss 0.020275\n",
      "batch 4224: loss 0.000960\n",
      "batch 4225: loss 0.002916\n",
      "batch 4226: loss 0.003639\n",
      "batch 4227: loss 0.000005\n",
      "batch 4228: loss 0.000090\n",
      "batch 4229: loss 0.125445\n",
      "batch 4230: loss 0.001115\n",
      "batch 4231: loss 0.032869\n",
      "batch 4232: loss 0.057931\n",
      "batch 4233: loss 0.433972\n",
      "batch 4234: loss 0.000474\n",
      "batch 4235: loss 0.000132\n",
      "batch 4236: loss 0.003487\n",
      "batch 4237: loss 0.005116\n",
      "batch 4238: loss 0.003684\n",
      "batch 4239: loss 0.001818\n",
      "batch 4240: loss 0.008472\n",
      "batch 4241: loss 0.002852\n",
      "batch 4242: loss 0.084897\n",
      "batch 4243: loss 0.001532\n",
      "batch 4244: loss 0.001792\n",
      "batch 4245: loss 0.000067\n",
      "batch 4246: loss 0.000305\n",
      "batch 4247: loss 0.002558\n",
      "batch 4248: loss 0.005875\n",
      "batch 4249: loss 0.000680\n",
      "batch 4250: loss 0.003981\n",
      "batch 4251: loss 0.004015\n",
      "batch 4252: loss 0.002502\n",
      "batch 4253: loss 0.009371\n",
      "batch 4254: loss 0.083038\n",
      "batch 4255: loss 0.007696\n",
      "batch 4256: loss 0.007546\n",
      "batch 4257: loss 0.047726\n",
      "batch 4258: loss 0.000647\n",
      "batch 4259: loss 0.014033\n",
      "batch 4260: loss 0.000541\n",
      "batch 4261: loss 0.001511\n",
      "batch 4262: loss 0.002437\n",
      "batch 4263: loss 0.000740\n",
      "batch 4264: loss 0.047595\n",
      "batch 4265: loss 0.002651\n",
      "batch 4266: loss 0.031038\n",
      "batch 4267: loss 0.004980\n",
      "batch 4268: loss 0.000484\n",
      "batch 4269: loss 0.004483\n",
      "batch 4270: loss 0.023939\n",
      "batch 4271: loss 0.003457\n",
      "batch 4272: loss 0.025791\n",
      "batch 4273: loss 0.003556\n",
      "batch 4274: loss 0.027500\n",
      "batch 4275: loss 0.008921\n",
      "batch 4276: loss 0.000592\n",
      "batch 4277: loss 0.000041\n",
      "batch 4278: loss 0.027151\n",
      "batch 4279: loss 0.000243\n",
      "batch 4280: loss 0.001678\n",
      "batch 4281: loss 0.002128\n",
      "batch 4282: loss 0.073133\n",
      "batch 4283: loss 0.000294\n",
      "batch 4284: loss 0.000103\n",
      "batch 4285: loss 0.038656\n",
      "batch 4286: loss 0.000171\n",
      "batch 4287: loss 0.000062\n",
      "batch 4288: loss 0.000637\n",
      "batch 4289: loss 0.015009\n",
      "batch 4290: loss 0.000124\n",
      "batch 4291: loss 0.012227\n",
      "batch 4292: loss 0.009383\n",
      "batch 4293: loss 0.047009\n",
      "batch 4294: loss 0.001114\n",
      "batch 4295: loss 0.001617\n",
      "batch 4296: loss 0.000460\n",
      "batch 4297: loss 0.009594\n",
      "batch 4298: loss 0.002741\n",
      "batch 4299: loss 0.153034\n",
      "batch 4300: loss 0.002090\n",
      "batch 4301: loss 0.000085\n",
      "batch 4302: loss 0.001684\n",
      "batch 4303: loss 0.008729\n",
      "batch 4304: loss 0.001391\n",
      "batch 4305: loss 0.006479\n",
      "batch 4306: loss 0.003521\n",
      "batch 4307: loss 0.007691\n",
      "batch 4308: loss 0.009085\n",
      "batch 4309: loss 0.163165\n",
      "batch 4310: loss 0.047494\n",
      "batch 4311: loss 0.000207\n",
      "batch 4312: loss 0.056185\n",
      "batch 4313: loss 0.001328\n",
      "batch 4314: loss 0.000697\n",
      "batch 4315: loss 0.083286\n",
      "batch 4316: loss 0.001107\n",
      "batch 4317: loss 0.007380\n",
      "batch 4318: loss 0.035829\n",
      "batch 4319: loss 0.030871\n",
      "batch 4320: loss 0.012693\n",
      "batch 4321: loss 0.050936\n",
      "batch 4322: loss 0.047146\n",
      "batch 4323: loss 0.001280\n",
      "batch 4324: loss 0.047127\n",
      "batch 4325: loss 0.059307\n",
      "batch 4326: loss 0.005479\n",
      "batch 4327: loss 0.002036\n",
      "batch 4328: loss 0.013533\n",
      "batch 4329: loss 0.019681\n",
      "batch 4330: loss 0.002675\n",
      "batch 4331: loss 0.003200\n",
      "batch 4332: loss 0.003219\n",
      "batch 4333: loss 0.022191\n",
      "batch 4334: loss 0.005876\n",
      "batch 4335: loss 0.001215\n",
      "batch 4336: loss 0.008814\n",
      "batch 4337: loss 0.002453\n",
      "batch 4338: loss 0.002898\n",
      "batch 4339: loss 0.003551\n",
      "batch 4340: loss 0.095716\n",
      "batch 4341: loss 0.000427\n",
      "batch 4342: loss 0.010777\n",
      "batch 4343: loss 0.000099\n",
      "batch 4344: loss 0.011559\n",
      "batch 4345: loss 0.056030\n",
      "batch 4346: loss 0.331856\n",
      "batch 4347: loss 0.028979\n",
      "batch 4348: loss 0.077711\n",
      "batch 4349: loss 0.000193\n",
      "batch 4350: loss 0.066367\n",
      "batch 4351: loss 0.024026\n",
      "batch 4352: loss 0.000452\n",
      "batch 4353: loss 0.003213\n",
      "batch 4354: loss 0.002728\n",
      "batch 4355: loss 0.154187\n",
      "batch 4356: loss 0.000987\n",
      "batch 4357: loss 0.036945\n",
      "batch 4358: loss 0.008465\n",
      "batch 4359: loss 0.000561\n",
      "batch 4360: loss 0.000988\n",
      "batch 4361: loss 0.000419\n",
      "batch 4362: loss 0.003396\n",
      "batch 4363: loss 0.000445\n",
      "batch 4364: loss 0.013099\n",
      "batch 4365: loss 0.003380\n",
      "batch 4366: loss 0.142180\n",
      "batch 4367: loss 0.011277\n",
      "batch 4368: loss 0.001031\n",
      "batch 4369: loss 0.000283\n",
      "batch 4370: loss 0.004171\n",
      "batch 4371: loss 0.000991\n",
      "batch 4372: loss 0.000144\n",
      "batch 4373: loss 0.001102\n",
      "batch 4374: loss 0.002460\n",
      "batch 4375: loss 0.001207\n",
      "batch 4376: loss 0.001193\n",
      "batch 4377: loss 0.005646\n",
      "batch 4378: loss 0.005047\n",
      "batch 4379: loss 0.000988\n",
      "batch 4380: loss 0.001039\n",
      "batch 4381: loss 0.007187\n",
      "batch 4382: loss 0.004099\n",
      "batch 4383: loss 0.014895\n",
      "batch 4384: loss 0.022790\n",
      "batch 4385: loss 0.000492\n",
      "batch 4386: loss 0.018261\n",
      "batch 4387: loss 0.000142\n",
      "batch 4388: loss 0.000530\n",
      "batch 4389: loss 0.003014\n",
      "batch 4390: loss 0.000406\n",
      "batch 4391: loss 0.041937\n",
      "batch 4392: loss 0.002484\n",
      "batch 4393: loss 0.030199\n",
      "batch 4394: loss 0.001607\n",
      "batch 4395: loss 0.000151\n",
      "batch 4396: loss 0.000398\n",
      "batch 4397: loss 0.002210\n",
      "batch 4398: loss 0.000188\n",
      "batch 4399: loss 0.000201\n",
      "batch 4400: loss 0.011418\n",
      "batch 4401: loss 0.004823\n",
      "batch 4402: loss 0.005795\n",
      "batch 4403: loss 0.000318\n",
      "batch 4404: loss 0.003182\n",
      "batch 4405: loss 0.098429\n",
      "batch 4406: loss 0.001686\n",
      "batch 4407: loss 0.004260\n",
      "batch 4408: loss 0.023461\n",
      "batch 4409: loss 0.001204\n",
      "batch 4410: loss 0.010688\n",
      "batch 4411: loss 0.031667\n",
      "batch 4412: loss 0.001209\n",
      "batch 4413: loss 0.006018\n",
      "batch 4414: loss 0.006853\n",
      "batch 4415: loss 0.000327\n",
      "batch 4416: loss 0.001125\n",
      "batch 4417: loss 0.171318\n",
      "batch 4418: loss 0.006396\n",
      "batch 4419: loss 0.022649\n",
      "batch 4420: loss 0.004585\n",
      "batch 4421: loss 0.000967\n",
      "batch 4422: loss 0.014274\n",
      "batch 4423: loss 0.000741\n",
      "batch 4424: loss 0.020269\n",
      "batch 4425: loss 0.000847\n",
      "batch 4426: loss 0.000791\n",
      "batch 4427: loss 0.000369\n",
      "batch 4428: loss 0.046260\n",
      "batch 4429: loss 0.004688\n",
      "batch 4430: loss 0.000898\n",
      "batch 4431: loss 0.000132\n",
      "batch 4432: loss 0.013217\n",
      "batch 4433: loss 0.000935\n",
      "batch 4434: loss 0.000984\n",
      "batch 4435: loss 0.003232\n",
      "batch 4436: loss 0.008112\n",
      "batch 4437: loss 0.004260\n",
      "batch 4438: loss 0.002953\n",
      "batch 4439: loss 0.002556\n",
      "batch 4440: loss 0.048272\n",
      "batch 4441: loss 0.000284\n",
      "batch 4442: loss 0.006266\n",
      "batch 4443: loss 0.008591\n",
      "batch 4444: loss 0.003324\n",
      "batch 4445: loss 0.005909\n",
      "batch 4446: loss 0.000630\n",
      "batch 4447: loss 0.015867\n",
      "batch 4448: loss 0.015012\n",
      "batch 4449: loss 0.023158\n",
      "batch 4450: loss 0.000500\n",
      "batch 4451: loss 0.000725\n",
      "batch 4452: loss 0.002339\n",
      "batch 4453: loss 0.022427\n",
      "batch 4454: loss 0.065554\n",
      "batch 4455: loss 0.027662\n",
      "batch 4456: loss 0.003303\n",
      "batch 4457: loss 0.324653\n",
      "batch 4458: loss 0.003445\n",
      "batch 4459: loss 0.000247\n",
      "batch 4460: loss 0.000998\n",
      "batch 4461: loss 0.021844\n",
      "batch 4462: loss 0.088732\n",
      "batch 4463: loss 0.000770\n",
      "batch 4464: loss 0.000182\n",
      "batch 4465: loss 0.000229\n",
      "batch 4466: loss 0.054488\n",
      "batch 4467: loss 0.027620\n",
      "batch 4468: loss 0.002173\n",
      "batch 4469: loss 0.000217\n",
      "batch 4470: loss 0.003098\n",
      "batch 4471: loss 0.014071\n",
      "batch 4472: loss 0.001964\n",
      "batch 4473: loss 0.061867\n",
      "batch 4474: loss 0.001468\n",
      "batch 4475: loss 0.003195\n",
      "batch 4476: loss 0.000117\n",
      "batch 4477: loss 0.020941\n",
      "batch 4478: loss 0.000377\n",
      "batch 4479: loss 0.002225\n",
      "batch 4480: loss 0.009056\n",
      "batch 4481: loss 0.001049\n",
      "batch 4482: loss 0.002724\n",
      "batch 4483: loss 0.005058\n",
      "batch 4484: loss 0.015103\n",
      "batch 4485: loss 0.007160\n",
      "batch 4486: loss 0.000903\n",
      "batch 4487: loss 0.003131\n",
      "batch 4488: loss 0.004420\n",
      "batch 4489: loss 0.008936\n",
      "batch 4490: loss 0.000074\n",
      "batch 4491: loss 0.003307\n",
      "batch 4492: loss 0.000733\n",
      "batch 4493: loss 0.001093\n",
      "batch 4494: loss 0.002588\n",
      "batch 4495: loss 0.000202\n",
      "batch 4496: loss 0.000395\n",
      "batch 4497: loss 0.000273\n",
      "batch 4498: loss 0.003888\n",
      "batch 4499: loss 0.002168\n",
      "batch 4500: loss 0.009897\n",
      "batch 4501: loss 0.005425\n",
      "batch 4502: loss 0.001459\n",
      "batch 4503: loss 0.020708\n",
      "batch 4504: loss 0.014988\n",
      "batch 4505: loss 0.003878\n",
      "batch 4506: loss 0.000161\n",
      "batch 4507: loss 0.014761\n",
      "batch 4508: loss 0.001468\n",
      "batch 4509: loss 0.024639\n",
      "batch 4510: loss 0.002296\n",
      "batch 4511: loss 0.000621\n",
      "batch 4512: loss 0.000335\n",
      "batch 4513: loss 0.005548\n",
      "batch 4514: loss 0.000524\n",
      "batch 4515: loss 0.003484\n",
      "batch 4516: loss 0.002012\n",
      "batch 4517: loss 0.000076\n",
      "batch 4518: loss 0.002718\n",
      "batch 4519: loss 0.000209\n",
      "batch 4520: loss 0.000243\n",
      "batch 4521: loss 0.000547\n",
      "batch 4522: loss 0.083038\n",
      "batch 4523: loss 0.003831\n",
      "batch 4524: loss 0.000150\n",
      "batch 4525: loss 0.001665\n",
      "batch 4526: loss 0.000996\n",
      "batch 4527: loss 0.000090\n",
      "batch 4528: loss 0.001451\n",
      "batch 4529: loss 0.055110\n",
      "batch 4530: loss 0.000086\n",
      "batch 4531: loss 0.000121\n",
      "batch 4532: loss 0.000062\n",
      "batch 4533: loss 0.002323\n",
      "batch 4534: loss 0.011386\n",
      "batch 4535: loss 0.000321\n",
      "batch 4536: loss 0.000255\n",
      "batch 4537: loss 0.000132\n",
      "batch 4538: loss 0.002971\n",
      "batch 4539: loss 0.000464\n",
      "batch 4540: loss 0.000337\n",
      "batch 4541: loss 0.000886\n",
      "batch 4542: loss 0.103683\n",
      "batch 4543: loss 0.015214\n",
      "batch 4544: loss 0.003904\n",
      "batch 4545: loss 0.002244\n",
      "batch 4546: loss 0.002477\n",
      "batch 4547: loss 0.000105\n",
      "batch 4548: loss 0.000051\n",
      "batch 4549: loss 0.000785\n",
      "batch 4550: loss 0.009004\n",
      "batch 4551: loss 0.071750\n",
      "batch 4552: loss 0.005580\n",
      "batch 4553: loss 0.000690\n",
      "batch 4554: loss 0.000886\n",
      "batch 4555: loss 0.012159\n",
      "batch 4556: loss 0.001907\n",
      "batch 4557: loss 0.000675\n",
      "batch 4558: loss 0.003421\n",
      "batch 4559: loss 0.014011\n",
      "batch 4560: loss 0.003475\n",
      "batch 4561: loss 0.170455\n",
      "batch 4562: loss 0.072217\n",
      "batch 4563: loss 0.012279\n",
      "batch 4564: loss 0.000306\n",
      "batch 4565: loss 0.002073\n",
      "batch 4566: loss 0.009450\n",
      "batch 4567: loss 0.009684\n",
      "batch 4568: loss 0.000306\n",
      "batch 4569: loss 0.030690\n",
      "batch 4570: loss 0.000018\n",
      "batch 4571: loss 0.002794\n",
      "batch 4572: loss 0.023286\n",
      "batch 4573: loss 0.097997\n",
      "batch 4574: loss 0.001432\n",
      "batch 4575: loss 0.000113\n",
      "batch 4576: loss 0.009754\n",
      "batch 4577: loss 0.021404\n",
      "batch 4578: loss 0.000403\n",
      "batch 4579: loss 0.013158\n",
      "batch 4580: loss 0.004299\n",
      "batch 4581: loss 0.005944\n",
      "batch 4582: loss 0.082457\n",
      "batch 4583: loss 0.016777\n",
      "batch 4584: loss 0.004089\n",
      "batch 4585: loss 0.071853\n",
      "batch 4586: loss 0.002688\n",
      "batch 4587: loss 0.109402\n",
      "batch 4588: loss 0.190777\n",
      "batch 4589: loss 0.000959\n",
      "batch 4590: loss 0.001534\n",
      "batch 4591: loss 0.015850\n",
      "batch 4592: loss 0.038773\n",
      "batch 4593: loss 0.045088\n",
      "batch 4594: loss 0.000443\n",
      "batch 4595: loss 0.001836\n",
      "batch 4596: loss 0.006187\n",
      "batch 4597: loss 0.059377\n",
      "batch 4598: loss 0.005198\n",
      "batch 4599: loss 0.001653\n",
      "batch 4600: loss 0.106871\n",
      "batch 4601: loss 0.014692\n",
      "batch 4602: loss 0.017665\n",
      "batch 4603: loss 0.003500\n",
      "batch 4604: loss 0.000741\n",
      "batch 4605: loss 0.040688\n",
      "batch 4606: loss 0.007802\n",
      "batch 4607: loss 0.009783\n",
      "batch 4608: loss 0.001790\n",
      "batch 4609: loss 0.003021\n",
      "batch 4610: loss 0.000542\n",
      "batch 4611: loss 0.043654\n",
      "batch 4612: loss 0.005723\n",
      "batch 4613: loss 0.025809\n",
      "batch 4614: loss 0.001811\n",
      "batch 4615: loss 0.000236\n",
      "batch 4616: loss 0.000087\n",
      "batch 4617: loss 0.005597\n",
      "batch 4618: loss 0.012997\n",
      "batch 4619: loss 0.010148\n",
      "batch 4620: loss 0.002306\n",
      "batch 4621: loss 0.000982\n",
      "batch 4622: loss 0.001911\n",
      "batch 4623: loss 0.005311\n",
      "batch 4624: loss 0.000771\n",
      "batch 4625: loss 0.050289\n",
      "batch 4626: loss 0.002842\n",
      "batch 4627: loss 0.008945\n",
      "batch 4628: loss 0.010387\n",
      "batch 4629: loss 0.004030\n",
      "batch 4630: loss 0.058417\n",
      "batch 4631: loss 0.043691\n",
      "batch 4632: loss 0.001119\n",
      "batch 4633: loss 0.004325\n",
      "batch 4634: loss 0.039453\n",
      "batch 4635: loss 0.002386\n",
      "batch 4636: loss 0.018757\n",
      "batch 4637: loss 0.031463\n",
      "batch 4638: loss 0.000539\n",
      "batch 4639: loss 0.054958\n",
      "batch 4640: loss 0.014412\n",
      "batch 4641: loss 0.001566\n",
      "batch 4642: loss 0.004073\n",
      "batch 4643: loss 0.004548\n",
      "batch 4644: loss 0.004174\n",
      "batch 4645: loss 0.174897\n",
      "batch 4646: loss 0.001260\n",
      "batch 4647: loss 0.065288\n",
      "batch 4648: loss 0.023782\n",
      "batch 4649: loss 0.007782\n",
      "batch 4650: loss 0.000221\n",
      "batch 4651: loss 0.002878\n",
      "batch 4652: loss 0.015000\n",
      "batch 4653: loss 0.001150\n",
      "batch 4654: loss 0.000854\n",
      "batch 4655: loss 0.006278\n",
      "batch 4656: loss 0.060275\n",
      "batch 4657: loss 0.068115\n",
      "batch 4658: loss 0.008307\n",
      "batch 4659: loss 0.000440\n",
      "batch 4660: loss 0.004581\n",
      "batch 4661: loss 0.016433\n",
      "batch 4662: loss 0.000057\n",
      "batch 4663: loss 0.000226\n",
      "batch 4664: loss 0.066833\n",
      "batch 4665: loss 0.000629\n",
      "batch 4666: loss 0.000159\n",
      "batch 4667: loss 0.002267\n",
      "batch 4668: loss 0.007573\n",
      "batch 4669: loss 0.022947\n",
      "batch 4670: loss 0.000379\n",
      "batch 4671: loss 0.002321\n",
      "batch 4672: loss 0.026831\n",
      "batch 4673: loss 0.101767\n",
      "batch 4674: loss 0.001738\n",
      "batch 4675: loss 0.005350\n",
      "batch 4676: loss 0.010643\n",
      "batch 4677: loss 0.016840\n",
      "batch 4678: loss 0.001807\n",
      "batch 4679: loss 0.001150\n",
      "batch 4680: loss 0.034871\n",
      "batch 4681: loss 0.001242\n",
      "batch 4682: loss 0.087602\n",
      "batch 4683: loss 0.002779\n",
      "batch 4684: loss 0.024250\n",
      "batch 4685: loss 0.001377\n",
      "batch 4686: loss 0.013922\n",
      "batch 4687: loss 0.000497\n",
      "batch 4688: loss 0.014678\n",
      "batch 4689: loss 0.003496\n",
      "batch 4690: loss 0.000255\n",
      "batch 4691: loss 0.100034\n",
      "batch 4692: loss 0.000196\n",
      "batch 4693: loss 0.005006\n",
      "batch 4694: loss 0.074297\n",
      "batch 4695: loss 0.000241\n",
      "batch 4696: loss 0.018103\n",
      "batch 4697: loss 0.000064\n",
      "batch 4698: loss 0.000765\n",
      "batch 4699: loss 0.002935\n",
      "batch 4700: loss 0.001825\n",
      "batch 4701: loss 0.072331\n",
      "batch 4702: loss 0.000040\n",
      "batch 4703: loss 0.009355\n",
      "batch 4704: loss 0.048044\n",
      "batch 4705: loss 0.002513\n",
      "batch 4706: loss 0.012166\n",
      "batch 4707: loss 0.036939\n",
      "batch 4708: loss 0.035394\n",
      "batch 4709: loss 0.001698\n",
      "batch 4710: loss 0.196832\n",
      "batch 4711: loss 0.000079\n",
      "batch 4712: loss 0.000362\n",
      "batch 4713: loss 0.029002\n",
      "batch 4714: loss 0.001690\n",
      "batch 4715: loss 0.072994\n",
      "batch 4716: loss 0.027707\n",
      "batch 4717: loss 0.016095\n",
      "batch 4718: loss 0.010648\n",
      "batch 4719: loss 0.004852\n",
      "batch 4720: loss 0.000095\n",
      "batch 4721: loss 0.000241\n",
      "batch 4722: loss 0.000104\n",
      "batch 4723: loss 0.001767\n",
      "batch 4724: loss 0.001464\n",
      "batch 4725: loss 0.034534\n",
      "batch 4726: loss 0.000521\n",
      "batch 4727: loss 0.000886\n",
      "batch 4728: loss 0.019177\n",
      "batch 4729: loss 0.001220\n",
      "batch 4730: loss 0.001043\n",
      "batch 4731: loss 0.044386\n",
      "batch 4732: loss 0.002278\n",
      "batch 4733: loss 0.012634\n",
      "batch 4734: loss 0.001376\n",
      "batch 4735: loss 0.009202\n",
      "batch 4736: loss 0.000228\n",
      "batch 4737: loss 0.005070\n",
      "batch 4738: loss 0.000188\n",
      "batch 4739: loss 0.114102\n",
      "batch 4740: loss 0.117069\n",
      "batch 4741: loss 0.000891\n",
      "batch 4742: loss 0.071901\n",
      "batch 4743: loss 0.018340\n",
      "batch 4744: loss 0.002704\n",
      "batch 4745: loss 0.004081\n",
      "batch 4746: loss 0.000072\n",
      "batch 4747: loss 0.001830\n",
      "batch 4748: loss 0.000554\n",
      "batch 4749: loss 0.005060\n",
      "batch 4750: loss 0.036101\n",
      "batch 4751: loss 0.059702\n",
      "batch 4752: loss 0.005147\n",
      "batch 4753: loss 0.001845\n",
      "batch 4754: loss 0.000763\n",
      "batch 4755: loss 0.000998\n",
      "batch 4756: loss 0.010403\n",
      "batch 4757: loss 0.011834\n",
      "batch 4758: loss 0.016600\n",
      "batch 4759: loss 0.000101\n",
      "batch 4760: loss 0.005363\n",
      "batch 4761: loss 0.005960\n",
      "batch 4762: loss 0.006093\n",
      "batch 4763: loss 0.022170\n",
      "batch 4764: loss 0.005266\n",
      "batch 4765: loss 0.000328\n",
      "batch 4766: loss 0.003985\n",
      "batch 4767: loss 0.042554\n",
      "batch 4768: loss 0.000195\n",
      "batch 4769: loss 0.010589\n",
      "batch 4770: loss 0.000378\n",
      "batch 4771: loss 0.001050\n",
      "batch 4772: loss 0.003979\n",
      "batch 4773: loss 0.000290\n",
      "batch 4774: loss 0.000274\n",
      "batch 4775: loss 0.000743\n",
      "batch 4776: loss 0.000529\n",
      "batch 4777: loss 0.001312\n",
      "batch 4778: loss 0.001636\n",
      "batch 4779: loss 0.006827\n",
      "batch 4780: loss 0.004084\n",
      "batch 4781: loss 0.058636\n",
      "batch 4782: loss 0.004902\n",
      "batch 4783: loss 0.004357\n",
      "batch 4784: loss 0.001337\n",
      "batch 4785: loss 0.001864\n",
      "batch 4786: loss 0.001090\n",
      "batch 4787: loss 0.181641\n",
      "batch 4788: loss 0.005562\n",
      "batch 4789: loss 0.000242\n",
      "batch 4790: loss 0.000247\n",
      "batch 4791: loss 0.020383\n",
      "batch 4792: loss 0.000863\n",
      "batch 4793: loss 0.001346\n",
      "batch 4794: loss 0.035295\n",
      "batch 4795: loss 0.001697\n",
      "batch 4796: loss 0.011827\n",
      "batch 4797: loss 0.000434\n",
      "batch 4798: loss 0.000367\n",
      "batch 4799: loss 0.105913\n",
      "batch 4800: loss 0.037301\n",
      "batch 4801: loss 0.026996\n",
      "batch 4802: loss 0.040355\n",
      "batch 4803: loss 0.000239\n",
      "batch 4804: loss 0.011808\n",
      "batch 4805: loss 0.000227\n",
      "batch 4806: loss 0.000372\n",
      "batch 4807: loss 0.015376\n",
      "batch 4808: loss 0.002184\n",
      "batch 4809: loss 0.007175\n",
      "batch 4810: loss 0.000288\n",
      "batch 4811: loss 0.000638\n",
      "batch 4812: loss 0.000222\n",
      "batch 4813: loss 0.001245\n",
      "batch 4814: loss 0.021692\n",
      "batch 4815: loss 0.024699\n",
      "batch 4816: loss 0.007432\n",
      "batch 4817: loss 0.000986\n",
      "batch 4818: loss 0.408655\n",
      "batch 4819: loss 0.021976\n",
      "batch 4820: loss 0.013824\n",
      "batch 4821: loss 0.001523\n",
      "batch 4822: loss 0.007945\n",
      "batch 4823: loss 0.001429\n",
      "batch 4824: loss 0.001153\n",
      "batch 4825: loss 0.073091\n",
      "batch 4826: loss 0.000716\n",
      "batch 4827: loss 0.000727\n",
      "batch 4828: loss 0.002672\n",
      "batch 4829: loss 0.000827\n",
      "batch 4830: loss 0.002220\n",
      "batch 4831: loss 0.005931\n",
      "batch 4832: loss 0.000175\n",
      "batch 4833: loss 0.000365\n",
      "batch 4834: loss 0.000485\n",
      "batch 4835: loss 0.013359\n",
      "batch 4836: loss 0.067479\n",
      "batch 4837: loss 0.000226\n",
      "batch 4838: loss 0.024425\n",
      "batch 4839: loss 0.000351\n",
      "batch 4840: loss 0.000357\n",
      "batch 4841: loss 0.000201\n",
      "batch 4842: loss 0.001706\n",
      "batch 4843: loss 0.005973\n",
      "batch 4844: loss 0.001055\n",
      "batch 4845: loss 0.015638\n",
      "batch 4846: loss 0.002358\n",
      "batch 4847: loss 0.000220\n",
      "batch 4848: loss 0.000676\n",
      "batch 4849: loss 0.046448\n",
      "batch 4850: loss 0.004027\n",
      "batch 4851: loss 0.004524\n",
      "batch 4852: loss 0.005242\n",
      "batch 4853: loss 0.000142\n",
      "batch 4854: loss 0.001894\n",
      "batch 4855: loss 0.004566\n",
      "batch 4856: loss 0.003831\n",
      "batch 4857: loss 0.000833\n",
      "batch 4858: loss 0.000937\n",
      "batch 4859: loss 0.000200\n",
      "batch 4860: loss 0.004251\n",
      "batch 4861: loss 0.021337\n",
      "batch 4862: loss 0.003514\n",
      "batch 4863: loss 0.004683\n",
      "batch 4864: loss 0.004078\n",
      "batch 4865: loss 0.000346\n",
      "batch 4866: loss 0.055572\n",
      "batch 4867: loss 0.000968\n",
      "batch 4868: loss 0.004147\n",
      "batch 4869: loss 0.086754\n",
      "batch 4870: loss 0.000757\n",
      "batch 4871: loss 0.000148\n",
      "batch 4872: loss 0.003431\n",
      "batch 4873: loss 0.019369\n",
      "batch 4874: loss 0.000733\n",
      "batch 4875: loss 0.001900\n",
      "batch 4876: loss 0.000631\n",
      "batch 4877: loss 0.006657\n",
      "batch 4878: loss 0.016000\n",
      "batch 4879: loss 0.052680\n",
      "batch 4880: loss 0.000144\n",
      "batch 4881: loss 0.000705\n",
      "batch 4882: loss 0.004198\n",
      "batch 4883: loss 0.000617\n",
      "batch 4884: loss 0.009648\n",
      "batch 4885: loss 0.000055\n",
      "batch 4886: loss 0.007456\n",
      "batch 4887: loss 0.028525\n",
      "batch 4888: loss 0.000221\n",
      "batch 4889: loss 0.001521\n",
      "batch 4890: loss 0.000415\n",
      "batch 4891: loss 0.075948\n",
      "batch 4892: loss 0.090897\n",
      "batch 4893: loss 0.000831\n",
      "batch 4894: loss 0.026361\n",
      "batch 4895: loss 0.021858\n",
      "batch 4896: loss 0.016464\n",
      "batch 4897: loss 0.134410\n",
      "batch 4898: loss 0.006433\n",
      "batch 4899: loss 0.005408\n",
      "batch 4900: loss 0.018951\n",
      "batch 4901: loss 0.002749\n",
      "batch 4902: loss 0.006929\n",
      "batch 4903: loss 0.016364\n",
      "batch 4904: loss 0.021362\n",
      "batch 4905: loss 0.003972\n",
      "batch 4906: loss 0.000216\n",
      "batch 4907: loss 0.019847\n",
      "batch 4908: loss 0.000432\n",
      "batch 4909: loss 0.002917\n",
      "batch 4910: loss 0.003961\n",
      "batch 4911: loss 0.000147\n",
      "batch 4912: loss 0.000213\n",
      "batch 4913: loss 0.000767\n",
      "batch 4914: loss 0.012695\n",
      "batch 4915: loss 0.000177\n",
      "batch 4916: loss 0.001031\n",
      "batch 4917: loss 0.003664\n",
      "batch 4918: loss 0.001223\n",
      "batch 4919: loss 0.001994\n",
      "batch 4920: loss 0.000238\n",
      "batch 4921: loss 0.001541\n",
      "batch 4922: loss 0.038932\n",
      "batch 4923: loss 0.002026\n",
      "batch 4924: loss 0.000970\n",
      "batch 4925: loss 0.000888\n",
      "batch 4926: loss 0.003649\n",
      "batch 4927: loss 0.007889\n",
      "batch 4928: loss 0.003979\n",
      "batch 4929: loss 0.003277\n",
      "batch 4930: loss 0.001792\n",
      "batch 4931: loss 0.005563\n",
      "batch 4932: loss 0.172930\n",
      "batch 4933: loss 0.001789\n",
      "batch 4934: loss 0.108110\n",
      "batch 4935: loss 0.080446\n",
      "batch 4936: loss 0.000866\n",
      "batch 4937: loss 0.002416\n",
      "batch 4938: loss 0.000775\n",
      "batch 4939: loss 0.018496\n",
      "batch 4940: loss 0.000257\n",
      "batch 4941: loss 0.005133\n",
      "batch 4942: loss 0.000159\n",
      "batch 4943: loss 0.002016\n",
      "batch 4944: loss 0.000210\n",
      "batch 4945: loss 0.001725\n",
      "batch 4946: loss 0.010042\n",
      "batch 4947: loss 0.006380\n",
      "batch 4948: loss 0.001515\n",
      "batch 4949: loss 0.000294\n",
      "batch 4950: loss 0.004026\n",
      "batch 4951: loss 0.000973\n",
      "batch 4952: loss 0.210605\n",
      "batch 4953: loss 0.079291\n",
      "batch 4954: loss 0.003343\n",
      "batch 4955: loss 0.046886\n",
      "batch 4956: loss 0.003462\n",
      "batch 4957: loss 0.108236\n",
      "batch 4958: loss 0.044163\n",
      "batch 4959: loss 0.000254\n",
      "batch 4960: loss 0.001334\n",
      "batch 4961: loss 0.042463\n",
      "batch 4962: loss 0.000244\n",
      "batch 4963: loss 0.012545\n",
      "batch 4964: loss 0.107913\n",
      "batch 4965: loss 0.007745\n",
      "batch 4966: loss 0.007601\n",
      "batch 4967: loss 0.007880\n",
      "batch 4968: loss 0.004551\n",
      "batch 4969: loss 0.078346\n",
      "batch 4970: loss 0.000573\n",
      "batch 4971: loss 0.000613\n",
      "batch 4972: loss 0.001458\n",
      "batch 4973: loss 0.005350\n",
      "batch 4974: loss 0.002474\n",
      "batch 4975: loss 0.025003\n",
      "batch 4976: loss 0.001134\n",
      "batch 4977: loss 0.001793\n",
      "batch 4978: loss 0.044769\n",
      "batch 4979: loss 0.000135\n",
      "batch 4980: loss 0.000291\n",
      "batch 4981: loss 0.001039\n",
      "batch 4982: loss 0.025816\n",
      "batch 4983: loss 0.001182\n",
      "batch 4984: loss 0.004899\n",
      "batch 4985: loss 0.000877\n",
      "batch 4986: loss 0.007351\n",
      "batch 4987: loss 0.019087\n",
      "batch 4988: loss 0.000393\n",
      "batch 4989: loss 0.006568\n",
      "batch 4990: loss 0.003927\n",
      "batch 4991: loss 0.002616\n",
      "batch 4992: loss 0.018603\n",
      "batch 4993: loss 0.009251\n",
      "batch 4994: loss 0.001435\n",
      "batch 4995: loss 0.030682\n",
      "batch 4996: loss 0.011977\n",
      "batch 4997: loss 0.009182\n",
      "batch 4998: loss 0.001942\n",
      "batch 4999: loss 0.003362\n",
      "batch 5000: loss 0.088247\n",
      "batch 5001: loss 0.007137\n",
      "batch 5002: loss 0.000257\n",
      "batch 5003: loss 0.063883\n",
      "batch 5004: loss 0.000067\n",
      "batch 5005: loss 0.001714\n",
      "batch 5006: loss 0.002323\n",
      "batch 5007: loss 0.012612\n",
      "batch 5008: loss 0.003714\n",
      "batch 5009: loss 0.011508\n",
      "batch 5010: loss 0.013181\n",
      "batch 5011: loss 0.002544\n",
      "batch 5012: loss 0.019159\n",
      "batch 5013: loss 0.005427\n",
      "batch 5014: loss 0.018076\n",
      "batch 5015: loss 0.004513\n",
      "batch 5016: loss 0.001791\n",
      "batch 5017: loss 0.008083\n",
      "batch 5018: loss 0.004396\n",
      "batch 5019: loss 0.000686\n",
      "batch 5020: loss 0.004572\n",
      "batch 5021: loss 0.000578\n",
      "batch 5022: loss 0.041442\n",
      "batch 5023: loss 0.000882\n",
      "batch 5024: loss 0.000954\n",
      "batch 5025: loss 0.000096\n",
      "batch 5026: loss 0.034361\n",
      "batch 5027: loss 0.027929\n",
      "batch 5028: loss 0.000166\n",
      "batch 5029: loss 0.020510\n",
      "batch 5030: loss 0.000591\n",
      "batch 5031: loss 0.009556\n",
      "batch 5032: loss 0.003163\n",
      "batch 5033: loss 0.021928\n",
      "batch 5034: loss 0.002062\n",
      "batch 5035: loss 0.003353\n",
      "batch 5036: loss 0.007663\n",
      "batch 5037: loss 0.002218\n",
      "batch 5038: loss 0.009440\n",
      "batch 5039: loss 0.000615\n",
      "batch 5040: loss 0.000202\n",
      "batch 5041: loss 0.000530\n",
      "batch 5042: loss 0.024994\n",
      "batch 5043: loss 0.004096\n",
      "batch 5044: loss 0.001329\n",
      "batch 5045: loss 0.001895\n",
      "batch 5046: loss 0.000260\n",
      "batch 5047: loss 0.033137\n",
      "batch 5048: loss 0.001086\n",
      "batch 5049: loss 0.050155\n",
      "batch 5050: loss 0.000102\n",
      "batch 5051: loss 0.006646\n",
      "batch 5052: loss 0.000104\n",
      "batch 5053: loss 0.011781\n",
      "batch 5054: loss 0.000532\n",
      "batch 5055: loss 0.000085\n",
      "batch 5056: loss 0.008062\n",
      "batch 5057: loss 0.003581\n",
      "batch 5058: loss 0.000626\n",
      "batch 5059: loss 0.000405\n",
      "batch 5060: loss 0.004546\n",
      "batch 5061: loss 0.019562\n",
      "batch 5062: loss 0.001352\n",
      "batch 5063: loss 0.003054\n",
      "batch 5064: loss 0.001218\n",
      "batch 5065: loss 0.059853\n",
      "batch 5066: loss 0.001631\n",
      "batch 5067: loss 0.027910\n",
      "batch 5068: loss 0.003808\n",
      "batch 5069: loss 0.007116\n",
      "batch 5070: loss 0.003296\n",
      "batch 5071: loss 0.057683\n",
      "batch 5072: loss 0.000197\n",
      "batch 5073: loss 0.000010\n",
      "batch 5074: loss 0.000445\n",
      "batch 5075: loss 0.038163\n",
      "batch 5076: loss 0.002984\n",
      "batch 5077: loss 0.000101\n",
      "batch 5078: loss 0.001434\n",
      "batch 5079: loss 0.018305\n",
      "batch 5080: loss 0.002359\n",
      "batch 5081: loss 0.000400\n",
      "batch 5082: loss 0.019205\n",
      "batch 5083: loss 0.006680\n",
      "batch 5084: loss 0.000593\n",
      "batch 5085: loss 0.004920\n",
      "batch 5086: loss 0.001307\n",
      "batch 5087: loss 0.043010\n",
      "batch 5088: loss 0.006185\n",
      "batch 5089: loss 0.006199\n",
      "batch 5090: loss 0.000225\n",
      "batch 5091: loss 0.000570\n",
      "batch 5092: loss 0.008601\n",
      "batch 5093: loss 0.030070\n",
      "batch 5094: loss 0.028017\n",
      "batch 5095: loss 0.077396\n",
      "batch 5096: loss 0.000570\n",
      "batch 5097: loss 0.002586\n",
      "batch 5098: loss 0.000130\n",
      "batch 5099: loss 0.015546\n",
      "batch 5100: loss 0.001800\n",
      "batch 5101: loss 0.002926\n",
      "batch 5102: loss 0.008792\n",
      "batch 5103: loss 0.002788\n",
      "batch 5104: loss 0.000040\n",
      "batch 5105: loss 0.000323\n",
      "batch 5106: loss 0.021787\n",
      "batch 5107: loss 0.001262\n",
      "batch 5108: loss 0.000124\n",
      "batch 5109: loss 0.010441\n",
      "batch 5110: loss 0.044662\n",
      "batch 5111: loss 0.000483\n",
      "batch 5112: loss 0.000294\n",
      "batch 5113: loss 0.005079\n",
      "batch 5114: loss 0.002130\n",
      "batch 5115: loss 0.007597\n",
      "batch 5116: loss 0.000430\n",
      "batch 5117: loss 0.001121\n",
      "batch 5118: loss 0.004572\n",
      "batch 5119: loss 0.000530\n",
      "batch 5120: loss 0.006058\n",
      "batch 5121: loss 0.001970\n",
      "batch 5122: loss 0.050200\n",
      "batch 5123: loss 0.004087\n",
      "batch 5124: loss 0.000710\n",
      "batch 5125: loss 0.003741\n",
      "batch 5126: loss 0.000261\n",
      "batch 5127: loss 0.008237\n",
      "batch 5128: loss 0.001628\n",
      "batch 5129: loss 0.000561\n",
      "batch 5130: loss 0.000127\n",
      "batch 5131: loss 0.020803\n",
      "batch 5132: loss 0.014729\n",
      "batch 5133: loss 0.000165\n",
      "batch 5134: loss 0.000838\n",
      "batch 5135: loss 0.021899\n",
      "batch 5136: loss 0.001127\n",
      "batch 5137: loss 0.002610\n",
      "batch 5138: loss 0.003566\n",
      "batch 5139: loss 0.008559\n",
      "batch 5140: loss 0.000020\n",
      "batch 5141: loss 0.000176\n",
      "batch 5142: loss 0.020099\n",
      "batch 5143: loss 0.000072\n",
      "batch 5144: loss 0.011493\n",
      "batch 5145: loss 0.000039\n",
      "batch 5146: loss 0.000480\n",
      "batch 5147: loss 0.000080\n",
      "batch 5148: loss 0.000185\n",
      "batch 5149: loss 0.000277\n",
      "batch 5150: loss 0.000045\n",
      "batch 5151: loss 0.000277\n",
      "batch 5152: loss 0.000028\n",
      "batch 5153: loss 0.000038\n",
      "batch 5154: loss 0.008649\n",
      "batch 5155: loss 0.000612\n",
      "batch 5156: loss 0.000037\n",
      "batch 5157: loss 0.010547\n",
      "batch 5158: loss 0.000374\n",
      "batch 5159: loss 0.002993\n",
      "batch 5160: loss 0.000388\n",
      "batch 5161: loss 0.005480\n",
      "batch 5162: loss 0.001867\n",
      "batch 5163: loss 0.001879\n",
      "batch 5164: loss 0.000410\n",
      "batch 5165: loss 0.002277\n",
      "batch 5166: loss 0.132476\n",
      "batch 5167: loss 0.000081\n",
      "batch 5168: loss 0.000593\n",
      "batch 5169: loss 0.000112\n",
      "batch 5170: loss 0.083752\n",
      "batch 5171: loss 0.000043\n",
      "batch 5172: loss 0.000711\n",
      "batch 5173: loss 0.000300\n",
      "batch 5174: loss 0.005210\n",
      "batch 5175: loss 0.001987\n",
      "batch 5176: loss 0.001416\n",
      "batch 5177: loss 0.037324\n",
      "batch 5178: loss 0.019427\n",
      "batch 5179: loss 0.013336\n",
      "batch 5180: loss 0.000167\n",
      "batch 5181: loss 0.094874\n",
      "batch 5182: loss 0.008747\n",
      "batch 5183: loss 0.015270\n",
      "batch 5184: loss 0.017455\n",
      "batch 5185: loss 0.001352\n",
      "batch 5186: loss 0.000780\n",
      "batch 5187: loss 0.061509\n",
      "batch 5188: loss 0.000571\n",
      "batch 5189: loss 0.001260\n",
      "batch 5190: loss 0.036778\n",
      "batch 5191: loss 0.013349\n",
      "batch 5192: loss 0.006562\n",
      "batch 5193: loss 0.004964\n",
      "batch 5194: loss 0.010307\n",
      "batch 5195: loss 0.000190\n",
      "batch 5196: loss 0.026896\n",
      "batch 5197: loss 0.000358\n",
      "batch 5198: loss 0.000871\n",
      "batch 5199: loss 0.003713\n",
      "batch 5200: loss 0.003407\n",
      "batch 5201: loss 0.118107\n",
      "batch 5202: loss 0.004895\n",
      "batch 5203: loss 0.004134\n",
      "batch 5204: loss 0.001490\n",
      "batch 5205: loss 0.001210\n",
      "batch 5206: loss 0.058096\n",
      "batch 5207: loss 0.002709\n",
      "batch 5208: loss 0.086589\n",
      "batch 5209: loss 0.000856\n",
      "batch 5210: loss 0.001674\n",
      "batch 5211: loss 0.000087\n",
      "batch 5212: loss 0.001650\n",
      "batch 5213: loss 0.019718\n",
      "batch 5214: loss 0.003106\n",
      "batch 5215: loss 0.000067\n",
      "batch 5216: loss 0.075523\n",
      "batch 5217: loss 0.000977\n",
      "batch 5218: loss 0.055049\n",
      "batch 5219: loss 0.001106\n",
      "batch 5220: loss 0.001715\n",
      "batch 5221: loss 0.006513\n",
      "batch 5222: loss 0.003541\n",
      "batch 5223: loss 0.021381\n",
      "batch 5224: loss 0.037449\n",
      "batch 5225: loss 0.000321\n",
      "batch 5226: loss 0.003079\n",
      "batch 5227: loss 0.078558\n",
      "batch 5228: loss 0.003211\n",
      "batch 5229: loss 0.006326\n",
      "batch 5230: loss 0.020153\n",
      "batch 5231: loss 0.102717\n",
      "batch 5232: loss 0.043130\n",
      "batch 5233: loss 0.002699\n",
      "batch 5234: loss 0.002005\n",
      "batch 5235: loss 0.015073\n",
      "batch 5236: loss 0.051398\n",
      "batch 5237: loss 0.012230\n",
      "batch 5238: loss 0.000684\n",
      "batch 5239: loss 0.014264\n",
      "batch 5240: loss 0.180132\n",
      "batch 5241: loss 0.012907\n",
      "batch 5242: loss 0.000165\n",
      "batch 5243: loss 0.062724\n",
      "batch 5244: loss 0.000712\n",
      "batch 5245: loss 0.000938\n",
      "batch 5246: loss 0.000814\n",
      "batch 5247: loss 0.003679\n",
      "batch 5248: loss 0.041706\n",
      "batch 5249: loss 0.005522\n",
      "batch 5250: loss 0.014193\n",
      "batch 5251: loss 0.000085\n",
      "batch 5252: loss 0.002068\n",
      "batch 5253: loss 0.000324\n",
      "batch 5254: loss 0.017911\n",
      "batch 5255: loss 0.022998\n",
      "batch 5256: loss 0.002806\n",
      "batch 5257: loss 0.034763\n",
      "batch 5258: loss 0.001382\n",
      "batch 5259: loss 0.000104\n",
      "batch 5260: loss 0.000602\n",
      "batch 5261: loss 0.002645\n",
      "batch 5262: loss 0.099506\n",
      "batch 5263: loss 0.006984\n",
      "batch 5264: loss 0.000272\n",
      "batch 5265: loss 0.001301\n",
      "batch 5266: loss 0.000015\n",
      "batch 5267: loss 0.000959\n",
      "batch 5268: loss 0.050022\n",
      "batch 5269: loss 0.015562\n",
      "batch 5270: loss 0.001074\n",
      "batch 5271: loss 0.000100\n",
      "batch 5272: loss 0.034060\n",
      "batch 5273: loss 0.017560\n",
      "batch 5274: loss 0.001982\n",
      "batch 5275: loss 0.000111\n",
      "batch 5276: loss 0.022589\n",
      "batch 5277: loss 0.004158\n",
      "batch 5278: loss 0.000488\n",
      "batch 5279: loss 0.064657\n",
      "batch 5280: loss 0.059678\n",
      "batch 5281: loss 0.000665\n",
      "batch 5282: loss 0.000338\n",
      "batch 5283: loss 0.088153\n",
      "batch 5284: loss 0.000839\n",
      "batch 5285: loss 0.137744\n",
      "batch 5286: loss 0.000037\n",
      "batch 5287: loss 0.003430\n",
      "batch 5288: loss 0.015694\n",
      "batch 5289: loss 0.003374\n",
      "batch 5290: loss 0.000504\n",
      "batch 5291: loss 0.021441\n",
      "batch 5292: loss 0.001170\n",
      "batch 5293: loss 0.000592\n",
      "batch 5294: loss 0.008064\n",
      "batch 5295: loss 0.003948\n",
      "batch 5296: loss 0.044971\n",
      "batch 5297: loss 0.008544\n",
      "batch 5298: loss 0.013727\n",
      "batch 5299: loss 0.001422\n",
      "batch 5300: loss 0.000513\n",
      "batch 5301: loss 0.003895\n",
      "batch 5302: loss 0.086791\n",
      "batch 5303: loss 0.000051\n",
      "batch 5304: loss 0.014881\n",
      "batch 5305: loss 0.008413\n",
      "batch 5306: loss 0.014436\n",
      "batch 5307: loss 0.000577\n",
      "batch 5308: loss 0.001017\n",
      "batch 5309: loss 0.004359\n",
      "batch 5310: loss 0.002607\n",
      "batch 5311: loss 0.005695\n",
      "batch 5312: loss 0.022865\n",
      "batch 5313: loss 0.014538\n",
      "batch 5314: loss 0.000273\n",
      "batch 5315: loss 0.002880\n",
      "batch 5316: loss 0.000504\n",
      "batch 5317: loss 0.098941\n",
      "batch 5318: loss 0.000282\n",
      "batch 5319: loss 0.000193\n",
      "batch 5320: loss 0.006123\n",
      "batch 5321: loss 0.002473\n",
      "batch 5322: loss 0.178994\n",
      "batch 5323: loss 0.000802\n",
      "batch 5324: loss 0.056529\n",
      "batch 5325: loss 0.000018\n",
      "batch 5326: loss 0.003637\n",
      "batch 5327: loss 0.001509\n",
      "batch 5328: loss 0.078973\n",
      "batch 5329: loss 0.000694\n",
      "batch 5330: loss 0.022709\n",
      "batch 5331: loss 0.004557\n",
      "batch 5332: loss 0.000684\n",
      "batch 5333: loss 0.000480\n",
      "batch 5334: loss 0.001257\n",
      "batch 5335: loss 0.000617\n",
      "batch 5336: loss 0.000602\n",
      "batch 5337: loss 0.007187\n",
      "batch 5338: loss 0.020310\n",
      "batch 5339: loss 0.131204\n",
      "batch 5340: loss 0.101958\n",
      "batch 5341: loss 0.004961\n",
      "batch 5342: loss 0.000491\n",
      "batch 5343: loss 0.015942\n",
      "batch 5344: loss 0.008749\n",
      "batch 5345: loss 0.000933\n",
      "batch 5346: loss 0.016373\n",
      "batch 5347: loss 0.003198\n",
      "batch 5348: loss 0.001842\n",
      "batch 5349: loss 0.036549\n",
      "batch 5350: loss 0.000025\n",
      "batch 5351: loss 0.081031\n",
      "batch 5352: loss 0.027795\n",
      "batch 5353: loss 0.002767\n",
      "batch 5354: loss 0.022486\n",
      "batch 5355: loss 0.000652\n",
      "batch 5356: loss 0.003607\n",
      "batch 5357: loss 0.020854\n",
      "batch 5358: loss 0.027638\n",
      "batch 5359: loss 0.000035\n",
      "batch 5360: loss 0.002023\n",
      "batch 5361: loss 0.011890\n",
      "batch 5362: loss 0.005709\n",
      "batch 5363: loss 0.004623\n",
      "batch 5364: loss 0.324826\n",
      "batch 5365: loss 0.002679\n",
      "batch 5366: loss 0.000072\n",
      "batch 5367: loss 0.000203\n",
      "batch 5368: loss 0.000184\n",
      "batch 5369: loss 0.000240\n",
      "batch 5370: loss 0.049944\n",
      "batch 5371: loss 0.047691\n",
      "batch 5372: loss 0.013451\n",
      "batch 5373: loss 0.044588\n",
      "batch 5374: loss 0.004811\n",
      "batch 5375: loss 0.010061\n",
      "batch 5376: loss 0.001117\n",
      "batch 5377: loss 0.001960\n",
      "batch 5378: loss 0.000735\n",
      "batch 5379: loss 0.002133\n",
      "batch 5380: loss 0.000436\n",
      "batch 5381: loss 0.000314\n",
      "batch 5382: loss 0.000379\n",
      "batch 5383: loss 0.019599\n",
      "batch 5384: loss 0.000416\n",
      "batch 5385: loss 0.007522\n",
      "batch 5386: loss 0.003918\n",
      "batch 5387: loss 0.000148\n",
      "batch 5388: loss 0.001733\n",
      "batch 5389: loss 0.108448\n",
      "batch 5390: loss 0.000078\n",
      "batch 5391: loss 0.000098\n",
      "batch 5392: loss 0.013718\n",
      "batch 5393: loss 0.000850\n",
      "batch 5394: loss 0.006642\n",
      "batch 5395: loss 0.039954\n",
      "batch 5396: loss 0.121465\n",
      "batch 5397: loss 0.013179\n",
      "batch 5398: loss 0.000982\n",
      "batch 5399: loss 0.000443\n",
      "batch 5400: loss 0.004843\n",
      "batch 5401: loss 0.046035\n",
      "batch 5402: loss 0.000143\n",
      "batch 5403: loss 0.005986\n",
      "batch 5404: loss 0.003541\n",
      "batch 5405: loss 0.032613\n",
      "batch 5406: loss 0.001377\n",
      "batch 5407: loss 0.009781\n",
      "batch 5408: loss 0.003554\n",
      "batch 5409: loss 0.006697\n",
      "batch 5410: loss 0.000660\n",
      "batch 5411: loss 0.022947\n",
      "batch 5412: loss 0.000056\n",
      "batch 5413: loss 0.001749\n",
      "batch 5414: loss 0.000106\n",
      "batch 5415: loss 0.000751\n",
      "batch 5416: loss 0.043318\n",
      "batch 5417: loss 0.008658\n",
      "batch 5418: loss 0.022904\n",
      "batch 5419: loss 0.000046\n",
      "batch 5420: loss 0.026758\n",
      "batch 5421: loss 0.001214\n",
      "batch 5422: loss 0.000171\n",
      "batch 5423: loss 0.001072\n",
      "batch 5424: loss 0.001783\n",
      "batch 5425: loss 0.000200\n",
      "batch 5426: loss 0.000145\n",
      "batch 5427: loss 0.001725\n",
      "batch 5428: loss 0.023229\n",
      "batch 5429: loss 0.039926\n",
      "batch 5430: loss 0.051301\n",
      "batch 5431: loss 0.013915\n",
      "batch 5432: loss 0.014433\n",
      "batch 5433: loss 0.001214\n",
      "batch 5434: loss 0.001159\n",
      "batch 5435: loss 0.084254\n",
      "batch 5436: loss 0.003405\n",
      "batch 5437: loss 0.050471\n",
      "batch 5438: loss 0.185301\n",
      "batch 5439: loss 0.000917\n",
      "batch 5440: loss 0.006817\n",
      "batch 5441: loss 0.000267\n",
      "batch 5442: loss 0.058340\n",
      "batch 5443: loss 0.002147\n",
      "batch 5444: loss 0.000423\n",
      "batch 5445: loss 0.007202\n",
      "batch 5446: loss 0.000597\n",
      "batch 5447: loss 0.007422\n",
      "batch 5448: loss 0.003705\n",
      "batch 5449: loss 0.009764\n",
      "batch 5450: loss 0.025352\n",
      "batch 5451: loss 0.000479\n",
      "batch 5452: loss 0.000385\n",
      "batch 5453: loss 0.001470\n",
      "batch 5454: loss 0.000148\n",
      "batch 5455: loss 0.057053\n",
      "batch 5456: loss 0.003801\n",
      "batch 5457: loss 0.013859\n",
      "batch 5458: loss 0.000115\n",
      "batch 5459: loss 0.005294\n",
      "batch 5460: loss 0.021524\n",
      "batch 5461: loss 0.001531\n",
      "batch 5462: loss 0.000759\n",
      "batch 5463: loss 0.111098\n",
      "batch 5464: loss 0.011510\n",
      "batch 5465: loss 0.000556\n",
      "batch 5466: loss 0.091775\n",
      "batch 5467: loss 0.000924\n",
      "batch 5468: loss 0.015804\n",
      "batch 5469: loss 0.024994\n",
      "batch 5470: loss 0.013346\n",
      "batch 5471: loss 0.000843\n",
      "batch 5472: loss 0.018794\n",
      "batch 5473: loss 0.001253\n",
      "batch 5474: loss 0.009212\n",
      "batch 5475: loss 0.001131\n",
      "batch 5476: loss 0.036364\n",
      "batch 5477: loss 0.000537\n",
      "batch 5478: loss 0.025365\n",
      "batch 5479: loss 0.000391\n",
      "batch 5480: loss 0.001147\n",
      "batch 5481: loss 0.004745\n",
      "batch 5482: loss 0.004922\n",
      "batch 5483: loss 0.100777\n",
      "batch 5484: loss 0.008537\n",
      "batch 5485: loss 0.004500\n",
      "batch 5486: loss 0.006683\n",
      "batch 5487: loss 0.006427\n",
      "batch 5488: loss 0.033618\n",
      "batch 5489: loss 0.000100\n",
      "batch 5490: loss 0.000440\n",
      "batch 5491: loss 0.000245\n",
      "batch 5492: loss 0.000837\n",
      "batch 5493: loss 0.023105\n",
      "batch 5494: loss 0.000635\n",
      "batch 5495: loss 0.001243\n",
      "batch 5496: loss 0.000868\n",
      "batch 5497: loss 0.004897\n",
      "batch 5498: loss 0.113043\n",
      "batch 5499: loss 0.002826\n",
      "batch 5500: loss 0.003903\n",
      "batch 5501: loss 0.000247\n",
      "batch 5502: loss 0.000495\n",
      "batch 5503: loss 0.025398\n",
      "batch 5504: loss 0.083235\n",
      "batch 5505: loss 0.010542\n",
      "batch 5506: loss 0.009641\n",
      "batch 5507: loss 0.003649\n",
      "batch 5508: loss 0.000090\n",
      "batch 5509: loss 0.239915\n",
      "batch 5510: loss 0.004203\n",
      "batch 5511: loss 0.000949\n",
      "batch 5512: loss 0.001940\n",
      "batch 5513: loss 0.006116\n",
      "batch 5514: loss 0.000648\n",
      "batch 5515: loss 0.003030\n",
      "batch 5516: loss 0.003683\n",
      "batch 5517: loss 0.044082\n",
      "batch 5518: loss 0.000480\n",
      "batch 5519: loss 0.002637\n",
      "batch 5520: loss 0.000946\n",
      "batch 5521: loss 0.018367\n",
      "batch 5522: loss 0.000579\n",
      "batch 5523: loss 0.024936\n",
      "batch 5524: loss 0.015065\n",
      "batch 5525: loss 0.010171\n",
      "batch 5526: loss 0.000305\n",
      "batch 5527: loss 0.007613\n",
      "batch 5528: loss 0.000860\n",
      "batch 5529: loss 0.002728\n",
      "batch 5530: loss 0.000639\n",
      "batch 5531: loss 0.000558\n",
      "batch 5532: loss 0.000867\n",
      "batch 5533: loss 0.000181\n",
      "batch 5534: loss 0.007011\n",
      "batch 5535: loss 0.002430\n",
      "batch 5536: loss 0.000345\n",
      "batch 5537: loss 0.000858\n",
      "batch 5538: loss 0.000659\n",
      "batch 5539: loss 0.001328\n",
      "batch 5540: loss 0.000985\n",
      "batch 5541: loss 0.005235\n",
      "batch 5542: loss 0.056087\n",
      "batch 5543: loss 0.000628\n",
      "batch 5544: loss 0.005394\n",
      "batch 5545: loss 0.022999\n",
      "batch 5546: loss 0.009651\n",
      "batch 5547: loss 0.002800\n",
      "batch 5548: loss 0.000332\n",
      "batch 5549: loss 0.000825\n",
      "batch 5550: loss 0.001251\n",
      "batch 5551: loss 0.003597\n",
      "batch 5552: loss 0.008303\n",
      "batch 5553: loss 0.001177\n",
      "batch 5554: loss 0.001141\n",
      "batch 5555: loss 0.014305\n",
      "batch 5556: loss 0.000082\n",
      "batch 5557: loss 0.002584\n",
      "batch 5558: loss 0.000121\n",
      "batch 5559: loss 0.000275\n",
      "batch 5560: loss 0.000073\n",
      "batch 5561: loss 0.005931\n",
      "batch 5562: loss 0.000009\n",
      "batch 5563: loss 0.002996\n",
      "batch 5564: loss 0.113676\n",
      "batch 5565: loss 0.000258\n",
      "batch 5566: loss 0.000849\n",
      "batch 5567: loss 0.007671\n",
      "batch 5568: loss 0.001980\n",
      "batch 5569: loss 0.001386\n",
      "batch 5570: loss 0.003563\n",
      "batch 5571: loss 0.027535\n",
      "batch 5572: loss 0.007048\n",
      "batch 5573: loss 0.000747\n",
      "batch 5574: loss 0.006529\n",
      "batch 5575: loss 0.004961\n",
      "batch 5576: loss 0.000036\n",
      "batch 5577: loss 0.000227\n",
      "batch 5578: loss 0.058448\n",
      "batch 5579: loss 0.002126\n",
      "batch 5580: loss 0.000856\n",
      "batch 5581: loss 0.000592\n",
      "batch 5582: loss 0.010252\n",
      "batch 5583: loss 0.008472\n",
      "batch 5584: loss 0.000028\n",
      "batch 5585: loss 0.002655\n",
      "batch 5586: loss 0.055897\n",
      "batch 5587: loss 0.000084\n",
      "batch 5588: loss 0.126818\n",
      "batch 5589: loss 0.005575\n",
      "batch 5590: loss 0.005116\n",
      "batch 5591: loss 0.000148\n",
      "batch 5592: loss 0.001298\n",
      "batch 5593: loss 0.000842\n",
      "batch 5594: loss 0.015157\n",
      "batch 5595: loss 0.104755\n",
      "batch 5596: loss 0.000279\n",
      "batch 5597: loss 0.000346\n",
      "batch 5598: loss 0.051251\n",
      "batch 5599: loss 0.021807\n",
      "batch 5600: loss 0.116369\n",
      "batch 5601: loss 0.090475\n",
      "batch 5602: loss 0.000912\n",
      "batch 5603: loss 0.008729\n",
      "batch 5604: loss 0.001837\n",
      "batch 5605: loss 0.034318\n",
      "batch 5606: loss 0.080505\n",
      "batch 5607: loss 0.034584\n",
      "batch 5608: loss 0.018346\n",
      "batch 5609: loss 0.070044\n",
      "batch 5610: loss 0.002211\n",
      "batch 5611: loss 0.000203\n",
      "batch 5612: loss 0.001021\n",
      "batch 5613: loss 0.001067\n",
      "batch 5614: loss 0.001431\n",
      "batch 5615: loss 0.080470\n",
      "batch 5616: loss 0.001487\n",
      "batch 5617: loss 0.108912\n",
      "batch 5618: loss 0.044375\n",
      "batch 5619: loss 0.070738\n",
      "batch 5620: loss 0.000289\n",
      "batch 5621: loss 0.000240\n",
      "batch 5622: loss 0.000710\n",
      "batch 5623: loss 0.000748\n",
      "batch 5624: loss 0.028827\n",
      "batch 5625: loss 0.002133\n",
      "batch 5626: loss 0.041998\n",
      "batch 5627: loss 0.001022\n",
      "batch 5628: loss 0.005479\n",
      "batch 5629: loss 0.000553\n",
      "batch 5630: loss 0.000828\n",
      "batch 5631: loss 0.017161\n",
      "batch 5632: loss 0.010933\n",
      "batch 5633: loss 0.013450\n",
      "batch 5634: loss 0.010710\n",
      "batch 5635: loss 0.000421\n",
      "batch 5636: loss 0.008226\n",
      "batch 5637: loss 0.058155\n",
      "batch 5638: loss 0.000098\n",
      "batch 5639: loss 0.001657\n",
      "batch 5640: loss 0.039113\n",
      "batch 5641: loss 0.000241\n",
      "batch 5642: loss 0.031475\n",
      "batch 5643: loss 0.000215\n",
      "batch 5644: loss 0.002520\n",
      "batch 5645: loss 0.002323\n",
      "batch 5646: loss 0.000199\n",
      "batch 5647: loss 0.000131\n",
      "batch 5648: loss 0.000077\n",
      "batch 5649: loss 0.001599\n",
      "batch 5650: loss 0.008379\n",
      "batch 5651: loss 0.010371\n",
      "batch 5652: loss 0.000754\n",
      "batch 5653: loss 0.017223\n",
      "batch 5654: loss 0.002809\n",
      "batch 5655: loss 0.020285\n",
      "batch 5656: loss 0.014981\n",
      "batch 5657: loss 0.000896\n",
      "batch 5658: loss 0.001983\n",
      "batch 5659: loss 0.157475\n",
      "batch 5660: loss 0.004432\n",
      "batch 5661: loss 0.001617\n",
      "batch 5662: loss 0.007610\n",
      "batch 5663: loss 0.001143\n",
      "batch 5664: loss 0.012240\n",
      "batch 5665: loss 0.011211\n",
      "batch 5666: loss 0.002227\n",
      "batch 5667: loss 0.124770\n",
      "batch 5668: loss 0.022031\n",
      "batch 5669: loss 0.000373\n",
      "batch 5670: loss 0.007235\n",
      "batch 5671: loss 0.008166\n",
      "batch 5672: loss 0.031796\n",
      "batch 5673: loss 0.003227\n",
      "batch 5674: loss 0.159828\n",
      "batch 5675: loss 0.002207\n",
      "batch 5676: loss 0.040941\n",
      "batch 5677: loss 0.002565\n",
      "batch 5678: loss 0.003562\n",
      "batch 5679: loss 0.001091\n",
      "batch 5680: loss 0.036529\n",
      "batch 5681: loss 0.001279\n",
      "batch 5682: loss 0.019226\n",
      "batch 5683: loss 0.004093\n",
      "batch 5684: loss 0.002521\n",
      "batch 5685: loss 0.091633\n",
      "batch 5686: loss 0.012904\n",
      "batch 5687: loss 0.051804\n",
      "batch 5688: loss 0.022615\n",
      "batch 5689: loss 0.009128\n",
      "batch 5690: loss 0.007233\n",
      "batch 5691: loss 0.003010\n",
      "batch 5692: loss 0.009281\n",
      "batch 5693: loss 0.000965\n",
      "batch 5694: loss 0.002647\n",
      "batch 5695: loss 0.020981\n",
      "batch 5696: loss 0.002739\n",
      "batch 5697: loss 0.021565\n",
      "batch 5698: loss 0.043348\n",
      "batch 5699: loss 0.039593\n",
      "batch 5700: loss 0.010909\n",
      "batch 5701: loss 0.015299\n",
      "batch 5702: loss 0.000624\n",
      "batch 5703: loss 0.010664\n",
      "batch 5704: loss 0.048443\n",
      "batch 5705: loss 0.001854\n",
      "batch 5706: loss 0.000385\n",
      "batch 5707: loss 0.155683\n",
      "batch 5708: loss 0.004385\n",
      "batch 5709: loss 0.013205\n",
      "batch 5710: loss 0.001542\n",
      "batch 5711: loss 0.007543\n",
      "batch 5712: loss 0.010242\n",
      "batch 5713: loss 0.000277\n",
      "batch 5714: loss 0.004324\n",
      "batch 5715: loss 0.007932\n",
      "batch 5716: loss 0.006529\n",
      "batch 5717: loss 0.000735\n",
      "batch 5718: loss 0.012663\n",
      "batch 5719: loss 0.000872\n",
      "batch 5720: loss 0.000487\n",
      "batch 5721: loss 0.004055\n",
      "batch 5722: loss 0.002109\n",
      "batch 5723: loss 0.003226\n",
      "batch 5724: loss 0.001916\n",
      "batch 5725: loss 0.011671\n",
      "batch 5726: loss 0.000900\n",
      "batch 5727: loss 0.001757\n",
      "batch 5728: loss 0.007459\n",
      "batch 5729: loss 0.000078\n",
      "batch 5730: loss 0.000611\n",
      "batch 5731: loss 0.074870\n",
      "batch 5732: loss 0.001417\n",
      "batch 5733: loss 0.001728\n",
      "batch 5734: loss 0.003291\n",
      "batch 5735: loss 0.000246\n",
      "batch 5736: loss 0.005493\n",
      "batch 5737: loss 0.038641\n",
      "batch 5738: loss 0.050548\n",
      "batch 5739: loss 0.001794\n",
      "batch 5740: loss 0.000883\n",
      "batch 5741: loss 0.000785\n",
      "batch 5742: loss 0.000541\n",
      "batch 5743: loss 0.000183\n",
      "batch 5744: loss 0.020779\n",
      "batch 5745: loss 0.000328\n",
      "batch 5746: loss 0.001062\n",
      "batch 5747: loss 0.002149\n",
      "batch 5748: loss 0.004508\n",
      "batch 5749: loss 0.000881\n",
      "batch 5750: loss 0.004022\n",
      "batch 5751: loss 0.000618\n",
      "batch 5752: loss 0.015201\n",
      "batch 5753: loss 0.073145\n",
      "batch 5754: loss 0.004982\n",
      "batch 5755: loss 0.009722\n",
      "batch 5756: loss 0.008994\n",
      "batch 5757: loss 0.012740\n",
      "batch 5758: loss 0.062221\n",
      "batch 5759: loss 0.039909\n",
      "batch 5760: loss 0.025729\n",
      "batch 5761: loss 0.003275\n",
      "batch 5762: loss 0.003773\n",
      "batch 5763: loss 0.000762\n",
      "batch 5764: loss 0.072647\n",
      "batch 5765: loss 0.002282\n",
      "batch 5766: loss 0.015572\n",
      "batch 5767: loss 0.001379\n",
      "batch 5768: loss 0.003567\n",
      "batch 5769: loss 0.000360\n",
      "batch 5770: loss 0.000939\n",
      "batch 5771: loss 0.112331\n",
      "batch 5772: loss 0.012706\n",
      "batch 5773: loss 0.001429\n",
      "batch 5774: loss 0.010065\n",
      "batch 5775: loss 0.005861\n",
      "batch 5776: loss 0.002795\n",
      "batch 5777: loss 0.027091\n",
      "batch 5778: loss 0.001036\n",
      "batch 5779: loss 0.002773\n",
      "batch 5780: loss 0.158235\n",
      "batch 5781: loss 0.003527\n",
      "batch 5782: loss 0.003984\n",
      "batch 5783: loss 0.071821\n",
      "batch 5784: loss 0.000892\n",
      "batch 5785: loss 0.000263\n",
      "batch 5786: loss 0.017832\n",
      "batch 5787: loss 0.003120\n",
      "batch 5788: loss 0.002991\n",
      "batch 5789: loss 0.000174\n",
      "batch 5790: loss 0.000156\n",
      "batch 5791: loss 0.000269\n",
      "batch 5792: loss 0.004052\n",
      "batch 5793: loss 0.001909\n",
      "batch 5794: loss 0.012940\n",
      "batch 5795: loss 0.001419\n",
      "batch 5796: loss 0.000414\n",
      "batch 5797: loss 0.025409\n",
      "batch 5798: loss 0.010252\n",
      "batch 5799: loss 0.002451\n",
      "batch 5800: loss 0.001439\n",
      "batch 5801: loss 0.153209\n",
      "batch 5802: loss 0.000694\n",
      "batch 5803: loss 0.001596\n",
      "batch 5804: loss 0.000780\n",
      "batch 5805: loss 0.000720\n",
      "batch 5806: loss 0.002247\n",
      "batch 5807: loss 0.000798\n",
      "batch 5808: loss 0.001669\n",
      "batch 5809: loss 0.005053\n",
      "batch 5810: loss 0.000334\n",
      "batch 5811: loss 0.003453\n",
      "batch 5812: loss 0.000724\n",
      "batch 5813: loss 0.000060\n",
      "batch 5814: loss 0.012864\n",
      "batch 5815: loss 0.002102\n",
      "batch 5816: loss 0.003246\n",
      "batch 5817: loss 0.003373\n",
      "batch 5818: loss 0.028086\n",
      "batch 5819: loss 0.001194\n",
      "batch 5820: loss 0.000610\n",
      "batch 5821: loss 0.001033\n",
      "batch 5822: loss 0.005982\n",
      "batch 5823: loss 0.000913\n",
      "batch 5824: loss 0.097704\n",
      "batch 5825: loss 0.002351\n",
      "batch 5826: loss 0.016355\n",
      "batch 5827: loss 0.023255\n",
      "batch 5828: loss 0.001121\n",
      "batch 5829: loss 0.008810\n",
      "batch 5830: loss 0.000926\n",
      "batch 5831: loss 0.030594\n",
      "batch 5832: loss 0.005908\n",
      "batch 5833: loss 0.004866\n",
      "batch 5834: loss 0.050551\n",
      "batch 5835: loss 0.000111\n",
      "batch 5836: loss 0.001381\n",
      "batch 5837: loss 0.002339\n",
      "batch 5838: loss 0.004127\n",
      "batch 5839: loss 0.008925\n",
      "batch 5840: loss 0.000080\n",
      "batch 5841: loss 0.000514\n",
      "batch 5842: loss 0.000119\n",
      "batch 5843: loss 0.000056\n",
      "batch 5844: loss 0.067822\n",
      "batch 5845: loss 0.003516\n",
      "batch 5846: loss 0.000828\n",
      "batch 5847: loss 0.000047\n",
      "batch 5848: loss 0.000043\n",
      "batch 5849: loss 0.035916\n",
      "batch 5850: loss 0.014933\n",
      "batch 5851: loss 0.055233\n",
      "batch 5852: loss 0.000440\n",
      "batch 5853: loss 0.000122\n",
      "batch 5854: loss 0.007463\n",
      "batch 5855: loss 0.000635\n",
      "batch 5856: loss 0.005407\n",
      "batch 5857: loss 0.023156\n",
      "batch 5858: loss 0.000322\n",
      "batch 5859: loss 0.000766\n",
      "batch 5860: loss 0.002368\n",
      "batch 5861: loss 0.023301\n",
      "batch 5862: loss 0.000104\n",
      "batch 5863: loss 0.001510\n",
      "batch 5864: loss 0.000058\n",
      "batch 5865: loss 0.000111\n",
      "batch 5866: loss 0.000525\n",
      "batch 5867: loss 0.000158\n",
      "batch 5868: loss 0.005888\n",
      "batch 5869: loss 0.000613\n",
      "batch 5870: loss 0.000102\n",
      "batch 5871: loss 0.000926\n",
      "batch 5872: loss 0.000610\n",
      "batch 5873: loss 0.000148\n",
      "batch 5874: loss 0.000093\n",
      "batch 5875: loss 0.000547\n",
      "batch 5876: loss 0.033175\n",
      "batch 5877: loss 0.000101\n",
      "batch 5878: loss 0.066102\n",
      "batch 5879: loss 0.000066\n",
      "batch 5880: loss 0.003950\n",
      "batch 5881: loss 0.162676\n",
      "batch 5882: loss 0.011394\n",
      "batch 5883: loss 0.003235\n",
      "batch 5884: loss 0.000419\n",
      "batch 5885: loss 0.008110\n",
      "batch 5886: loss 0.001952\n",
      "batch 5887: loss 0.004178\n",
      "batch 5888: loss 0.016469\n",
      "batch 5889: loss 0.001605\n",
      "batch 5890: loss 0.015214\n",
      "batch 5891: loss 0.062205\n",
      "batch 5892: loss 0.009688\n",
      "batch 5893: loss 0.012507\n",
      "batch 5894: loss 0.004148\n",
      "batch 5895: loss 0.039568\n",
      "batch 5896: loss 0.003115\n",
      "batch 5897: loss 0.047982\n",
      "batch 5898: loss 0.029335\n",
      "batch 5899: loss 0.000675\n",
      "batch 5900: loss 0.001924\n",
      "batch 5901: loss 0.000964\n",
      "batch 5902: loss 0.009154\n",
      "batch 5903: loss 0.018344\n",
      "batch 5904: loss 0.039703\n",
      "batch 5905: loss 0.003800\n",
      "batch 5906: loss 0.004358\n",
      "batch 5907: loss 0.172776\n",
      "batch 5908: loss 0.056652\n",
      "batch 5909: loss 0.000113\n",
      "batch 5910: loss 0.002281\n",
      "batch 5911: loss 0.020850\n",
      "batch 5912: loss 0.020526\n",
      "batch 5913: loss 0.009872\n",
      "batch 5914: loss 0.008324\n",
      "batch 5915: loss 0.004651\n",
      "batch 5916: loss 0.001616\n",
      "batch 5917: loss 0.014952\n",
      "batch 5918: loss 0.004313\n",
      "batch 5919: loss 0.000604\n",
      "batch 5920: loss 0.004454\n",
      "batch 5921: loss 0.007452\n",
      "batch 5922: loss 0.023213\n",
      "batch 5923: loss 0.016891\n",
      "batch 5924: loss 0.000664\n",
      "batch 5925: loss 0.029837\n",
      "batch 5926: loss 0.000062\n",
      "batch 5927: loss 0.005259\n",
      "batch 5928: loss 0.000564\n",
      "batch 5929: loss 0.002859\n",
      "batch 5930: loss 0.004031\n",
      "batch 5931: loss 0.002677\n",
      "batch 5932: loss 0.020439\n",
      "batch 5933: loss 0.005368\n",
      "batch 5934: loss 0.001964\n",
      "batch 5935: loss 0.009665\n",
      "batch 5936: loss 0.001633\n",
      "batch 5937: loss 0.000079\n",
      "batch 5938: loss 0.000391\n",
      "batch 5939: loss 0.000785\n",
      "batch 5940: loss 0.000300\n",
      "batch 5941: loss 0.031581\n",
      "batch 5942: loss 0.000128\n",
      "batch 5943: loss 0.020226\n",
      "batch 5944: loss 0.036954\n",
      "batch 5945: loss 0.002071\n",
      "batch 5946: loss 0.149561\n",
      "batch 5947: loss 0.001513\n",
      "batch 5948: loss 0.014387\n",
      "batch 5949: loss 0.000838\n",
      "batch 5950: loss 0.066870\n",
      "batch 5951: loss 0.003902\n",
      "batch 5952: loss 0.003968\n",
      "batch 5953: loss 0.011531\n",
      "batch 5954: loss 0.000128\n",
      "batch 5955: loss 0.000521\n",
      "batch 5956: loss 0.003178\n",
      "batch 5957: loss 0.000210\n",
      "batch 5958: loss 0.054030\n",
      "batch 5959: loss 0.000413\n",
      "batch 5960: loss 0.000434\n",
      "batch 5961: loss 0.003881\n",
      "batch 5962: loss 0.000953\n",
      "batch 5963: loss 0.000018\n",
      "batch 5964: loss 0.000296\n",
      "batch 5965: loss 0.067570\n",
      "batch 5966: loss 0.007723\n",
      "batch 5967: loss 0.123052\n",
      "batch 5968: loss 0.000084\n",
      "batch 5969: loss 0.010059\n",
      "batch 5970: loss 0.000346\n",
      "batch 5971: loss 0.000538\n",
      "batch 5972: loss 0.001564\n",
      "batch 5973: loss 0.009284\n",
      "batch 5974: loss 0.000060\n",
      "batch 5975: loss 0.000475\n",
      "batch 5976: loss 0.000316\n",
      "batch 5977: loss 0.008533\n",
      "batch 5978: loss 0.001028\n",
      "batch 5979: loss 0.000298\n",
      "batch 5980: loss 0.004930\n",
      "batch 5981: loss 0.000167\n",
      "batch 5982: loss 0.003639\n",
      "batch 5983: loss 0.000589\n",
      "batch 5984: loss 0.000603\n",
      "batch 5985: loss 0.000280\n",
      "batch 5986: loss 0.053295\n",
      "batch 5987: loss 0.000552\n",
      "batch 5988: loss 0.036564\n",
      "batch 5989: loss 0.007786\n",
      "batch 5990: loss 0.000484\n",
      "batch 5991: loss 0.007088\n",
      "batch 5992: loss 0.002075\n",
      "batch 5993: loss 0.000771\n",
      "batch 5994: loss 0.003240\n",
      "batch 5995: loss 0.000176\n",
      "batch 5996: loss 0.020122\n",
      "batch 5997: loss 0.011109\n",
      "batch 5998: loss 0.000249\n",
      "batch 5999: loss 0.002046\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.992900\n"
     ]
    }
   ],
   "source": [
    "    sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    num_batches = int(data_loader.num_test_data // batch_size)\n",
    "    for batch_index in range(num_batches):\n",
    "        start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "        y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "        sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "    print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 Keras 中預定義的典型卷積神經網路結構\n",
    "tf.keras.applications 中有一些預定義好的典型卷積神經網路結構，如 VGG16 、 VGG19 、 ResNet 、 MobileNet 等。我們可以直接呼叫這些典型的卷積神經網路結構（甚至載入預訓練的參數），而無需手動定義網路結構。\n",
    "\n",
    "例如，我們可以使用以下代碼來實例化一個 MobileNetV2 網路結構："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.applications.MobileNetV2()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "當執行以上程式碼時，TensorFlow 會自動從網路上下載 MobileNetV2 網路結構，因此在第一次執行程式碼時需要具備網路連接。每個網路結構具有自己特定的詳細參數設置，一些共通的常用參數如下：\n",
    "\n",
    "input_shape ：輸入張量的形狀（不含第一維的 Batch），大多預設為 224 × 224 × 3 。一般而言，模型對輸入張量的大小有下限，長和寬至少為 32 × 32 或 75 × 75 ；\n",
    "\n",
    "include_top ：在網路的最後是否包含全連接層，默認為 True ；\n",
    "\n",
    "weights ：預訓練權重值，預設為 'imagenet' ，即為當前模型載入在 ImageNet 資料集上預訓練的權重值。如需隨機初始化變數可設為 None ；\n",
    "\n",
    "classes ：分類數，預設為 1000。修改該參數需要 include_top 參數為 True 且 weights 參數為 None 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下展示一個例子，使用 MobileNetV2 網路在 tf_flowers 五種分類數據集上進行訓練（為了程式碼的簡短高效，在該範例中我們使用了 TensorFlow Datasets 和 tf.data 載入和預處理資料）。通過將 weights 設置為 None ，我們隨機初始化變數而不使用預訓練權重值。同時將 classes 設置為 5，對應於 5 種分類的資料集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epoch = 5\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(lambda img, label: (tf.image.resize(img, (224, 224)) / 255.0, label)).shuffle(1024).batch(batch_size)\n",
    "model = tf.keras.applications.MobileNetV2(weights=None, classes=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20,14,14,576] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    922\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_FallbackException\u001b[0m: Expecting int64_t value for attr strides, got numpy.int32",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a554c6533732>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mlabels_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    967\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    715\u001b[0m                                 ' implement a `call` method.')\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m     return self._run_internal_graph(\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    967\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    205\u001b[0m       \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m   1104\u001b[0m           call_from_convolution=False)\n\u001b[0;32m   1105\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m     return self.conv_op(\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   2004\u001b[0m   \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2005\u001b[0m   \u001b[0mdilations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dilations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2006\u001b[1;33m   return gen_nn_ops.conv2d(input,  # pylint: disable=redefined-builtin\n\u001b[0m\u001b[0;32m   2007\u001b[0m                            \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m                            \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         return conv2d_eager_fallback(\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[0;32m   1019\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m-> 1021\u001b[1;33m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0m\u001b[0;32m   1022\u001b[0m                              ctx=ctx, name=name)\n\u001b[0;32m   1023\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,14,14,576] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    for images, labels in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(images, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=labels, y_pred=labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"loss %f\" % loss.numpy())\n",
    "            \n",
    "            \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "    print(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循環神經網路（RNN）\n",
    "循環神經網路（Recurrent Neural Network, RNN）是一種適宜於處理序列資料的神經網路，被廣泛用於語言模型、文本生成、機器翻譯等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "\n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "\n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)   # 获得 RNN 的初始状态\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)   # 通过当前输入和前一时刻的状态，得到输出和当前时刻的状态\n",
    "        logits = self.dense(output)\n",
    "        if from_logits:                     # from_logits 参数控制输出是否通过 softmax 函数进行归一化\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)\n",
    "\n",
    "    def predict(self, inputs, temperature=1.):\n",
    "        batch_size, _ = tf.shape(inputs)\n",
    "        logits = self(inputs, from_logits=True)                         # 调用训练好的RNN模型，预测下一个字符的概率分布\n",
    "        prob = tf.nn.softmax(logits / temperature).numpy()              # 使用带 temperature 参数的 softmax 函数获得归一化的概率分布值\n",
    "        return np.array([np.random.choice(self.num_chars, p=prob[i, :]) # 使用 np.random.choice 函数，\n",
    "                         for i in range(batch_size.numpy())])           # 在预测的概率分布 prob 上进行随机取样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練過程與前節基本一致，在此不再贅述：\n",
    "\n",
    "從 DataLoader 中隨機取一批訓練資料；\n",
    "\n",
    "將這批資料送入模型，計算出模型的預測值；\n",
    "\n",
    "將模型預測值與真實值進行比較，計算損失函數（loss）；\n",
    "\n",
    "計算損失函數關於模型變數的導數；\n",
    "\n",
    "使用優化器更新模型參數以最小化損失函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 4.036938\n",
      "batch 1: loss 4.020241\n",
      "batch 2: loss 3.995863\n",
      "batch 3: loss 3.979443\n",
      "batch 4: loss 3.933723\n",
      "batch 5: loss 3.844876\n",
      "batch 6: loss 3.720203\n",
      "batch 7: loss 3.147954\n",
      "batch 8: loss 3.681298\n",
      "batch 9: loss 3.044731\n",
      "batch 10: loss 3.345973\n",
      "batch 11: loss 2.880627\n",
      "batch 12: loss 3.048766\n",
      "batch 13: loss 3.168972\n",
      "batch 14: loss 3.336415\n",
      "batch 15: loss 3.113639\n",
      "batch 16: loss 2.930874\n",
      "batch 17: loss 3.050998\n",
      "batch 18: loss 3.122120\n",
      "batch 19: loss 3.300528\n",
      "batch 20: loss 2.820194\n",
      "batch 21: loss 3.309232\n",
      "batch 22: loss 2.638648\n",
      "batch 23: loss 2.983929\n",
      "batch 24: loss 3.433769\n",
      "batch 25: loss 3.165073\n",
      "batch 26: loss 3.042756\n",
      "batch 27: loss 2.875337\n",
      "batch 28: loss 2.862890\n",
      "batch 29: loss 2.971168\n",
      "batch 30: loss 3.247563\n",
      "batch 31: loss 3.186876\n",
      "batch 32: loss 3.120157\n",
      "batch 33: loss 3.162197\n",
      "batch 34: loss 3.149079\n",
      "batch 35: loss 3.033498\n",
      "batch 36: loss 3.206494\n",
      "batch 37: loss 3.240856\n",
      "batch 38: loss 2.985980\n",
      "batch 39: loss 2.887172\n",
      "batch 40: loss 2.995800\n",
      "batch 41: loss 3.142247\n",
      "batch 42: loss 3.138734\n",
      "batch 43: loss 2.763322\n",
      "batch 44: loss 3.016282\n",
      "batch 45: loss 3.055440\n",
      "batch 46: loss 3.089084\n",
      "batch 47: loss 2.809244\n",
      "batch 48: loss 3.146588\n",
      "batch 49: loss 2.802170\n",
      "batch 50: loss 2.995690\n",
      "batch 51: loss 2.969301\n",
      "batch 52: loss 3.189155\n",
      "batch 53: loss 2.628882\n",
      "batch 54: loss 3.037577\n",
      "batch 55: loss 2.874055\n",
      "batch 56: loss 3.048935\n",
      "batch 57: loss 2.825959\n",
      "batch 58: loss 2.929582\n",
      "batch 59: loss 3.209490\n",
      "batch 60: loss 3.030348\n",
      "batch 61: loss 2.920972\n",
      "batch 62: loss 2.992087\n",
      "batch 63: loss 3.060796\n",
      "batch 64: loss 3.120463\n",
      "batch 65: loss 2.885402\n",
      "batch 66: loss 2.985095\n",
      "batch 67: loss 3.134810\n",
      "batch 68: loss 3.134650\n",
      "batch 69: loss 3.049126\n",
      "batch 70: loss 2.753964\n",
      "batch 71: loss 2.826140\n",
      "batch 72: loss 3.156531\n",
      "batch 73: loss 2.782815\n",
      "batch 74: loss 2.970175\n",
      "batch 75: loss 3.074306\n",
      "batch 76: loss 2.866878\n",
      "batch 77: loss 3.093054\n",
      "batch 78: loss 2.788586\n",
      "batch 79: loss 2.768376\n",
      "batch 80: loss 2.867438\n",
      "batch 81: loss 3.032284\n",
      "batch 82: loss 2.915313\n",
      "batch 83: loss 3.108612\n",
      "batch 84: loss 2.863505\n",
      "batch 85: loss 3.146517\n",
      "batch 86: loss 2.930010\n",
      "batch 87: loss 2.973010\n",
      "batch 88: loss 2.808626\n",
      "batch 89: loss 2.967735\n",
      "batch 90: loss 3.247777\n",
      "batch 91: loss 3.151794\n",
      "batch 92: loss 3.055092\n",
      "batch 93: loss 3.054955\n",
      "batch 94: loss 2.761280\n",
      "batch 95: loss 2.940412\n",
      "batch 96: loss 2.994487\n",
      "batch 97: loss 3.099227\n",
      "batch 98: loss 3.086047\n",
      "batch 99: loss 2.849862\n",
      "batch 100: loss 3.281322\n",
      "batch 101: loss 3.010669\n",
      "batch 102: loss 3.017844\n",
      "batch 103: loss 3.163629\n",
      "batch 104: loss 3.112755\n",
      "batch 105: loss 2.988314\n",
      "batch 106: loss 2.746992\n",
      "batch 107: loss 3.008945\n",
      "batch 108: loss 2.906924\n",
      "batch 109: loss 2.893803\n",
      "batch 110: loss 3.139733\n",
      "batch 111: loss 2.843533\n",
      "batch 112: loss 3.130900\n",
      "batch 113: loss 2.875420\n",
      "batch 114: loss 2.909168\n",
      "batch 115: loss 2.871597\n",
      "batch 116: loss 3.057028\n",
      "batch 117: loss 3.173787\n",
      "batch 118: loss 2.976437\n",
      "batch 119: loss 2.908125\n",
      "batch 120: loss 2.944386\n",
      "batch 121: loss 2.782446\n",
      "batch 122: loss 3.164166\n",
      "batch 123: loss 2.987130\n",
      "batch 124: loss 3.250782\n",
      "batch 125: loss 2.969030\n",
      "batch 126: loss 2.980840\n",
      "batch 127: loss 2.745936\n",
      "batch 128: loss 2.985821\n",
      "batch 129: loss 2.942771\n",
      "batch 130: loss 2.940445\n",
      "batch 131: loss 3.006879\n",
      "batch 132: loss 3.006374\n",
      "batch 133: loss 3.303950\n",
      "batch 134: loss 3.113592\n",
      "batch 135: loss 3.049625\n",
      "batch 136: loss 2.811967\n",
      "batch 137: loss 3.138368\n",
      "batch 138: loss 2.766594\n",
      "batch 139: loss 2.992882\n",
      "batch 140: loss 3.015365\n",
      "batch 141: loss 2.844584\n",
      "batch 142: loss 3.012068\n",
      "batch 143: loss 3.267459\n",
      "batch 144: loss 2.975946\n",
      "batch 145: loss 3.183599\n",
      "batch 146: loss 2.863564\n",
      "batch 147: loss 3.238217\n",
      "batch 148: loss 3.147802\n",
      "batch 149: loss 2.959272\n",
      "batch 150: loss 2.967131\n",
      "batch 151: loss 2.985323\n",
      "batch 152: loss 2.832029\n",
      "batch 153: loss 2.873530\n",
      "batch 154: loss 2.864994\n",
      "batch 155: loss 2.866194\n",
      "batch 156: loss 2.927684\n",
      "batch 157: loss 2.917350\n",
      "batch 158: loss 3.003319\n",
      "batch 159: loss 3.073850\n",
      "batch 160: loss 2.984940\n",
      "batch 161: loss 2.902514\n",
      "batch 162: loss 3.197715\n",
      "batch 163: loss 2.814716\n",
      "batch 164: loss 2.877880\n",
      "batch 165: loss 3.139558\n",
      "batch 166: loss 3.133332\n",
      "batch 167: loss 3.088901\n",
      "batch 168: loss 2.925826\n",
      "batch 169: loss 3.103173\n",
      "batch 170: loss 2.797217\n",
      "batch 171: loss 2.851685\n",
      "batch 172: loss 2.842001\n",
      "batch 173: loss 3.113737\n",
      "batch 174: loss 2.909542\n",
      "batch 175: loss 3.005951\n",
      "batch 176: loss 3.099658\n",
      "batch 177: loss 3.098144\n",
      "batch 178: loss 2.814213\n",
      "batch 179: loss 3.032538\n",
      "batch 180: loss 2.777964\n",
      "batch 181: loss 2.934110\n",
      "batch 182: loss 2.957474\n",
      "batch 183: loss 3.032640\n",
      "batch 184: loss 2.874542\n",
      "batch 185: loss 2.927794\n",
      "batch 186: loss 2.950854\n",
      "batch 187: loss 2.952458\n",
      "batch 188: loss 3.033523\n",
      "batch 189: loss 3.086024\n",
      "batch 190: loss 3.053668\n",
      "batch 191: loss 3.049336\n",
      "batch 192: loss 2.834049\n",
      "batch 193: loss 2.975535\n",
      "batch 194: loss 2.965784\n",
      "batch 195: loss 3.140522\n",
      "batch 196: loss 2.705246\n",
      "batch 197: loss 2.777863\n",
      "batch 198: loss 3.111204\n",
      "batch 199: loss 2.957729\n",
      "batch 200: loss 3.064547\n",
      "batch 201: loss 2.855078\n",
      "batch 202: loss 2.939021\n",
      "batch 203: loss 2.887035\n",
      "batch 204: loss 3.013582\n",
      "batch 205: loss 2.845016\n",
      "batch 206: loss 2.925724\n",
      "batch 207: loss 2.903359\n",
      "batch 208: loss 3.002978\n",
      "batch 209: loss 2.922166\n",
      "batch 210: loss 2.912128\n",
      "batch 211: loss 3.000146\n",
      "batch 212: loss 2.780460\n",
      "batch 213: loss 3.172293\n",
      "batch 214: loss 3.008692\n",
      "batch 215: loss 2.955557\n",
      "batch 216: loss 2.972257\n",
      "batch 217: loss 3.017262\n",
      "batch 218: loss 2.801412\n",
      "batch 219: loss 2.823064\n",
      "batch 220: loss 2.942084\n",
      "batch 221: loss 2.981654\n",
      "batch 222: loss 2.951442\n",
      "batch 223: loss 2.881057\n",
      "batch 224: loss 2.782906\n",
      "batch 225: loss 2.832082\n",
      "batch 226: loss 2.865232\n",
      "batch 227: loss 2.924129\n",
      "batch 228: loss 3.099969\n",
      "batch 229: loss 2.787050\n",
      "batch 230: loss 3.038506\n",
      "batch 231: loss 2.809211\n",
      "batch 232: loss 2.947155\n",
      "batch 233: loss 2.984367\n",
      "batch 234: loss 2.953187\n",
      "batch 235: loss 2.958046\n",
      "batch 236: loss 3.107974\n",
      "batch 237: loss 2.667115\n",
      "batch 238: loss 2.740990\n",
      "batch 239: loss 3.199020\n",
      "batch 240: loss 2.664052\n",
      "batch 241: loss 3.005103\n",
      "batch 242: loss 2.721692\n",
      "batch 243: loss 3.174208\n",
      "batch 244: loss 2.985504\n",
      "batch 245: loss 2.599216\n",
      "batch 246: loss 2.742332\n",
      "batch 247: loss 2.853294\n",
      "batch 248: loss 2.853269\n",
      "batch 249: loss 2.862498\n",
      "batch 250: loss 2.861701\n",
      "batch 251: loss 2.670430\n",
      "batch 252: loss 2.896884\n",
      "batch 253: loss 2.924564\n",
      "batch 254: loss 2.746739\n",
      "batch 255: loss 2.783383\n",
      "batch 256: loss 3.136212\n",
      "batch 257: loss 2.913480\n",
      "batch 258: loss 2.976407\n",
      "batch 259: loss 3.135733\n",
      "batch 260: loss 2.869876\n",
      "batch 261: loss 2.780091\n",
      "batch 262: loss 2.799332\n",
      "batch 263: loss 2.848457\n",
      "batch 264: loss 2.949838\n",
      "batch 265: loss 2.620878\n",
      "batch 266: loss 2.839434\n",
      "batch 267: loss 3.067297\n",
      "batch 268: loss 2.869172\n",
      "batch 269: loss 2.928396\n",
      "batch 270: loss 2.896324\n",
      "batch 271: loss 2.747365\n",
      "batch 272: loss 2.996736\n",
      "batch 273: loss 2.919888\n",
      "batch 274: loss 3.039175\n",
      "batch 275: loss 2.943461\n",
      "batch 276: loss 2.906145\n",
      "batch 277: loss 2.856235\n",
      "batch 278: loss 2.892325\n",
      "batch 279: loss 3.131693\n",
      "batch 280: loss 2.705290\n",
      "batch 281: loss 2.962084\n",
      "batch 282: loss 2.953892\n",
      "batch 283: loss 2.826316\n",
      "batch 284: loss 3.107769\n",
      "batch 285: loss 2.846991\n",
      "batch 286: loss 2.931859\n",
      "batch 287: loss 2.913222\n",
      "batch 288: loss 2.628252\n",
      "batch 289: loss 2.737454\n",
      "batch 290: loss 2.900637\n",
      "batch 291: loss 3.123220\n",
      "batch 292: loss 3.130213\n",
      "batch 293: loss 2.764017\n",
      "batch 294: loss 2.859787\n",
      "batch 295: loss 2.801867\n",
      "batch 296: loss 2.746252\n",
      "batch 297: loss 2.769817\n",
      "batch 298: loss 2.648781\n",
      "batch 299: loss 2.797398\n",
      "batch 300: loss 2.846125\n",
      "batch 301: loss 2.978415\n",
      "batch 302: loss 2.996171\n",
      "batch 303: loss 2.867947\n",
      "batch 304: loss 2.940396\n",
      "batch 305: loss 3.062254\n",
      "batch 306: loss 2.968755\n",
      "batch 307: loss 2.965617\n",
      "batch 308: loss 3.104741\n",
      "batch 309: loss 2.747579\n",
      "batch 310: loss 2.791446\n",
      "batch 311: loss 2.850666\n",
      "batch 312: loss 2.913340\n",
      "batch 313: loss 2.782437\n",
      "batch 314: loss 3.105201\n",
      "batch 315: loss 2.955436\n",
      "batch 316: loss 2.665304\n",
      "batch 317: loss 2.830942\n",
      "batch 318: loss 2.674325\n",
      "batch 319: loss 2.830526\n",
      "batch 320: loss 2.593395\n",
      "batch 321: loss 2.684993\n",
      "batch 322: loss 3.075563\n",
      "batch 323: loss 2.823079\n",
      "batch 324: loss 2.791279\n",
      "batch 325: loss 2.901972\n",
      "batch 326: loss 2.659842\n",
      "batch 327: loss 2.779291\n",
      "batch 328: loss 2.882975\n",
      "batch 329: loss 2.341802\n",
      "batch 330: loss 2.513218\n",
      "batch 331: loss 2.556141\n",
      "batch 332: loss 2.848129\n",
      "batch 333: loss 2.622773\n",
      "batch 334: loss 2.649359\n",
      "batch 335: loss 2.585487\n",
      "batch 336: loss 2.685150\n",
      "batch 337: loss 2.660737\n",
      "batch 338: loss 2.726693\n",
      "batch 339: loss 2.611154\n",
      "batch 340: loss 2.586440\n",
      "batch 341: loss 2.942919\n",
      "batch 342: loss 2.866315\n",
      "batch 343: loss 2.919755\n",
      "batch 344: loss 2.730567\n",
      "batch 345: loss 2.735214\n",
      "batch 346: loss 2.601032\n",
      "batch 347: loss 2.558394\n",
      "batch 348: loss 2.721520\n",
      "batch 349: loss 2.449883\n",
      "batch 350: loss 2.670581\n",
      "batch 351: loss 2.913074\n",
      "batch 352: loss 2.660171\n",
      "batch 353: loss 2.980655\n",
      "batch 354: loss 2.648625\n",
      "batch 355: loss 2.732404\n",
      "batch 356: loss 2.891839\n",
      "batch 357: loss 2.515549\n",
      "batch 358: loss 2.608391\n",
      "batch 359: loss 2.840736\n",
      "batch 360: loss 2.671604\n",
      "batch 361: loss 2.608226\n",
      "batch 362: loss 2.740840\n",
      "batch 363: loss 2.780742\n",
      "batch 364: loss 2.764280\n",
      "batch 365: loss 2.674562\n",
      "batch 366: loss 2.734354\n",
      "batch 367: loss 2.524426\n",
      "batch 368: loss 2.844862\n",
      "batch 369: loss 2.741188\n",
      "batch 370: loss 2.530975\n",
      "batch 371: loss 2.967765\n",
      "batch 372: loss 2.802515\n",
      "batch 373: loss 2.477714\n",
      "batch 374: loss 2.712461\n",
      "batch 375: loss 2.985319\n",
      "batch 376: loss 2.660967\n",
      "batch 377: loss 2.610389\n",
      "batch 378: loss 2.788930\n",
      "batch 379: loss 2.597718\n",
      "batch 380: loss 2.436887\n",
      "batch 381: loss 2.805662\n",
      "batch 382: loss 2.655270\n",
      "batch 383: loss 2.523238\n",
      "batch 384: loss 2.977865\n",
      "batch 385: loss 2.903060\n",
      "batch 386: loss 3.090421\n",
      "batch 387: loss 2.646334\n",
      "batch 388: loss 2.897799\n",
      "batch 389: loss 2.767527\n",
      "batch 390: loss 2.502013\n",
      "batch 391: loss 2.657165\n",
      "batch 392: loss 2.542699\n",
      "batch 393: loss 2.659026\n",
      "batch 394: loss 2.631332\n",
      "batch 395: loss 2.667276\n",
      "batch 396: loss 2.392287\n",
      "batch 397: loss 2.380034\n",
      "batch 398: loss 2.518745\n",
      "batch 399: loss 3.000430\n",
      "batch 400: loss 2.616138\n",
      "batch 401: loss 2.649261\n",
      "batch 402: loss 2.828793\n",
      "batch 403: loss 2.505244\n",
      "batch 404: loss 2.793797\n",
      "batch 405: loss 2.491409\n",
      "batch 406: loss 2.802603\n",
      "batch 407: loss 2.589512\n",
      "batch 408: loss 2.775044\n",
      "batch 409: loss 2.487275\n",
      "batch 410: loss 2.593830\n",
      "batch 411: loss 2.470833\n",
      "batch 412: loss 2.540827\n",
      "batch 413: loss 2.715057\n",
      "batch 414: loss 2.870270\n",
      "batch 415: loss 2.499729\n",
      "batch 416: loss 2.608414\n",
      "batch 417: loss 2.506844\n",
      "batch 418: loss 2.776372\n",
      "batch 419: loss 2.551473\n",
      "batch 420: loss 2.623088\n",
      "batch 421: loss 2.843511\n",
      "batch 422: loss 2.814667\n",
      "batch 423: loss 2.707992\n",
      "batch 424: loss 2.587008\n",
      "batch 425: loss 2.368600\n",
      "batch 426: loss 2.964555\n",
      "batch 427: loss 2.352863\n",
      "batch 428: loss 2.724703\n",
      "batch 429: loss 2.512316\n",
      "batch 430: loss 2.583483\n",
      "batch 431: loss 2.859775\n",
      "batch 432: loss 2.724238\n",
      "batch 433: loss 2.545594\n",
      "batch 434: loss 2.700205\n",
      "batch 435: loss 2.636604\n",
      "batch 436: loss 2.502022\n",
      "batch 437: loss 2.724931\n",
      "batch 438: loss 2.616135\n",
      "batch 439: loss 2.714866\n",
      "batch 440: loss 2.805026\n",
      "batch 441: loss 2.394667\n",
      "batch 442: loss 2.852656\n",
      "batch 443: loss 2.501336\n",
      "batch 444: loss 2.750268\n",
      "batch 445: loss 2.794743\n",
      "batch 446: loss 2.325699\n",
      "batch 447: loss 2.597718\n",
      "batch 448: loss 2.689227\n",
      "batch 449: loss 3.009092\n",
      "batch 450: loss 2.579847\n",
      "batch 451: loss 2.764251\n",
      "batch 452: loss 2.436357\n",
      "batch 453: loss 2.761477\n",
      "batch 454: loss 2.331187\n",
      "batch 455: loss 2.514747\n",
      "batch 456: loss 2.568370\n",
      "batch 457: loss 2.340880\n",
      "batch 458: loss 2.553777\n",
      "batch 459: loss 2.621607\n",
      "batch 460: loss 2.586882\n",
      "batch 461: loss 2.653326\n",
      "batch 462: loss 2.577518\n",
      "batch 463: loss 2.469157\n",
      "batch 464: loss 2.413463\n",
      "batch 465: loss 2.533190\n",
      "batch 466: loss 2.937988\n",
      "batch 467: loss 2.575060\n",
      "batch 468: loss 2.417021\n",
      "batch 469: loss 2.456859\n",
      "batch 470: loss 2.559333\n",
      "batch 471: loss 2.481053\n",
      "batch 472: loss 2.602993\n",
      "batch 473: loss 2.494766\n",
      "batch 474: loss 2.945105\n",
      "batch 475: loss 2.681578\n",
      "batch 476: loss 2.604806\n",
      "batch 477: loss 2.777056\n",
      "batch 478: loss 2.871058\n",
      "batch 479: loss 2.780055\n",
      "batch 480: loss 2.708180\n",
      "batch 481: loss 2.716115\n",
      "batch 482: loss 2.534882\n",
      "batch 483: loss 2.518811\n",
      "batch 484: loss 2.649172\n",
      "batch 485: loss 2.473621\n",
      "batch 486: loss 2.353222\n",
      "batch 487: loss 2.372201\n",
      "batch 488: loss 2.406299\n",
      "batch 489: loss 3.071285\n",
      "batch 490: loss 2.527562\n",
      "batch 491: loss 2.744083\n",
      "batch 492: loss 2.653260\n",
      "batch 493: loss 3.047496\n",
      "batch 494: loss 2.609570\n",
      "batch 495: loss 2.193704\n",
      "batch 496: loss 2.848087\n",
      "batch 497: loss 2.618944\n",
      "batch 498: loss 2.511869\n",
      "batch 499: loss 2.446024\n",
      "batch 500: loss 2.738727\n",
      "batch 501: loss 2.701346\n",
      "batch 502: loss 2.475887\n",
      "batch 503: loss 2.725627\n",
      "batch 504: loss 2.748527\n",
      "batch 505: loss 2.542963\n",
      "batch 506: loss 2.483285\n",
      "batch 507: loss 2.412051\n",
      "batch 508: loss 2.344508\n",
      "batch 509: loss 2.727568\n",
      "batch 510: loss 2.422106\n",
      "batch 511: loss 2.645514\n",
      "batch 512: loss 3.045703\n",
      "batch 513: loss 2.530246\n",
      "batch 514: loss 2.634913\n",
      "batch 515: loss 2.375338\n",
      "batch 516: loss 2.622430\n",
      "batch 517: loss 2.722291\n",
      "batch 518: loss 2.331233\n",
      "batch 519: loss 2.768805\n",
      "batch 520: loss 2.602261\n",
      "batch 521: loss 2.650574\n",
      "batch 522: loss 2.490417\n",
      "batch 523: loss 2.358214\n",
      "batch 524: loss 2.256239\n",
      "batch 525: loss 2.730617\n",
      "batch 526: loss 2.332636\n",
      "batch 527: loss 2.561883\n",
      "batch 528: loss 2.973363\n",
      "batch 529: loss 2.516643\n",
      "batch 530: loss 2.504464\n",
      "batch 531: loss 2.426917\n",
      "batch 532: loss 2.837830\n",
      "batch 533: loss 2.726447\n",
      "batch 534: loss 2.482551\n",
      "batch 535: loss 2.530644\n",
      "batch 536: loss 2.451943\n",
      "batch 537: loss 2.885294\n",
      "batch 538: loss 2.352935\n",
      "batch 539: loss 2.515083\n",
      "batch 540: loss 2.631799\n",
      "batch 541: loss 2.698419\n",
      "batch 542: loss 2.776052\n",
      "batch 543: loss 2.701163\n",
      "batch 544: loss 2.778877\n",
      "batch 545: loss 2.362478\n",
      "batch 546: loss 2.395711\n",
      "batch 547: loss 2.314513\n",
      "batch 548: loss 2.498040\n",
      "batch 549: loss 2.747127\n",
      "batch 550: loss 2.468238\n",
      "batch 551: loss 2.827458\n",
      "batch 552: loss 2.400712\n",
      "batch 553: loss 2.373580\n",
      "batch 554: loss 2.611502\n",
      "batch 555: loss 2.364438\n",
      "batch 556: loss 2.585996\n",
      "batch 557: loss 2.379008\n",
      "batch 558: loss 2.242480\n",
      "batch 559: loss 2.540503\n",
      "batch 560: loss 2.620274\n",
      "batch 561: loss 2.357281\n",
      "batch 562: loss 2.792567\n",
      "batch 563: loss 2.446695\n",
      "batch 564: loss 2.665692\n",
      "batch 565: loss 2.448095\n",
      "batch 566: loss 2.583214\n",
      "batch 567: loss 2.130218\n",
      "batch 568: loss 2.514204\n",
      "batch 569: loss 2.598048\n",
      "batch 570: loss 2.592145\n",
      "batch 571: loss 2.372165\n",
      "batch 572: loss 2.502369\n",
      "batch 573: loss 2.718599\n",
      "batch 574: loss 2.567770\n",
      "batch 575: loss 2.942594\n",
      "batch 576: loss 2.757138\n",
      "batch 577: loss 2.346066\n",
      "batch 578: loss 2.601945\n",
      "batch 579: loss 2.265654\n",
      "batch 580: loss 2.536307\n",
      "batch 581: loss 2.532601\n",
      "batch 582: loss 2.321129\n",
      "batch 583: loss 2.608784\n",
      "batch 584: loss 2.834780\n",
      "batch 585: loss 2.553021\n",
      "batch 586: loss 2.383156\n",
      "batch 587: loss 2.519334\n",
      "batch 588: loss 2.784689\n",
      "batch 589: loss 2.419923\n",
      "batch 590: loss 2.471124\n",
      "batch 591: loss 2.297041\n",
      "batch 592: loss 2.770984\n",
      "batch 593: loss 2.693080\n",
      "batch 594: loss 2.669131\n",
      "batch 595: loss 2.616083\n",
      "batch 596: loss 2.518255\n",
      "batch 597: loss 2.659762\n",
      "batch 598: loss 2.458857\n",
      "batch 599: loss 2.507672\n",
      "batch 600: loss 2.625741\n",
      "batch 601: loss 2.581355\n",
      "batch 602: loss 2.642925\n",
      "batch 603: loss 2.610635\n",
      "batch 604: loss 2.644381\n",
      "batch 605: loss 2.454169\n",
      "batch 606: loss 2.347866\n",
      "batch 607: loss 2.707065\n",
      "batch 608: loss 2.359537\n",
      "batch 609: loss 2.451805\n",
      "batch 610: loss 2.608149\n",
      "batch 611: loss 2.454715\n",
      "batch 612: loss 2.663244\n",
      "batch 613: loss 2.637410\n",
      "batch 614: loss 2.589481\n",
      "batch 615: loss 2.354264\n",
      "batch 616: loss 2.698072\n",
      "batch 617: loss 2.311448\n",
      "batch 618: loss 2.374630\n",
      "batch 619: loss 2.811012\n",
      "batch 620: loss 2.620934\n",
      "batch 621: loss 2.382289\n",
      "batch 622: loss 2.378782\n",
      "batch 623: loss 2.495134\n",
      "batch 624: loss 2.304291\n",
      "batch 625: loss 2.519309\n",
      "batch 626: loss 2.605650\n",
      "batch 627: loss 2.298656\n",
      "batch 628: loss 2.773747\n",
      "batch 629: loss 2.628516\n",
      "batch 630: loss 2.358089\n",
      "batch 631: loss 2.426363\n",
      "batch 632: loss 2.463663\n",
      "batch 633: loss 2.879220\n",
      "batch 634: loss 2.560909\n",
      "batch 635: loss 2.289423\n",
      "batch 636: loss 2.507336\n",
      "batch 637: loss 2.290233\n",
      "batch 638: loss 2.395539\n",
      "batch 639: loss 2.711181\n",
      "batch 640: loss 2.612683\n",
      "batch 641: loss 2.214513\n",
      "batch 642: loss 2.500900\n",
      "batch 643: loss 2.547688\n",
      "batch 644: loss 2.108670\n",
      "batch 645: loss 2.279624\n",
      "batch 646: loss 2.310669\n",
      "batch 647: loss 2.436098\n",
      "batch 648: loss 2.500159\n",
      "batch 649: loss 2.589708\n",
      "batch 650: loss 2.392393\n",
      "batch 651: loss 2.622057\n",
      "batch 652: loss 2.169906\n",
      "batch 653: loss 2.294366\n",
      "batch 654: loss 2.456030\n",
      "batch 655: loss 2.335316\n",
      "batch 656: loss 2.431225\n",
      "batch 657: loss 2.389832\n",
      "batch 658: loss 2.677198\n",
      "batch 659: loss 2.501225\n",
      "batch 660: loss 2.534949\n",
      "batch 661: loss 2.241211\n",
      "batch 662: loss 2.501195\n",
      "batch 663: loss 2.530291\n",
      "batch 664: loss 2.844478\n",
      "batch 665: loss 2.397927\n",
      "batch 666: loss 2.648926\n",
      "batch 667: loss 2.435452\n",
      "batch 668: loss 2.756225\n",
      "batch 669: loss 2.442435\n",
      "batch 670: loss 2.387558\n",
      "batch 671: loss 2.885646\n",
      "batch 672: loss 2.529160\n",
      "batch 673: loss 2.142497\n",
      "batch 674: loss 2.415129\n",
      "batch 675: loss 2.458621\n",
      "batch 676: loss 2.555240\n",
      "batch 677: loss 2.282173\n",
      "batch 678: loss 2.698402\n",
      "batch 679: loss 2.685750\n",
      "batch 680: loss 2.627981\n",
      "batch 681: loss 2.714658\n",
      "batch 682: loss 2.163430\n",
      "batch 683: loss 2.684827\n",
      "batch 684: loss 2.657099\n",
      "batch 685: loss 2.135039\n",
      "batch 686: loss 2.502377\n",
      "batch 687: loss 2.202928\n",
      "batch 688: loss 2.532115\n",
      "batch 689: loss 2.350517\n",
      "batch 690: loss 2.525148\n",
      "batch 691: loss 2.175309\n",
      "batch 692: loss 2.513237\n",
      "batch 693: loss 2.394904\n",
      "batch 694: loss 2.649506\n",
      "batch 695: loss 2.671104\n",
      "batch 696: loss 2.483699\n",
      "batch 697: loss 2.734471\n",
      "batch 698: loss 2.322819\n",
      "batch 699: loss 2.505026\n",
      "batch 700: loss 2.590219\n",
      "batch 701: loss 2.592835\n",
      "batch 702: loss 2.548135\n",
      "batch 703: loss 2.514914\n",
      "batch 704: loss 2.401874\n",
      "batch 705: loss 2.453487\n",
      "batch 706: loss 2.346888\n",
      "batch 707: loss 2.457794\n",
      "batch 708: loss 2.431232\n",
      "batch 709: loss 2.322407\n",
      "batch 710: loss 2.440697\n",
      "batch 711: loss 2.659904\n",
      "batch 712: loss 2.953756\n",
      "batch 713: loss 2.349224\n",
      "batch 714: loss 2.667548\n",
      "batch 715: loss 2.667594\n",
      "batch 716: loss 2.586508\n",
      "batch 717: loss 2.222567\n",
      "batch 718: loss 2.460639\n",
      "batch 719: loss 2.543872\n",
      "batch 720: loss 2.460967\n",
      "batch 721: loss 2.527797\n",
      "batch 722: loss 2.446888\n",
      "batch 723: loss 2.385949\n",
      "batch 724: loss 2.563578\n",
      "batch 725: loss 2.186144\n",
      "batch 726: loss 2.153871\n",
      "batch 727: loss 2.352660\n",
      "batch 728: loss 2.272907\n",
      "batch 729: loss 2.291097\n",
      "batch 730: loss 2.530856\n",
      "batch 731: loss 2.556448\n",
      "batch 732: loss 2.255313\n",
      "batch 733: loss 2.160325\n",
      "batch 734: loss 2.403948\n",
      "batch 735: loss 2.710020\n",
      "batch 736: loss 3.087050\n",
      "batch 737: loss 2.363715\n",
      "batch 738: loss 2.522557\n",
      "batch 739: loss 2.617166\n",
      "batch 740: loss 2.682711\n",
      "batch 741: loss 2.749001\n",
      "batch 742: loss 2.324252\n",
      "batch 743: loss 2.464618\n",
      "batch 744: loss 2.647776\n",
      "batch 745: loss 2.566252\n",
      "batch 746: loss 2.739992\n",
      "batch 747: loss 2.441135\n",
      "batch 748: loss 2.446524\n",
      "batch 749: loss 2.486497\n",
      "batch 750: loss 2.142824\n",
      "batch 751: loss 2.127112\n",
      "batch 752: loss 2.453449\n",
      "batch 753: loss 2.532951\n",
      "batch 754: loss 2.536719\n",
      "batch 755: loss 2.477129\n",
      "batch 756: loss 2.547814\n",
      "batch 757: loss 2.303799\n",
      "batch 758: loss 2.289481\n",
      "batch 759: loss 2.523894\n",
      "batch 760: loss 2.616730\n",
      "batch 761: loss 2.830376\n",
      "batch 762: loss 2.540203\n",
      "batch 763: loss 2.330324\n",
      "batch 764: loss 2.413767\n",
      "batch 765: loss 2.336091\n",
      "batch 766: loss 2.228733\n",
      "batch 767: loss 2.688355\n",
      "batch 768: loss 2.297308\n",
      "batch 769: loss 2.424298\n",
      "batch 770: loss 1.901892\n",
      "batch 771: loss 2.469044\n",
      "batch 772: loss 2.007204\n",
      "batch 773: loss 1.986746\n",
      "batch 774: loss 2.507747\n",
      "batch 775: loss 2.229812\n",
      "batch 776: loss 2.533480\n",
      "batch 777: loss 2.540532\n",
      "batch 778: loss 2.275501\n",
      "batch 779: loss 2.505659\n",
      "batch 780: loss 2.364808\n",
      "batch 781: loss 2.140013\n",
      "batch 782: loss 2.486436\n",
      "batch 783: loss 2.929919\n",
      "batch 784: loss 2.337142\n",
      "batch 785: loss 2.456253\n",
      "batch 786: loss 2.642846\n",
      "batch 787: loss 2.376628\n",
      "batch 788: loss 2.057235\n",
      "batch 789: loss 2.416427\n",
      "batch 790: loss 2.229044\n",
      "batch 791: loss 2.834670\n",
      "batch 792: loss 2.272270\n",
      "batch 793: loss 2.526006\n",
      "batch 794: loss 2.523286\n",
      "batch 795: loss 2.373727\n",
      "batch 796: loss 2.553870\n",
      "batch 797: loss 2.658713\n",
      "batch 798: loss 2.551970\n",
      "batch 799: loss 2.305010\n",
      "batch 800: loss 2.400574\n",
      "batch 801: loss 2.304581\n",
      "batch 802: loss 2.254015\n",
      "batch 803: loss 2.437814\n",
      "batch 804: loss 2.359514\n",
      "batch 805: loss 2.396954\n",
      "batch 806: loss 2.625064\n",
      "batch 807: loss 2.588917\n",
      "batch 808: loss 2.592004\n",
      "batch 809: loss 2.727037\n",
      "batch 810: loss 2.376119\n",
      "batch 811: loss 2.442554\n",
      "batch 812: loss 2.425744\n",
      "batch 813: loss 2.410952\n",
      "batch 814: loss 2.765080\n",
      "batch 815: loss 2.703732\n",
      "batch 816: loss 2.443943\n",
      "batch 817: loss 2.336611\n",
      "batch 818: loss 2.302498\n",
      "batch 819: loss 2.421580\n",
      "batch 820: loss 2.605791\n",
      "batch 821: loss 2.423101\n",
      "batch 822: loss 2.384899\n",
      "batch 823: loss 2.314779\n",
      "batch 824: loss 2.433374\n",
      "batch 825: loss 2.532919\n",
      "batch 826: loss 2.751811\n",
      "batch 827: loss 2.152263\n",
      "batch 828: loss 2.103314\n",
      "batch 829: loss 2.276729\n",
      "batch 830: loss 2.363905\n",
      "batch 831: loss 2.375460\n",
      "batch 832: loss 2.275949\n",
      "batch 833: loss 2.648235\n",
      "batch 834: loss 2.566360\n",
      "batch 835: loss 2.550201\n",
      "batch 836: loss 2.454919\n",
      "batch 837: loss 2.433697\n",
      "batch 838: loss 2.305599\n",
      "batch 839: loss 2.756756\n",
      "batch 840: loss 2.227176\n",
      "batch 841: loss 2.446579\n",
      "batch 842: loss 2.708334\n",
      "batch 843: loss 2.688678\n",
      "batch 844: loss 2.114400\n",
      "batch 845: loss 2.283475\n",
      "batch 846: loss 2.174964\n",
      "batch 847: loss 2.402368\n",
      "batch 848: loss 2.391470\n",
      "batch 849: loss 2.304648\n",
      "batch 850: loss 2.414907\n",
      "batch 851: loss 2.285476\n",
      "batch 852: loss 2.589171\n",
      "batch 853: loss 2.374467\n",
      "batch 854: loss 2.293175\n",
      "batch 855: loss 2.392630\n",
      "batch 856: loss 2.603686\n",
      "batch 857: loss 2.502277\n",
      "batch 858: loss 2.371099\n",
      "batch 859: loss 2.107692\n",
      "batch 860: loss 2.464064\n",
      "batch 861: loss 2.717103\n",
      "batch 862: loss 2.413766\n",
      "batch 863: loss 2.429062\n",
      "batch 864: loss 2.421333\n",
      "batch 865: loss 2.637463\n",
      "batch 866: loss 2.495110\n",
      "batch 867: loss 2.494491\n",
      "batch 868: loss 2.342202\n",
      "batch 869: loss 2.276280\n",
      "batch 870: loss 2.390821\n",
      "batch 871: loss 2.436061\n",
      "batch 872: loss 2.355895\n",
      "batch 873: loss 2.335694\n",
      "batch 874: loss 2.323564\n",
      "batch 875: loss 2.728007\n",
      "batch 876: loss 2.371547\n",
      "batch 877: loss 2.478195\n",
      "batch 878: loss 2.914375\n",
      "batch 879: loss 2.337637\n",
      "batch 880: loss 2.238096\n",
      "batch 881: loss 1.937004\n",
      "batch 882: loss 2.516789\n",
      "batch 883: loss 2.318898\n",
      "batch 884: loss 2.645206\n",
      "batch 885: loss 2.572237\n",
      "batch 886: loss 2.504772\n",
      "batch 887: loss 2.464428\n",
      "batch 888: loss 2.361845\n",
      "batch 889: loss 2.296513\n",
      "batch 890: loss 2.353070\n",
      "batch 891: loss 2.582074\n",
      "batch 892: loss 2.592242\n",
      "batch 893: loss 2.483841\n",
      "batch 894: loss 2.291796\n",
      "batch 895: loss 2.302896\n",
      "batch 896: loss 2.454416\n",
      "batch 897: loss 2.362998\n",
      "batch 898: loss 2.547492\n",
      "batch 899: loss 2.169641\n",
      "batch 900: loss 2.284795\n",
      "batch 901: loss 2.760173\n",
      "batch 902: loss 2.655003\n",
      "batch 903: loss 2.275125\n",
      "batch 904: loss 2.386432\n",
      "batch 905: loss 2.140121\n",
      "batch 906: loss 2.555387\n",
      "batch 907: loss 2.572597\n",
      "batch 908: loss 2.344199\n",
      "batch 909: loss 2.547015\n",
      "batch 910: loss 2.358544\n",
      "batch 911: loss 2.653847\n",
      "batch 912: loss 2.383989\n",
      "batch 913: loss 2.596175\n",
      "batch 914: loss 2.562965\n",
      "batch 915: loss 2.217021\n",
      "batch 916: loss 2.440969\n",
      "batch 917: loss 2.242804\n",
      "batch 918: loss 2.402406\n",
      "batch 919: loss 2.155080\n",
      "batch 920: loss 2.651166\n",
      "batch 921: loss 2.174810\n",
      "batch 922: loss 2.402113\n",
      "batch 923: loss 2.576698\n",
      "batch 924: loss 2.523270\n",
      "batch 925: loss 2.469381\n",
      "batch 926: loss 2.295839\n",
      "batch 927: loss 2.664737\n",
      "batch 928: loss 2.106546\n",
      "batch 929: loss 2.377217\n",
      "batch 930: loss 2.079606\n",
      "batch 931: loss 2.104692\n",
      "batch 932: loss 2.379212\n",
      "batch 933: loss 2.349208\n",
      "batch 934: loss 2.407822\n",
      "batch 935: loss 2.354719\n",
      "batch 936: loss 2.333779\n",
      "batch 937: loss 2.547689\n",
      "batch 938: loss 2.177911\n",
      "batch 939: loss 2.290206\n",
      "batch 940: loss 2.548433\n",
      "batch 941: loss 2.419589\n",
      "batch 942: loss 2.512678\n",
      "batch 943: loss 2.275645\n",
      "batch 944: loss 2.481293\n",
      "batch 945: loss 2.489503\n",
      "batch 946: loss 2.884739\n",
      "batch 947: loss 2.475984\n",
      "batch 948: loss 2.391889\n",
      "batch 949: loss 2.412452\n",
      "batch 950: loss 2.390706\n",
      "batch 951: loss 2.295506\n",
      "batch 952: loss 2.034825\n",
      "batch 953: loss 2.144056\n",
      "batch 954: loss 2.441387\n",
      "batch 955: loss 2.203594\n",
      "batch 956: loss 2.304108\n",
      "batch 957: loss 2.283316\n",
      "batch 958: loss 2.258438\n",
      "batch 959: loss 2.556614\n",
      "batch 960: loss 2.543240\n",
      "batch 961: loss 2.747804\n",
      "batch 962: loss 2.333686\n",
      "batch 963: loss 2.345583\n",
      "batch 964: loss 2.251529\n",
      "batch 965: loss 2.313265\n",
      "batch 966: loss 2.534261\n",
      "batch 967: loss 2.408134\n",
      "batch 968: loss 2.329698\n",
      "batch 969: loss 2.596988\n",
      "batch 970: loss 2.516561\n",
      "batch 971: loss 2.443814\n",
      "batch 972: loss 2.578444\n",
      "batch 973: loss 2.371552\n",
      "batch 974: loss 2.517337\n",
      "batch 975: loss 2.348105\n",
      "batch 976: loss 2.224688\n",
      "batch 977: loss 2.076961\n",
      "batch 978: loss 2.236752\n",
      "batch 979: loss 2.592224\n",
      "batch 980: loss 2.276522\n",
      "batch 981: loss 2.525319\n",
      "batch 982: loss 2.474723\n",
      "batch 983: loss 2.268965\n",
      "batch 984: loss 2.481126\n",
      "batch 985: loss 2.856057\n",
      "batch 986: loss 2.141682\n",
      "batch 987: loss 2.492902\n",
      "batch 988: loss 2.164480\n",
      "batch 989: loss 2.383566\n",
      "batch 990: loss 2.537167\n",
      "batch 991: loss 2.535083\n",
      "batch 992: loss 2.182957\n",
      "batch 993: loss 2.517697\n",
      "batch 994: loss 2.119632\n",
      "batch 995: loss 2.372743\n",
      "batch 996: loss 2.434012\n",
      "batch 997: loss 1.807685\n",
      "batch 998: loss 2.243690\n",
      "batch 999: loss 2.134395\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "關於文本生成的過程有一點需要特別注意。之前，我們一直使用 tf.argmax() 函數，將對應機率最大的值作為預測值。然而對於文本生成而言，這樣的預測方式過於絕對，會使得生成的文本失去豐富性。於是，我們使用 np.random.choice() 函數按照生成的機率分佈取樣。這樣，即使是對應機率較小的字元，也有機會被取樣到。同時，我們加入一個 temperature 參數控制分佈的形狀，參數值越大則分佈越平緩（最大值和最小值的差值越小），生成文本的豐富度越高；參數值越小則分佈越陡峭，生成文本的豐富度越低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity 0.200000:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected int32 passed to parameter 'size' of op 'Slice', got [0.8] of type 'list' instead. Error: Expected int32, got 0.8 of type 'float' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     _ = [_check_failed(v) for v in nest.flatten(values)\n\u001b[0m\u001b[0;32m    264\u001b[0m          if not isinstance(v, expected_types)]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     _ = [_check_failed(v) for v in nest.flatten(values)\n\u001b[0m\u001b[0;32m    264\u001b[0m          if not isinstance(v, expected_types)]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_check_failed\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[1;31m# it is safe to use here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 0.8",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m           values = ops.convert_to_tensor(\n\u001b[0m\u001b[0;32m    466\u001b[0m               \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1341\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    320\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m--> 261\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    262\u001b[0m                         allow_broadcast=True)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    297\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m--> 298\u001b[1;33m       tensor_util.make_tensor_proto(\n\u001b[0m\u001b[0;32m    299\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m       \u001b[0m_AssertCompatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0m\u001b[0;32m    331\u001b[0m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected int32, got 0.8 of type 'float' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a62c01ac0c4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diversity %f:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# 预测下一个字符的编号\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 输出预测的字符\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# 将预测的字符接在输入 X 的末尾，并截断 X 的第一个字符，以保证 X 的长度不变\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m     87\u001b[0m           method.__name__))\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1241\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1650\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m     \"\"\"\n\u001b[1;32m-> 1652\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4069\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4070\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4071\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0;32m   4072\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3219\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3220\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3221\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2530\u001b[0m     \"\"\"\n\u001b[1;32m-> 2531\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2532\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 2657\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3213\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3214\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3154\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    345\u001b[0m       \"\"\"\n\u001b[0;32m    346\u001b[0m       \u001b[0mnum_in_full_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_full_batches\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m       \u001b[0mfirst_k_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[0;32m    349\u001b[0m           first_k_indices, [num_full_batches, batch_size])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \"\"\"\n\u001b[1;32m-> 1037\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   9092\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9093\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9094\u001b[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m   9095\u001b[0m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0;32m   9096\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m    476\u001b[0m                 \u001b[1;34m\"Expected %s passed to parameter '%s' of op '%s', got %s of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;34m\"type '%s' instead. Error: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected int32 passed to parameter 'size' of op 'Slice', got [0.8] of type 'list' instead. Error: Expected int32, got 0.8 of type 'float' instead."
     ]
    }
   ],
   "source": [
    "    X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:      # 丰富度（即temperature）分别设置为从小到大的 4 个值\n",
    "        X = X_\n",
    "        print(\"diversity %f:\" % diversity)\n",
    "        for t in range(400):\n",
    "            y_pred = model.predict(X, diversity)    # 预测下一个字符的编号\n",
    "            print(data_loader.indices_char[y_pred[0]], end='', flush=True)  # 输出预测的字符\n",
    "            X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)     # 将预测的字符接在输入 X 的末尾，并截断 X 的第一个字符，以保证 X 的长度不变\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected int32 passed to parameter 'size' of op 'Slice', got [0.8] of type 'list' instead. Error: Expected int32, got 0.8 of type 'float' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     _ = [_check_failed(v) for v in nest.flatten(values)\n\u001b[0m\u001b[0;32m    264\u001b[0m          if not isinstance(v, expected_types)]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     _ = [_check_failed(v) for v in nest.flatten(values)\n\u001b[0m\u001b[0;32m    264\u001b[0m          if not isinstance(v, expected_types)]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_check_failed\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[1;31m# it is safe to use here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 0.8",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m           values = ops.convert_to_tensor(\n\u001b[0m\u001b[0;32m    466\u001b[0m               \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1341\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    320\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m--> 261\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    262\u001b[0m                         allow_broadcast=True)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    297\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m--> 298\u001b[1;33m       tensor_util.make_tensor_proto(\n\u001b[0m\u001b[0;32m    299\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m       \u001b[0m_AssertCompatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0m\u001b[0;32m    331\u001b[0m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected int32, got 0.8 of type 'float' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f2bf0a656bcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m     87\u001b[0m           method.__name__))\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1241\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1650\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m     \"\"\"\n\u001b[1;32m-> 1652\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4069\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4070\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4071\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0;32m   4072\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3219\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3220\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3221\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2530\u001b[0m     \"\"\"\n\u001b[1;32m-> 2531\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2532\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 2657\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3213\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3214\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3154\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    345\u001b[0m       \"\"\"\n\u001b[0;32m    346\u001b[0m       \u001b[0mnum_in_full_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_full_batches\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m       \u001b[0mfirst_k_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_in_full_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m       first_k_indices = array_ops.reshape(\n\u001b[0;32m    349\u001b[0m           first_k_indices, [num_full_batches, batch_size])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \"\"\"\n\u001b[1;32m-> 1037\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   9092\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9093\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9094\u001b[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m   9095\u001b[0m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0;32m   9096\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m    476\u001b[0m                 \u001b[1;34m\"Expected %s passed to parameter '%s' of op '%s', got %s of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;34m\"type '%s' instead. Error: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected int32 passed to parameter 'size' of op 'Slice', got [0.8] of type 'list' instead. Error: Expected int32, got 0.8 of type 'float' instead."
     ]
    }
   ],
   "source": [
    "y_pred= model.predict(X, diversity)\n",
    "y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
